{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4177,  1.0548, -0.0134, -0.7129])\n",
      "torch.Size([4])\n",
      "tensor([[-0.4177,  1.0548, -0.0134, -0.7129],\n",
      "        [ 0.0000, -0.4177,  1.0548, -0.0134],\n",
      "        [ 0.0000,  0.0000, -0.4177,  1.0548],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.4177]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def vector_to_matrix(v):\n",
    "    \"\"\"\n",
    "    Given a vector v of shape (m,), returns an (m x m) matrix M\n",
    "    where M[i, j] = v[j - i] if j >= i, and 0 otherwise.\n",
    "    \n",
    "    For example, if v = [a, b, c, d] then M will be:\n",
    "    \n",
    "       [ a  b  c  d ]\n",
    "       [ 0  a  b  c ]\n",
    "       [ 0  0  a  b ]\n",
    "       [ 0  0  0  a ]\n",
    "    \"\"\"\n",
    "    v = v.reshape(-1)  # Ensure v is a 1D tensor\n",
    "    m = v.shape[0]\n",
    "    # Create index grids for rows and columns\n",
    "    i, j = torch.meshgrid(torch.arange(m, device=v.device),\n",
    "                            torch.arange(m, device=v.device), \n",
    "                            indexing='ij')\n",
    "    # j - i gives the offset into v. When j < i, we want a 0.\n",
    "    M = torch.where(j >= i, v[j - i], torch.zeros(m, m, device=v.device, dtype=v.dtype))\n",
    "    return M\n",
    "\n",
    "# Example usage:\n",
    "v = torch.randn(4)\n",
    "print(v)\n",
    "print(v.shape)\n",
    "M = vector_to_matrix(v)\n",
    "print(M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4, 7],\n",
      "        [2, 5, 8],\n",
      "        [3, 6, 9]])\n",
      "tensor([[  10,  100, 1000],\n",
      "        [   0,   10,  100],\n",
      "        [   0,    0,   10]])\n",
      "tensor([[  10,   20,   30],\n",
      "        [ 140,  250,  360],\n",
      "        [1470, 2580, 3690]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "vectors = [[1,2,3],[4,5,6],[7,8,9]]\n",
    "vectors = torch.tensor(vectors)\n",
    "print(vectors.T)\n",
    "\n",
    "weights = [[10,100,1000]]\n",
    "weights = torch.tensor(weights)\n",
    "# print(weights)\n",
    "\n",
    "weights_matrix = vector_to_matrix(weights[0])\n",
    "print(weights_matrix)\n",
    "\n",
    "# Perform matrix multiplication\n",
    "result = vectors.T @ weights_matrix\n",
    "print(result.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 4., 13., 28.]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the vectors; assume they have requires_grad=True if needed.\n",
    "v1 = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)  # kernel: [a, b, c]\n",
    "v2 = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)  # input: [x, y, z]\n",
    "\n",
    "# Reshape to shape (batch_size, channels, length)\n",
    "v1_reshaped = v1.view(1, 1, -1)  # shape: (1, 1, 3)\n",
    "v2_reshaped = v2.view(1, 1, -1)  # shape: (1, 1, 3)\n",
    "\n",
    "# Flip the kernel to convert cross-correlation to convolution.\n",
    "kernel = v1_reshaped.flip(-1)  # becomes [c, b, a]\n",
    "\n",
    "# Manually pad v2 on the left with (L-1) zeros.\n",
    "L = v1_reshaped.size(2)  # kernel length, e.g., 3\n",
    "padded_v2 = F.pad(v2_reshaped, (L-1, 0))  # pad left only; no padding on right\n",
    "\n",
    "# Now perform the convolution with padding=0.\n",
    "# The padded input has length N + (L-1), so output length = (N + (L-1)) - L + 1 = N.\n",
    "result = F.conv1d(padded_v2, kernel, padding=0)\n",
    "\n",
    "# result now has shape (1, 1, N), which in our example is (1, 1, 3).\n",
    "print(result)  # This directly gives [a*x, b*x + a*y, c*x + b*y + a*z]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 2., 2., 3., 3., 4., 4.],\n",
      "        [0., 0., 1., 1., 2., 2., 3., 3.],\n",
      "        [0., 0., 0., 0., 1., 1., 2., 2.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1.]], grad_fn=<WhereBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def vector_to_matrix(v, N):\n",
    "    \"\"\"\n",
    "    Given a vector v of shape (M,) and a target number of columns N (which must be a multiple of M),\n",
    "    returns an (M x N) matrix with the following pattern:\n",
    "    \n",
    "    If N == M (i.e. r = 1) then the output is:\n",
    "        [ v[0],   v[1],  ..., v[M-1] ]\n",
    "        [   0,    v[0],  ..., v[M-2] ]\n",
    "        [  ... ]\n",
    "        [   0,      0,   ...,  v[0]  ]\n",
    "    \n",
    "    If N > M, let r = N // M (must be an integer). Then each element of v is repeated r times.\n",
    "    For example, for M=4 and N=8 (r=2), the output is:\n",
    "    \n",
    "        [ v[0], v[0], v[1], v[1], v[2], v[2], v[3], v[3] ]\n",
    "        [   0,    0,  v[0], v[0], v[1], v[1], v[2], v[2] ]\n",
    "        [   0,    0,    0,    0,  v[0], v[0], v[1], v[1] ]\n",
    "        [   0,    0,    0,    0,    0,    0,  v[0], v[0] ]\n",
    "    \n",
    "    The operations are all differentiable so that gradients will flow back to v.\n",
    "    \"\"\"\n",
    "    M = v.shape[0]\n",
    "    # Check that N is a multiple of M\n",
    "    assert N % M == 0, \"N must be a multiple of M\"\n",
    "    r = N // M  # number of times each element is repeated\n",
    "    \n",
    "    device = v.device\n",
    "    # Create a grid of row and column indices.\n",
    "    # i: shape (M, N), where each row i is filled with the row index.\n",
    "    i = torch.arange(M, device=device).unsqueeze(1).expand(M, N)\n",
    "    # j: shape (M, N), where each row is the column indices.\n",
    "    j = torch.arange(N, device=device).unsqueeze(0).expand(M, N)\n",
    "    \n",
    "    # We divide the columns into blocks of size r.\n",
    "    # For each column, b is the block index.\n",
    "    b = j // r  # shape (M, N)\n",
    "    \n",
    "    # For each row i and block index b, we want to use the vector element at position (b - i)\n",
    "    # but only when (b - i) is in the valid range [0, M).\n",
    "    offset = b - i  # shape (M, N)\n",
    "    \n",
    "    # Build a mask to check valid offsets.\n",
    "    valid = (offset >= 0) & (offset < M)\n",
    "    \n",
    "    # Use torch.where: if valid, output v[offset] (which works as a differentiable gather),\n",
    "    # otherwise output 0.\n",
    "    out = torch.where(valid, v[offset], torch.zeros_like(offset, dtype=v.dtype))\n",
    "    return out\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    # Let's take an example with M=4 and N=8.\n",
    "    v = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "    M, N = v.shape[0], 8  # Here r = 2\n",
    "    M_out = vector_to_matrix(v, N)\n",
    "    print(M_out)\n",
    "    # Expected output:\n",
    "    # tensor([[1., 1., 2., 2., 3., 3., 4., 4.],\n",
    "    #         [0., 0., 1., 1., 2., 2., 3., 3.],\n",
    "    #         [0., 0., 0., 0., 1., 1., 2., 2.],\n",
    "    #         [0., 0., 0., 0., 0., 0., 1., 1.]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide matrix (shape torch.Size([4, 8])):\n",
      "tensor([[1., 1., 2., 2., 3., 3., 4., 4.],\n",
      "        [0., 0., 1., 1., 2., 2., 3., 3.],\n",
      "        [0., 0., 0., 0., 1., 1., 2., 2.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1.]], grad_fn=<WhereBackward0>)\n",
      "\n",
      "Tall matrix (shape torch.Size([8, 4])):\n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [0., 1., 2., 3.],\n",
      "        [0., 1., 2., 3.],\n",
      "        [0., 0., 1., 2.],\n",
      "        [0., 0., 1., 2.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.]], grad_fn=<WhereBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def vector_to_matrix(v, N, tall=False):\n",
    "    \"\"\"\n",
    "    Given a vector v of shape (M,) and a target dimension N (which must be an integer multiple of M),\n",
    "    returns a matrix constructed from v as follows.\n",
    "    \n",
    "    If tall == False (the default), the output is of shape (M, N) and has the pattern:\n",
    "    \n",
    "         Row 0: [ v[0], v[0], v[1], v[1], ..., v[M-1], v[M-1] ]\n",
    "         Row 1: [ 0,    0,    v[0], v[0], ..., v[M-2], v[M-2] ]\n",
    "         Row 2: [ 0,    0,    0,    0,    ..., v[0],  v[0]    ]\n",
    "         ...\n",
    "         Row M-1: [ 0, ..., 0, v[0], v[0] ]\n",
    "    \n",
    "    If tall == True, the output is of shape (N, M) and has the pattern:\n",
    "    \n",
    "         Row 0: [ v[0], v[1], v[2], ..., v[M-1] ]\n",
    "         Row 1: [ v[0], v[1], v[2], ..., v[M-1] ]\n",
    "         Row 2: [ 0,    v[0], v[1], ..., v[M-2] ]\n",
    "         Row 3: [ 0,    v[0], v[1], ..., v[M-2] ]\n",
    "         Row 4: [ 0,    0,    v[0], ..., v[M-3] ]\n",
    "         Row 5: [ 0,    0,    v[0], ..., v[M-3] ]\n",
    "         ...\n",
    "         \n",
    "    Here, N must be a multiple of M; if we write N = r*M then the repeating factor is r.\n",
    "    All operations are differentiable, so gradients will correctly propagate back to v.\n",
    "    \"\"\"\n",
    "    M = v.shape[0]\n",
    "    if N % M != 0:\n",
    "        raise ValueError(\"N must be a multiple of the length of v (M).\")\n",
    "    r = N // M  # repetition factor\n",
    "    \n",
    "    if not tall:\n",
    "        # Wide variant: produce an output of shape (M, N)\n",
    "        # Create row and column indices.\n",
    "        i = torch.arange(M, device=v.device).unsqueeze(1).expand(M, N)  # shape: (M, N)\n",
    "        j = torch.arange(N, device=v.device).unsqueeze(0).expand(M, N)  # shape: (M, N)\n",
    "        # Determine which “block” (of size r) each column belongs to.\n",
    "        block = j // r\n",
    "        # For each row i, we want to start using elements from v only after block >= i.\n",
    "        # Compute the effective index into v.\n",
    "        offset = block - i\n",
    "        valid = (offset >= 0) & (offset < M)\n",
    "        result = torch.where(valid, v[offset], torch.zeros(M, N, device=v.device, dtype=v.dtype))\n",
    "        return result\n",
    "    else:\n",
    "        # Tall variant: produce an output of shape (N, M)\n",
    "        # Create indices for rows (i) and columns (j).\n",
    "        i = torch.arange(N, device=v.device).unsqueeze(1).expand(N, M)  # shape: (N, M)\n",
    "        j = torch.arange(M, device=v.device).unsqueeze(0).expand(N, M)  # shape: (N, M)\n",
    "        # Here, each row i belongs to a “block” determined by its index.\n",
    "        block = i // r  # block number for each row (broadcasted along columns)\n",
    "        # Now, for each row, the effective index into v is j - block.\n",
    "        offset = j - block\n",
    "        valid = (offset >= 0) & (offset < M)\n",
    "        result = torch.where(valid, v[offset], torch.zeros(N, M, device=v.device, dtype=v.dtype))\n",
    "        return result\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    # Define a vector v. For clarity, let v = [1, 2, 3, 4]\n",
    "    v = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "    \n",
    "    # Wide variant: shape (4, 8) when N = 8 (here r = 2)\n",
    "    wide_matrix = vector_to_matrix(v, 8, tall=False)\n",
    "    print(\"Wide matrix (shape {}):\".format(wide_matrix.shape))\n",
    "    print(wide_matrix)\n",
    "    # Expected output:\n",
    "    # tensor([[1., 1., 2., 2., 3., 3., 4., 4.],\n",
    "    #         [0., 0., 1., 1., 2., 2., 3., 3.],\n",
    "    #         [0., 0., 0., 0., 1., 1., 2., 2.],\n",
    "    #         [0., 0., 0., 0., 0., 0., 1., 1.]])\n",
    "    \n",
    "    # Tall variant: shape (8, 4)\n",
    "    tall_matrix = vector_to_matrix(v, 8, tall=True)\n",
    "    print(\"\\nTall matrix (shape {}):\".format(tall_matrix.shape))\n",
    "    print(tall_matrix)\n",
    "    # Expected output:\n",
    "    # tensor([[1., 2., 3., 4.],\n",
    "    #         [1., 2., 3., 4.],\n",
    "    #         [0., 1., 2., 3.],\n",
    "    #         [0., 1., 2., 3.],\n",
    "    #         [0., 0., 1., 2.],\n",
    "    #         [0., 0., 1., 2.],\n",
    "    #         [0., 0., 0., 1.],\n",
    "    #         [0., 0., 0., 1.]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
