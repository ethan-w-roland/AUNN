{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#deallocate all cuda memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#print cuda memory\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal-Masked MLP Mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a MLP Mixer based causal-language-model using weight masking\n",
    "\n",
    "class CausalLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A linear layer with a triangular (causal) mask applied to the weight matrix.\n",
    "    This ensures each position i cannot use info from positions > i.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        if in_dim != out_dim:\n",
    "            raise NotImplementedError(\"Only square matrices are supported.\")\n",
    "\n",
    "        # Standard weight + bias\n",
    "        self.weight = nn.Parameter(torch.randn(in_dim, out_dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_dim))\n",
    "\n",
    "        # triangular mask\n",
    "        mask = torch.triu(torch.ones(in_dim, out_dim))\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch, embed_dim, seq_len)\n",
    "        \"\"\"\n",
    "        B, E, S = x.shape\n",
    "        W = self.weight * self.mask    # elementwise multiply\n",
    "        x_reshaped = x.reshape(B * E, S)  # (B*E, S)\n",
    "        out = x_reshaped @ W           # (B*E, S)\n",
    "        out = out + self.bias          # broadcast bias\n",
    "        out = out.view(B, E, S)        # reshape back\n",
    "\n",
    "        return out\n",
    "\n",
    "class MixerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim:int,\n",
    "        seq_len:int,\n",
    "        expansion_factor:int=2,\n",
    "        dropout:float=0.1):\n",
    "\n",
    "        super(MixerBlock, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.expansion_factor = expansion_factor\n",
    "\n",
    "        #channel-norm\n",
    "        self.channel_norm = nn.RMSNorm(hidden_dim)\n",
    "\n",
    "        #channel-mixing layer\n",
    "        self.channel_mixing_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * expansion_factor),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * expansion_factor, hidden_dim)\n",
    "        )\n",
    "\n",
    "        #token-norm\n",
    "        self.token_norm = nn.RMSNorm(hidden_dim)\n",
    "\n",
    "        #token-mixing layer\n",
    "        self.token_mixing_layer = nn.Sequential(\n",
    "            CausalLinear(seq_len, seq_len),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            CausalLinear(seq_len, seq_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        res = x\n",
    "        x = self.channel_norm(x)\n",
    "        x = self.channel_mixing_layer(x)\n",
    "        x = x + res\n",
    "\n",
    "        res = x\n",
    "        x = self.token_norm(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.token_mixing_layer(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = x + res\n",
    "\n",
    "        return x\n",
    "\n",
    "class MLPMixer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size:int,\n",
    "        hidden_dim:int,\n",
    "        seq_len:int,\n",
    "        num_blocks:int):\n",
    "\n",
    "        super(MLPMixer, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        # Input Embedding\n",
    "        self.input_layer = nn.Embedding(vocab_size, hidden_dim)\n",
    "\n",
    "        # Mixer Blocks\n",
    "        self.mixer_blocks = nn.ModuleList(\n",
    "            [MixerBlock(hidden_dim, seq_len) for _ in range(num_blocks)]\n",
    "        )\n",
    "\n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size, bias=False)\n",
    "\n",
    "        # Tie input and output layer weights\n",
    "        self.output_layer.weight = self.input_layer.weight\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "        # Define loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def _init_weights(self):\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear) or isinstance(m, CausalLinear):\n",
    "                # Kaiming He initialization for Swish activation\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "\n",
    "        x = self.input_layer(x)\n",
    "        for block in self.mixer_blocks:\n",
    "            x = block(x)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        if not labels is None:\n",
    "\n",
    "            logits = logits.view(-1, self.vocab_size)\n",
    "            labels = labels.view(-1)\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            return loss, logits\n",
    "\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to save the model checkpoint\n",
    "def save_checkpoint(model, params, optimizer, losses, filename=\"checkpoint.pth\"):\n",
    "    \n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'losses': losses,\n",
    "        'params':{}\n",
    "    }\n",
    "\n",
    "    keys = ['vocab_size', 'hidden_dim', 'seq_len', 'num_blocks']\n",
    "    assert all(k in params for k in keys)\n",
    "    for k in keys:\n",
    "        checkpoint['params'][k] = params[k]\n",
    "\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved with loss {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filename=\"checkpoint.pth\"):\n",
    "\n",
    "    checkpoint = torch.load(filename, weights_only=True)\n",
    "    \n",
    "    params = checkpoint['params']\n",
    "    print(params)\n",
    "    model = MLPMixer(**params)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    losses = checkpoint['losses']\n",
    "\n",
    "    print(f\"Checkpoint loaded: loss {losses[-1]:.4f}\")\n",
    "\n",
    "    return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load text\n",
    "with open('shakespeare/shakespeare.data', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(len(text))\n",
    "\n",
    "# #load text\n",
    "# with open('mixer/TinyStories-train.txt', 'r', encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "# print(len(text))\n",
    "\n",
    "#get unique characters\n",
    "all_chars = sorted(list(set(text)))\n",
    "vocab_size = len(all_chars)\n",
    "print(f\"Unique characters: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2id = {ch: i for i, ch in enumerate(tqdm(all_chars))}\n",
    "id2char = {i: ch for i, ch in enumerate(tqdm(all_chars))}\n",
    "tokens = [char2id[ch] for ch in tqdm(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_dim = 128  # Size of hidden layers\n",
    "seq_len = 128\n",
    "num_blocks = 6\n",
    "batch_size = 32\n",
    "num_epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = MLPMixer(vocab_size, hidden_dim, seq_len, num_blocks).to(device)\n",
    "print(f\"Model has {model.count_params():,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_size = seq_len - (len(tokens) % seq_len) + 1 #we add one to account for x-y offset\n",
    "pad = [char2id[' ']] * pad_size\n",
    "train_data = torch.tensor(tokens + pad, dtype=torch.long).to(device)\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "#get index every seq_len, starting at idx 0\n",
    "idx = list(range(0, len(train_data)-1, seq_len))\n",
    "num_batches = len(idx) // batch_size\n",
    "if len(idx) % batch_size != 0:\n",
    "    num_batches += 1\n",
    "\n",
    "SHUFFLE_BATCHES = True\n",
    "random.seed(42)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print(f\"EPOCH {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    if SHUFFLE_BATCHES:\n",
    "        random.shuffle(idx)\n",
    "\n",
    "    batch_nums = list(range(num_batches))\n",
    "    for i in batch_nums:\n",
    "        \n",
    "        batch_start = i * batch_size\n",
    "        batch_end = min((i+1) * batch_size, len(idx))\n",
    "        batch_idx = idx[batch_start:batch_end]\n",
    "\n",
    "        # Get the batch data\n",
    "        batch = torch.stack([train_data[i:i+seq_len+1] for i in batch_idx], dim=0)\n",
    "        x = batch[:, :-1].contiguous()\n",
    "        y = batch[:, 1:].contiguous()\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss, output = model(x, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    avg_loss = sum(losses[-num_batches:]) / num_batches\n",
    "    print(f\"Average loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(model, {\n",
    "    'vocab_size': vocab_size,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'seq_len': seq_len,\n",
    "    'num_blocks': num_blocks\n",
    "}, optimizer, losses, filename=\"mixer/checkpoint_CLM.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, losses = load_checkpoint(\"mixer/checkpoint_CLM.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "downsample = 1\n",
    "window = 10\n",
    "\n",
    "temp = []\n",
    "for epoch in losses[num_batches*2::downsample]:\n",
    "    avg = np.mean(epoch)\n",
    "    temp.append(avg)\n",
    "temp = np.convolve(temp, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(temp, label=\"Training Loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate(model, prompt, generation_length=100, seq_len=128):\n",
    "    \"\"\"\n",
    "    Generates text from the model given a prompt.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained MLPMixer model.\n",
    "        prompt (str): The prompt to condition generation on.\n",
    "        generation_length (int): Number of characters to generate.\n",
    "        seq_len (int): The sequence length the model expects.\n",
    "        temperature (float): Temperature parameter for sampling.\n",
    "    \n",
    "    Returns:\n",
    "        str: The prompt plus the generated text.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    # Convert prompt into a list of token ids\n",
    "    tokens = [char2id[c] for c in prompt]\n",
    "    \n",
    "    # Autoregressive generation loop\n",
    "    with torch.no_grad():\n",
    "        for _ in range(generation_length):\n",
    "            # Prepare the input context:\n",
    "            # If the context is shorter than seq_len, pad to the left (using the token for space)\n",
    "            if len(tokens) < seq_len:\n",
    "                # pad with the space character (or any chosen pad token)\n",
    "                padded = [char2id[' ']] * (seq_len - len(tokens)) + tokens\n",
    "            else:\n",
    "                padded = tokens[-seq_len:]\n",
    "            \n",
    "            # Convert to a tensor and add batch dimension\n",
    "            x = torch.tensor(padded, dtype=torch.long, device=device).unsqueeze(0)  # shape (1, seq_len)\n",
    "            \n",
    "            # Get logits from the model; output shape is (1, seq_len, vocab_size)\n",
    "            logits = model(x)\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            #intermediate string output\n",
    "            temp = [id2char[t] for t in predictions[0].tolist()]\n",
    "            print(''.join(temp))\n",
    "\n",
    "            next_token = predictions[0][-1].item()\n",
    "            tokens.append(next_token)\n",
    "    \n",
    "    # Convert the list of tokens back to a string\n",
    "    generated_text = ''.join([id2char[t] for t in tokens])\n",
    "    return generated_text\n",
    "\n",
    "# Example usage:\n",
    "prompt = \"We are accounted poor citizens, the patricians good. What authority surfeits on would relieve us: if they would yield us but the superfluity, while it were wholesome, we might guess they relieved us humanely;\"\n",
    "print(len(prompt))\n",
    "generated = evaluate(model, prompt, generation_length=100, seq_len=seq_len)\n",
    "print(\"Generated text:\")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toeplitz Causal MLP Mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a MLP Mixer based causal-language-model using weight masking\n",
    "\n",
    "class ToeplitzCausalLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A linear layer with a triangular (causal) mask applied to the weight matrix.\n",
    "    This ensures each position i cannot use info from positions > i.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # Standard weight + bias\n",
    "        self.weight = nn.Parameter(torch.randn(1, dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def vector_to_matrix(self, v):\n",
    "        \"\"\"\n",
    "        Given a vector v of shape (m,), returns an (m x m) matrix M\n",
    "        where M[i, j] = v[j - i] if j >= i, and 0 otherwise.\n",
    "        \n",
    "        For example, if v = [a, b, c, d] then M will be:\n",
    "        \n",
    "        [ a  b  c  d ]\n",
    "        [ 0  a  b  c ]\n",
    "        [ 0  0  a  b ]\n",
    "        [ 0  0  0  a ]\n",
    "        \"\"\"\n",
    "        v = v.reshape(-1)  # Ensure v is a 1D tensor\n",
    "        m = v.shape[0]\n",
    "        # Create index grids for rows and columns\n",
    "        i, j = torch.meshgrid(torch.arange(m, device=v.device),\n",
    "                                torch.arange(m, device=v.device), \n",
    "                                indexing='ij')\n",
    "        # j - i gives the offset into v. When j < i, we want a 0.\n",
    "        M = torch.where(j >= i, v[j - i], torch.zeros(m, m, device=v.device, dtype=v.dtype))\n",
    "        return M\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch, embed_dim, seq_len)\n",
    "        \"\"\"\n",
    "        B, E, S = x.shape\n",
    "        W = self.vector_to_matrix(self.weight)\n",
    "        x_reshaped = x.reshape(B * E, S)  # (B*E, S)\n",
    "        out = x_reshaped @ W           # (B*E, S)\n",
    "        out = out + self.bias          # broadcast bias\n",
    "        out = out.view(B, E, S)        # reshape back\n",
    "\n",
    "        return out\n",
    "\n",
    "class MixerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim:int,\n",
    "        seq_len:int,\n",
    "        expansion_factor:int=2,\n",
    "        dropout:float=0.1):\n",
    "\n",
    "        super(MixerBlock, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.expansion_factor = expansion_factor\n",
    "\n",
    "        #channel-norm\n",
    "        self.channel_norm = nn.RMSNorm(hidden_dim)\n",
    "\n",
    "        #channel-mixing layer\n",
    "        self.channel_mixing_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * expansion_factor),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * expansion_factor, hidden_dim)\n",
    "        )\n",
    "\n",
    "        #token-norm\n",
    "        self.token_norm = nn.RMSNorm(hidden_dim)\n",
    "\n",
    "        #token-mixing layer\n",
    "        self.token_mixing_layer = nn.Sequential(\n",
    "            ToeplitzCausalLinear(seq_len),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            ToeplitzCausalLinear(seq_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        res = x\n",
    "        x = self.channel_norm(x)\n",
    "        x = self.channel_mixing_layer(x)\n",
    "        x = x + res\n",
    "\n",
    "        res = x\n",
    "        x = self.token_norm(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.token_mixing_layer(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = x + res\n",
    "\n",
    "        return x\n",
    "\n",
    "class MLPMixer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size:int,\n",
    "        hidden_dim:int,\n",
    "        seq_len:int,\n",
    "        num_blocks:int):\n",
    "\n",
    "        super(MLPMixer, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        # Input Embedding\n",
    "        self.input_layer = nn.Embedding(vocab_size, hidden_dim)\n",
    "\n",
    "        # Mixer Blocks\n",
    "        self.mixer_blocks = nn.ModuleList(\n",
    "            [MixerBlock(hidden_dim, seq_len) for _ in range(num_blocks)]\n",
    "        )\n",
    "\n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size, bias=False)\n",
    "\n",
    "        # Tie input and output layer weights\n",
    "        self.output_layer.weight = self.input_layer.weight\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "        # Define loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def _init_weights(self):\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear) or isinstance(m, ToeplitzCausalLinear):\n",
    "                # Kaiming He initialization for Swish activation\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "\n",
    "        x = self.input_layer(x)\n",
    "        for block in self.mixer_blocks:\n",
    "            x = block(x)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        if not labels is None:\n",
    "\n",
    "            logits = logits.view(-1, self.vocab_size)\n",
    "            labels = labels.view(-1)\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            return loss, logits\n",
    "\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to save the model checkpoint\n",
    "def save_checkpoint(model, params, optimizer, losses, filename=\"checkpoint.pth\"):\n",
    "    \n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'losses': losses,\n",
    "        'params':{}\n",
    "    }\n",
    "\n",
    "    keys = ['vocab_size', 'hidden_dim', 'seq_len', 'num_blocks']\n",
    "    assert all(k in params for k in keys)\n",
    "    for k in keys:\n",
    "        checkpoint['params'][k] = params[k]\n",
    "\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved with loss {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filename=\"checkpoint.pth\"):\n",
    "\n",
    "    checkpoint = torch.load(filename, weights_only=True)\n",
    "    \n",
    "    params = checkpoint['params']\n",
    "    print(params)\n",
    "    model = MLPMixer(**params)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    losses = checkpoint['losses']\n",
    "\n",
    "    print(f\"Checkpoint loaded: loss {losses[-1]:.4f}\")\n",
    "\n",
    "    return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load text\n",
    "with open('shakespeare/shakespeare.data', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(len(text))\n",
    "\n",
    "# #load text\n",
    "# with open('mixer/TinyStories-train.txt', 'r', encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "# print(len(text))\n",
    "\n",
    "#get unique characters\n",
    "all_chars = sorted(list(set(text)))\n",
    "vocab_size = len(all_chars)\n",
    "print(f\"Unique characters: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2id = {ch: i for i, ch in enumerate(tqdm(all_chars))}\n",
    "id2char = {i: ch for i, ch in enumerate(tqdm(all_chars))}\n",
    "tokens = [char2id[ch] for ch in tqdm(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_dim = 512  # Size of hidden layers\n",
    "seq_len = 512\n",
    "num_blocks = 6\n",
    "batch_size = 32\n",
    "num_epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 6,352,384 parameters\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = MLPMixer(vocab_size, hidden_dim, seq_len, num_blocks).to(device)\n",
    "print(f\"Model has {model.count_params():,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115649\n"
     ]
    }
   ],
   "source": [
    "pad_size = seq_len - (len(tokens) % seq_len) + 1 #we add one to account for x->y next-token-pred offset\n",
    "pad = [char2id[' ']] * pad_size\n",
    "train_data = torch.tensor(tokens + pad, dtype=torch.long).to(device)\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e895eb7882e64f0892089199ca499b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1/2000\n",
      "Average loss: 4.5805\n",
      "EPOCH 2/2000\n",
      "Average loss: 3.1433\n",
      "EPOCH 3/2000\n",
      "Average loss: 2.8725\n",
      "EPOCH 4/2000\n",
      "Average loss: 2.7542\n",
      "EPOCH 5/2000\n",
      "Average loss: 2.6854\n",
      "EPOCH 6/2000\n",
      "Average loss: 2.6448\n",
      "EPOCH 7/2000\n",
      "Average loss: 2.6142\n",
      "EPOCH 8/2000\n",
      "Average loss: 2.5938\n",
      "EPOCH 9/2000\n",
      "Average loss: 2.5770\n",
      "EPOCH 10/2000\n",
      "Average loss: 2.5654\n",
      "EPOCH 11/2000\n",
      "Average loss: 2.5533\n",
      "EPOCH 12/2000\n",
      "Average loss: 2.5433\n",
      "EPOCH 13/2000\n",
      "Average loss: 2.5356\n",
      "EPOCH 14/2000\n",
      "Average loss: 2.5278\n",
      "EPOCH 15/2000\n",
      "Average loss: 2.5236\n",
      "EPOCH 16/2000\n",
      "Average loss: 2.5201\n",
      "EPOCH 17/2000\n",
      "Average loss: 2.5155\n",
      "EPOCH 18/2000\n",
      "Average loss: 2.5102\n",
      "EPOCH 19/2000\n",
      "Average loss: 2.5090\n",
      "EPOCH 20/2000\n",
      "Average loss: 2.5050\n",
      "EPOCH 21/2000\n",
      "Average loss: 2.5023\n",
      "EPOCH 22/2000\n",
      "Average loss: 2.5004\n",
      "EPOCH 23/2000\n",
      "Average loss: 2.4974\n",
      "EPOCH 24/2000\n",
      "Average loss: 2.4941\n",
      "EPOCH 25/2000\n",
      "Average loss: 2.4930\n",
      "EPOCH 26/2000\n",
      "Average loss: 2.4921\n",
      "EPOCH 27/2000\n",
      "Average loss: 2.4909\n",
      "EPOCH 28/2000\n",
      "Average loss: 2.4870\n",
      "EPOCH 29/2000\n",
      "Average loss: 2.4846\n",
      "EPOCH 30/2000\n",
      "Average loss: 2.4828\n",
      "EPOCH 31/2000\n",
      "Average loss: 2.4809\n",
      "EPOCH 32/2000\n",
      "Average loss: 2.4794\n",
      "EPOCH 33/2000\n",
      "Average loss: 2.4760\n",
      "EPOCH 34/2000\n",
      "Average loss: 2.4730\n",
      "EPOCH 35/2000\n",
      "Average loss: 2.4706\n",
      "EPOCH 36/2000\n",
      "Average loss: 2.4663\n",
      "EPOCH 37/2000\n",
      "Average loss: 2.4606\n",
      "EPOCH 38/2000\n",
      "Average loss: 2.4551\n",
      "EPOCH 39/2000\n",
      "Average loss: 2.4473\n",
      "EPOCH 40/2000\n",
      "Average loss: 2.4383\n",
      "EPOCH 41/2000\n",
      "Average loss: 2.4278\n",
      "EPOCH 42/2000\n",
      "Average loss: 2.4133\n",
      "EPOCH 43/2000\n",
      "Average loss: 2.3925\n",
      "EPOCH 44/2000\n",
      "Average loss: 2.3710\n",
      "EPOCH 45/2000\n",
      "Average loss: 2.3439\n",
      "EPOCH 46/2000\n",
      "Average loss: 2.3108\n",
      "EPOCH 47/2000\n",
      "Average loss: 2.2741\n",
      "EPOCH 48/2000\n",
      "Average loss: 2.2335\n",
      "EPOCH 49/2000\n",
      "Average loss: 2.1885\n",
      "EPOCH 50/2000\n",
      "Average loss: 2.1499\n",
      "EPOCH 51/2000\n",
      "Average loss: 2.1140\n",
      "EPOCH 52/2000\n",
      "Average loss: 2.0814\n",
      "EPOCH 53/2000\n",
      "Average loss: 2.0485\n",
      "EPOCH 54/2000\n",
      "Average loss: 2.0185\n",
      "EPOCH 55/2000\n",
      "Average loss: 1.9940\n",
      "EPOCH 56/2000\n",
      "Average loss: 1.9659\n",
      "EPOCH 57/2000\n",
      "Average loss: 1.9452\n",
      "EPOCH 58/2000\n",
      "Average loss: 1.9211\n",
      "EPOCH 59/2000\n",
      "Average loss: 1.9006\n",
      "EPOCH 60/2000\n",
      "Average loss: 1.8815\n",
      "EPOCH 61/2000\n",
      "Average loss: 1.8602\n",
      "EPOCH 62/2000\n",
      "Average loss: 1.8438\n",
      "EPOCH 63/2000\n",
      "Average loss: 1.8246\n",
      "EPOCH 64/2000\n",
      "Average loss: 1.8085\n",
      "EPOCH 65/2000\n",
      "Average loss: 1.7942\n",
      "EPOCH 66/2000\n",
      "Average loss: 1.7817\n",
      "EPOCH 67/2000\n",
      "Average loss: 1.7675\n",
      "EPOCH 68/2000\n",
      "Average loss: 1.7524\n",
      "EPOCH 69/2000\n",
      "Average loss: 1.7420\n",
      "EPOCH 70/2000\n",
      "Average loss: 1.7287\n",
      "EPOCH 71/2000\n",
      "Average loss: 1.7162\n",
      "EPOCH 72/2000\n",
      "Average loss: 1.7050\n",
      "EPOCH 73/2000\n",
      "Average loss: 1.6934\n",
      "EPOCH 74/2000\n",
      "Average loss: 1.6855\n",
      "EPOCH 75/2000\n",
      "Average loss: 1.6729\n",
      "EPOCH 76/2000\n",
      "Average loss: 1.6643\n",
      "EPOCH 77/2000\n",
      "Average loss: 1.6563\n",
      "EPOCH 78/2000\n",
      "Average loss: 1.6424\n",
      "EPOCH 79/2000\n",
      "Average loss: 1.6365\n",
      "EPOCH 80/2000\n",
      "Average loss: 1.6263\n",
      "EPOCH 81/2000\n",
      "Average loss: 1.6186\n",
      "EPOCH 82/2000\n",
      "Average loss: 1.6097\n",
      "EPOCH 83/2000\n",
      "Average loss: 1.6036\n",
      "EPOCH 84/2000\n",
      "Average loss: 1.5925\n",
      "EPOCH 85/2000\n",
      "Average loss: 1.5870\n",
      "EPOCH 86/2000\n",
      "Average loss: 1.5764\n",
      "EPOCH 87/2000\n",
      "Average loss: 1.5680\n",
      "EPOCH 88/2000\n",
      "Average loss: 1.5603\n",
      "EPOCH 89/2000\n",
      "Average loss: 1.5540\n",
      "EPOCH 90/2000\n",
      "Average loss: 1.5462\n",
      "EPOCH 91/2000\n",
      "Average loss: 1.5398\n",
      "EPOCH 92/2000\n",
      "Average loss: 1.5332\n",
      "EPOCH 93/2000\n",
      "Average loss: 1.5276\n",
      "EPOCH 94/2000\n",
      "Average loss: 1.5205\n",
      "EPOCH 95/2000\n",
      "Average loss: 1.5156\n",
      "EPOCH 96/2000\n",
      "Average loss: 1.5106\n",
      "EPOCH 97/2000\n",
      "Average loss: 1.5039\n",
      "EPOCH 98/2000\n",
      "Average loss: 1.4966\n",
      "EPOCH 99/2000\n",
      "Average loss: 1.4915\n",
      "EPOCH 100/2000\n",
      "Average loss: 1.4849\n",
      "EPOCH 101/2000\n",
      "Average loss: 1.4798\n",
      "EPOCH 102/2000\n",
      "Average loss: 1.4751\n",
      "EPOCH 103/2000\n",
      "Average loss: 1.4704\n",
      "EPOCH 104/2000\n",
      "Average loss: 1.4637\n",
      "EPOCH 105/2000\n",
      "Average loss: 1.4593\n",
      "EPOCH 106/2000\n",
      "Average loss: 1.4540\n",
      "EPOCH 107/2000\n",
      "Average loss: 1.4476\n",
      "EPOCH 108/2000\n",
      "Average loss: 1.4443\n",
      "EPOCH 109/2000\n",
      "Average loss: 1.4402\n",
      "EPOCH 110/2000\n",
      "Average loss: 1.4359\n",
      "EPOCH 111/2000\n",
      "Average loss: 1.4319\n",
      "EPOCH 112/2000\n",
      "Average loss: 1.4260\n",
      "EPOCH 113/2000\n",
      "Average loss: 1.4208\n",
      "EPOCH 114/2000\n",
      "Average loss: 1.4168\n",
      "EPOCH 115/2000\n",
      "Average loss: 1.4138\n",
      "EPOCH 116/2000\n",
      "Average loss: 1.4085\n",
      "EPOCH 117/2000\n",
      "Average loss: 1.4050\n",
      "EPOCH 118/2000\n",
      "Average loss: 1.3985\n",
      "EPOCH 119/2000\n",
      "Average loss: 1.3945\n",
      "EPOCH 120/2000\n",
      "Average loss: 1.3913\n",
      "EPOCH 121/2000\n",
      "Average loss: 1.3887\n",
      "EPOCH 122/2000\n",
      "Average loss: 1.3836\n",
      "EPOCH 123/2000\n",
      "Average loss: 1.3822\n",
      "EPOCH 124/2000\n",
      "Average loss: 1.3774\n",
      "EPOCH 125/2000\n",
      "Average loss: 1.3737\n",
      "EPOCH 126/2000\n",
      "Average loss: 1.3685\n",
      "EPOCH 127/2000\n",
      "Average loss: 1.3649\n",
      "EPOCH 128/2000\n",
      "Average loss: 1.3638\n",
      "EPOCH 129/2000\n",
      "Average loss: 1.3602\n",
      "EPOCH 130/2000\n",
      "Average loss: 1.3567\n",
      "EPOCH 131/2000\n",
      "Average loss: 1.3532\n",
      "EPOCH 132/2000\n",
      "Average loss: 1.3506\n",
      "EPOCH 133/2000\n",
      "Average loss: 1.3455\n",
      "EPOCH 134/2000\n",
      "Average loss: 1.3402\n",
      "EPOCH 135/2000\n",
      "Average loss: 1.3387\n",
      "EPOCH 136/2000\n",
      "Average loss: 1.3358\n",
      "EPOCH 137/2000\n",
      "Average loss: 1.3318\n",
      "EPOCH 138/2000\n",
      "Average loss: 1.3277\n",
      "EPOCH 139/2000\n",
      "Average loss: 1.3252\n",
      "EPOCH 140/2000\n",
      "Average loss: 1.3222\n",
      "EPOCH 141/2000\n",
      "Average loss: 1.3177\n",
      "EPOCH 142/2000\n",
      "Average loss: 1.3179\n",
      "EPOCH 143/2000\n",
      "Average loss: 1.3142\n",
      "EPOCH 144/2000\n",
      "Average loss: 1.3106\n",
      "EPOCH 145/2000\n",
      "Average loss: 1.3069\n",
      "EPOCH 146/2000\n",
      "Average loss: 1.3031\n",
      "EPOCH 147/2000\n",
      "Average loss: 1.2997\n",
      "EPOCH 148/2000\n",
      "Average loss: 1.2971\n",
      "EPOCH 149/2000\n",
      "Average loss: 1.2928\n",
      "EPOCH 150/2000\n",
      "Average loss: 1.2890\n",
      "EPOCH 151/2000\n",
      "Average loss: 1.2878\n",
      "EPOCH 152/2000\n",
      "Average loss: 1.2846\n",
      "EPOCH 153/2000\n",
      "Average loss: 1.2805\n",
      "EPOCH 154/2000\n",
      "Average loss: 1.2788\n",
      "EPOCH 155/2000\n",
      "Average loss: 1.2777\n",
      "EPOCH 156/2000\n",
      "Average loss: 1.2743\n",
      "EPOCH 157/2000\n",
      "Average loss: 1.2714\n",
      "EPOCH 158/2000\n",
      "Average loss: 1.2697\n",
      "EPOCH 159/2000\n",
      "Average loss: 1.2668\n",
      "EPOCH 160/2000\n",
      "Average loss: 1.2628\n",
      "EPOCH 161/2000\n",
      "Average loss: 1.2591\n",
      "EPOCH 162/2000\n",
      "Average loss: 1.2570\n",
      "EPOCH 163/2000\n",
      "Average loss: 1.2524\n",
      "EPOCH 164/2000\n",
      "Average loss: 1.2512\n",
      "EPOCH 165/2000\n",
      "Average loss: 1.2480\n",
      "EPOCH 166/2000\n",
      "Average loss: 1.2442\n",
      "EPOCH 167/2000\n",
      "Average loss: 1.2429\n",
      "EPOCH 168/2000\n",
      "Average loss: 1.2396\n",
      "EPOCH 169/2000\n",
      "Average loss: 1.2351\n",
      "EPOCH 170/2000\n",
      "Average loss: 1.2329\n",
      "EPOCH 171/2000\n",
      "Average loss: 1.2320\n",
      "EPOCH 172/2000\n",
      "Average loss: 1.2277\n",
      "EPOCH 173/2000\n",
      "Average loss: 1.2251\n",
      "EPOCH 174/2000\n",
      "Average loss: 1.2226\n",
      "EPOCH 175/2000\n",
      "Average loss: 1.2187\n",
      "EPOCH 176/2000\n",
      "Average loss: 1.2188\n",
      "EPOCH 177/2000\n",
      "Average loss: 1.2134\n",
      "EPOCH 178/2000\n",
      "Average loss: 1.2132\n",
      "EPOCH 179/2000\n",
      "Average loss: 1.2093\n",
      "EPOCH 180/2000\n",
      "Average loss: 1.2054\n",
      "EPOCH 181/2000\n",
      "Average loss: 1.2036\n",
      "EPOCH 182/2000\n",
      "Average loss: 1.2005\n",
      "EPOCH 183/2000\n",
      "Average loss: 1.1959\n",
      "EPOCH 184/2000\n",
      "Average loss: 1.1942\n",
      "EPOCH 185/2000\n",
      "Average loss: 1.1913\n",
      "EPOCH 186/2000\n",
      "Average loss: 1.1901\n",
      "EPOCH 187/2000\n",
      "Average loss: 1.1846\n",
      "EPOCH 188/2000\n",
      "Average loss: 1.1835\n",
      "EPOCH 189/2000\n",
      "Average loss: 1.1817\n",
      "EPOCH 190/2000\n",
      "Average loss: 1.1782\n",
      "EPOCH 191/2000\n",
      "Average loss: 1.1753\n",
      "EPOCH 192/2000\n",
      "Average loss: 1.1716\n",
      "EPOCH 193/2000\n",
      "Average loss: 1.1690\n",
      "EPOCH 194/2000\n",
      "Average loss: 1.1666\n",
      "EPOCH 195/2000\n",
      "Average loss: 1.1642\n",
      "EPOCH 196/2000\n",
      "Average loss: 1.1625\n",
      "EPOCH 197/2000\n",
      "Average loss: 1.1568\n",
      "EPOCH 198/2000\n",
      "Average loss: 1.1551\n",
      "EPOCH 199/2000\n",
      "Average loss: 1.1518\n",
      "EPOCH 200/2000\n",
      "Average loss: 1.1498\n",
      "EPOCH 201/2000\n",
      "Average loss: 1.1459\n",
      "EPOCH 202/2000\n",
      "Average loss: 1.1459\n",
      "EPOCH 203/2000\n",
      "Average loss: 1.1412\n",
      "EPOCH 204/2000\n",
      "Average loss: 1.1372\n",
      "EPOCH 205/2000\n",
      "Average loss: 1.1348\n",
      "EPOCH 206/2000\n",
      "Average loss: 1.1330\n",
      "EPOCH 207/2000\n",
      "Average loss: 1.1280\n",
      "EPOCH 208/2000\n",
      "Average loss: 1.1271\n",
      "EPOCH 209/2000\n",
      "Average loss: 1.1256\n",
      "EPOCH 210/2000\n",
      "Average loss: 1.1202\n",
      "EPOCH 211/2000\n",
      "Average loss: 1.1182\n",
      "EPOCH 212/2000\n",
      "Average loss: 1.1150\n",
      "EPOCH 213/2000\n",
      "Average loss: 1.1138\n",
      "EPOCH 214/2000\n",
      "Average loss: 1.1102\n",
      "EPOCH 215/2000\n",
      "Average loss: 1.1088\n",
      "EPOCH 216/2000\n",
      "Average loss: 1.1056\n",
      "EPOCH 217/2000\n",
      "Average loss: 1.1012\n",
      "EPOCH 218/2000\n",
      "Average loss: 1.0992\n",
      "EPOCH 219/2000\n",
      "Average loss: 1.0964\n",
      "EPOCH 220/2000\n",
      "Average loss: 1.0931\n",
      "EPOCH 221/2000\n",
      "Average loss: 1.0921\n",
      "EPOCH 222/2000\n",
      "Average loss: 1.0863\n",
      "EPOCH 223/2000\n",
      "Average loss: 1.0829\n",
      "EPOCH 224/2000\n",
      "Average loss: 1.0813\n",
      "EPOCH 225/2000\n",
      "Average loss: 1.0795\n",
      "EPOCH 226/2000\n",
      "Average loss: 1.0774\n",
      "EPOCH 227/2000\n",
      "Average loss: 1.0744\n",
      "EPOCH 228/2000\n",
      "Average loss: 1.0712\n",
      "EPOCH 229/2000\n",
      "Average loss: 1.0674\n",
      "EPOCH 230/2000\n",
      "Average loss: 1.0635\n",
      "EPOCH 231/2000\n",
      "Average loss: 1.0621\n",
      "EPOCH 232/2000\n",
      "Average loss: 1.0584\n",
      "EPOCH 233/2000\n",
      "Average loss: 1.0557\n",
      "EPOCH 234/2000\n",
      "Average loss: 1.0534\n",
      "EPOCH 235/2000\n",
      "Average loss: 1.0511\n",
      "EPOCH 236/2000\n",
      "Average loss: 1.0468\n",
      "EPOCH 237/2000\n",
      "Average loss: 1.0453\n",
      "EPOCH 238/2000\n",
      "Average loss: 1.0410\n",
      "EPOCH 239/2000\n",
      "Average loss: 1.0392\n",
      "EPOCH 240/2000\n",
      "Average loss: 1.0372\n",
      "EPOCH 241/2000\n",
      "Average loss: 1.0350\n",
      "EPOCH 242/2000\n",
      "Average loss: 1.0318\n",
      "EPOCH 243/2000\n",
      "Average loss: 1.0275\n",
      "EPOCH 244/2000\n",
      "Average loss: 1.0257\n",
      "EPOCH 245/2000\n",
      "Average loss: 1.0217\n",
      "EPOCH 246/2000\n",
      "Average loss: 1.0199\n",
      "EPOCH 247/2000\n",
      "Average loss: 1.0170\n",
      "EPOCH 248/2000\n",
      "Average loss: 1.0145\n",
      "EPOCH 249/2000\n",
      "Average loss: 1.0103\n",
      "EPOCH 250/2000\n",
      "Average loss: 1.0075\n",
      "EPOCH 251/2000\n",
      "Average loss: 1.0050\n",
      "EPOCH 252/2000\n",
      "Average loss: 1.0028\n",
      "EPOCH 253/2000\n",
      "Average loss: 1.0001\n",
      "EPOCH 254/2000\n",
      "Average loss: 0.9980\n",
      "EPOCH 255/2000\n",
      "Average loss: 0.9956\n",
      "EPOCH 256/2000\n",
      "Average loss: 0.9915\n",
      "EPOCH 257/2000\n",
      "Average loss: 0.9887\n",
      "EPOCH 258/2000\n",
      "Average loss: 0.9844\n",
      "EPOCH 259/2000\n",
      "Average loss: 0.9817\n",
      "EPOCH 260/2000\n",
      "Average loss: 0.9816\n",
      "EPOCH 261/2000\n",
      "Average loss: 0.9773\n",
      "EPOCH 262/2000\n",
      "Average loss: 0.9746\n",
      "EPOCH 263/2000\n",
      "Average loss: 0.9714\n",
      "EPOCH 264/2000\n",
      "Average loss: 0.9683\n",
      "EPOCH 265/2000\n",
      "Average loss: 0.9680\n",
      "EPOCH 266/2000\n",
      "Average loss: 0.9645\n",
      "EPOCH 267/2000\n",
      "Average loss: 0.9618\n",
      "EPOCH 268/2000\n",
      "Average loss: 0.9604\n",
      "EPOCH 269/2000\n",
      "Average loss: 0.9565\n",
      "EPOCH 270/2000\n",
      "Average loss: 0.9527\n",
      "EPOCH 271/2000\n",
      "Average loss: 0.9507\n",
      "EPOCH 272/2000\n",
      "Average loss: 0.9484\n",
      "EPOCH 273/2000\n",
      "Average loss: 0.9455\n",
      "EPOCH 274/2000\n",
      "Average loss: 0.9431\n",
      "EPOCH 275/2000\n",
      "Average loss: 0.9388\n",
      "EPOCH 276/2000\n",
      "Average loss: 0.9373\n",
      "EPOCH 277/2000\n",
      "Average loss: 0.9361\n",
      "EPOCH 278/2000\n",
      "Average loss: 0.9320\n",
      "EPOCH 279/2000\n",
      "Average loss: 0.9294\n",
      "EPOCH 280/2000\n",
      "Average loss: 0.9271\n",
      "EPOCH 281/2000\n",
      "Average loss: 0.9234\n",
      "EPOCH 282/2000\n",
      "Average loss: 0.9216\n",
      "EPOCH 283/2000\n",
      "Average loss: 0.9190\n",
      "EPOCH 284/2000\n",
      "Average loss: 0.9159\n",
      "EPOCH 285/2000\n",
      "Average loss: 0.9133\n",
      "EPOCH 286/2000\n",
      "Average loss: 0.9102\n",
      "EPOCH 287/2000\n",
      "Average loss: 0.9079\n",
      "EPOCH 288/2000\n",
      "Average loss: 0.9047\n",
      "EPOCH 289/2000\n",
      "Average loss: 0.9029\n",
      "EPOCH 290/2000\n",
      "Average loss: 0.8984\n",
      "EPOCH 291/2000\n",
      "Average loss: 0.8968\n",
      "EPOCH 292/2000\n",
      "Average loss: 0.8955\n",
      "EPOCH 293/2000\n",
      "Average loss: 0.8937\n",
      "EPOCH 294/2000\n",
      "Average loss: 0.8902\n",
      "EPOCH 295/2000\n",
      "Average loss: 0.8879\n",
      "EPOCH 296/2000\n",
      "Average loss: 0.8843\n",
      "EPOCH 297/2000\n",
      "Average loss: 0.8815\n",
      "EPOCH 298/2000\n",
      "Average loss: 0.8791\n",
      "EPOCH 299/2000\n",
      "Average loss: 0.8766\n",
      "EPOCH 300/2000\n",
      "Average loss: 0.8731\n",
      "EPOCH 301/2000\n",
      "Average loss: 0.8710\n",
      "EPOCH 302/2000\n",
      "Average loss: 0.8689\n",
      "EPOCH 303/2000\n",
      "Average loss: 0.8670\n",
      "EPOCH 304/2000\n",
      "Average loss: 0.8633\n",
      "EPOCH 305/2000\n",
      "Average loss: 0.8625\n",
      "EPOCH 306/2000\n",
      "Average loss: 0.8589\n",
      "EPOCH 307/2000\n",
      "Average loss: 0.8571\n",
      "EPOCH 308/2000\n",
      "Average loss: 0.8542\n",
      "EPOCH 309/2000\n",
      "Average loss: 0.8519\n",
      "EPOCH 310/2000\n",
      "Average loss: 0.8487\n",
      "EPOCH 311/2000\n",
      "Average loss: 0.8478\n",
      "EPOCH 312/2000\n",
      "Average loss: 0.8438\n",
      "EPOCH 313/2000\n",
      "Average loss: 0.8426\n",
      "EPOCH 314/2000\n",
      "Average loss: 0.8386\n",
      "EPOCH 315/2000\n",
      "Average loss: 0.8385\n",
      "EPOCH 316/2000\n",
      "Average loss: 0.8352\n",
      "EPOCH 317/2000\n",
      "Average loss: 0.8334\n",
      "EPOCH 318/2000\n",
      "Average loss: 0.8299\n",
      "EPOCH 319/2000\n",
      "Average loss: 0.8289\n",
      "EPOCH 320/2000\n",
      "Average loss: 0.8263\n",
      "EPOCH 321/2000\n",
      "Average loss: 0.8237\n",
      "EPOCH 322/2000\n",
      "Average loss: 0.8208\n",
      "EPOCH 323/2000\n",
      "Average loss: 0.8185\n",
      "EPOCH 324/2000\n",
      "Average loss: 0.8169\n",
      "EPOCH 325/2000\n",
      "Average loss: 0.8148\n",
      "EPOCH 326/2000\n",
      "Average loss: 0.8127\n",
      "EPOCH 327/2000\n",
      "Average loss: 0.8084\n",
      "EPOCH 328/2000\n",
      "Average loss: 0.8059\n",
      "EPOCH 329/2000\n",
      "Average loss: 0.8045\n",
      "EPOCH 330/2000\n",
      "Average loss: 0.8023\n",
      "EPOCH 331/2000\n",
      "Average loss: 0.8001\n",
      "EPOCH 332/2000\n",
      "Average loss: 0.7975\n",
      "EPOCH 333/2000\n",
      "Average loss: 0.7968\n",
      "EPOCH 334/2000\n",
      "Average loss: 0.7942\n",
      "EPOCH 335/2000\n",
      "Average loss: 0.7910\n",
      "EPOCH 336/2000\n",
      "Average loss: 0.7880\n",
      "EPOCH 337/2000\n",
      "Average loss: 0.7872\n",
      "EPOCH 338/2000\n",
      "Average loss: 0.7834\n",
      "EPOCH 339/2000\n",
      "Average loss: 0.7822\n",
      "EPOCH 340/2000\n",
      "Average loss: 0.7798\n",
      "EPOCH 341/2000\n",
      "Average loss: 0.7768\n",
      "EPOCH 342/2000\n",
      "Average loss: 0.7760\n",
      "EPOCH 343/2000\n",
      "Average loss: 0.7731\n",
      "EPOCH 344/2000\n",
      "Average loss: 0.7719\n",
      "EPOCH 345/2000\n",
      "Average loss: 0.7678\n",
      "EPOCH 346/2000\n",
      "Average loss: 0.7664\n",
      "EPOCH 347/2000\n",
      "Average loss: 0.7637\n",
      "EPOCH 348/2000\n",
      "Average loss: 0.7628\n",
      "EPOCH 349/2000\n",
      "Average loss: 0.7590\n",
      "EPOCH 350/2000\n",
      "Average loss: 0.7570\n",
      "EPOCH 351/2000\n",
      "Average loss: 0.7552\n",
      "EPOCH 352/2000\n",
      "Average loss: 0.7527\n",
      "EPOCH 353/2000\n",
      "Average loss: 0.7500\n",
      "EPOCH 354/2000\n",
      "Average loss: 0.7481\n",
      "EPOCH 355/2000\n",
      "Average loss: 0.7476\n",
      "EPOCH 356/2000\n",
      "Average loss: 0.7463\n",
      "EPOCH 357/2000\n",
      "Average loss: 0.7439\n",
      "EPOCH 358/2000\n",
      "Average loss: 0.7411\n",
      "EPOCH 359/2000\n",
      "Average loss: 0.7394\n",
      "EPOCH 360/2000\n",
      "Average loss: 0.7382\n",
      "EPOCH 361/2000\n",
      "Average loss: 0.7350\n",
      "EPOCH 362/2000\n",
      "Average loss: 0.7332\n",
      "EPOCH 363/2000\n",
      "Average loss: 0.7312\n",
      "EPOCH 364/2000\n",
      "Average loss: 0.7301\n",
      "EPOCH 365/2000\n",
      "Average loss: 0.7277\n",
      "EPOCH 366/2000\n",
      "Average loss: 0.7245\n",
      "EPOCH 367/2000\n",
      "Average loss: 0.7227\n",
      "EPOCH 368/2000\n",
      "Average loss: 0.7201\n",
      "EPOCH 369/2000\n",
      "Average loss: 0.7173\n",
      "EPOCH 370/2000\n",
      "Average loss: 0.7162\n",
      "EPOCH 371/2000\n",
      "Average loss: 0.7156\n",
      "EPOCH 372/2000\n",
      "Average loss: 0.7118\n",
      "EPOCH 373/2000\n",
      "Average loss: 0.7116\n",
      "EPOCH 374/2000\n",
      "Average loss: 0.7078\n",
      "EPOCH 375/2000\n",
      "Average loss: 0.7071\n",
      "EPOCH 376/2000\n",
      "Average loss: 0.7059\n",
      "EPOCH 377/2000\n",
      "Average loss: 0.7037\n",
      "EPOCH 378/2000\n",
      "Average loss: 0.6996\n",
      "EPOCH 379/2000\n",
      "Average loss: 0.6996\n",
      "EPOCH 380/2000\n",
      "Average loss: 0.6983\n",
      "EPOCH 381/2000\n",
      "Average loss: 0.6958\n",
      "EPOCH 382/2000\n",
      "Average loss: 0.6951\n",
      "EPOCH 383/2000\n",
      "Average loss: 0.6919\n",
      "EPOCH 384/2000\n",
      "Average loss: 0.6900\n",
      "EPOCH 385/2000\n",
      "Average loss: 0.6869\n",
      "EPOCH 386/2000\n",
      "Average loss: 0.6865\n",
      "EPOCH 387/2000\n",
      "Average loss: 0.6832\n",
      "EPOCH 388/2000\n",
      "Average loss: 0.6809\n",
      "EPOCH 389/2000\n",
      "Average loss: 0.6801\n",
      "EPOCH 390/2000\n",
      "Average loss: 0.6774\n",
      "EPOCH 391/2000\n",
      "Average loss: 0.6769\n",
      "EPOCH 392/2000\n",
      "Average loss: 0.6745\n",
      "EPOCH 393/2000\n",
      "Average loss: 0.6719\n",
      "EPOCH 394/2000\n",
      "Average loss: 0.6714\n",
      "EPOCH 395/2000\n",
      "Average loss: 0.6689\n",
      "EPOCH 396/2000\n",
      "Average loss: 0.6684\n",
      "EPOCH 397/2000\n",
      "Average loss: 0.6662\n",
      "EPOCH 398/2000\n",
      "Average loss: 0.6654\n",
      "EPOCH 399/2000\n",
      "Average loss: 0.6629\n",
      "EPOCH 400/2000\n",
      "Average loss: 0.6602\n",
      "EPOCH 401/2000\n",
      "Average loss: 0.6590\n",
      "EPOCH 402/2000\n",
      "Average loss: 0.6579\n",
      "EPOCH 403/2000\n",
      "Average loss: 0.6557\n",
      "EPOCH 404/2000\n",
      "Average loss: 0.6519\n",
      "EPOCH 405/2000\n",
      "Average loss: 0.6515\n",
      "EPOCH 406/2000\n",
      "Average loss: 0.6490\n",
      "EPOCH 407/2000\n",
      "Average loss: 0.6489\n",
      "EPOCH 408/2000\n",
      "Average loss: 0.6472\n",
      "EPOCH 409/2000\n",
      "Average loss: 0.6462\n",
      "EPOCH 410/2000\n",
      "Average loss: 0.6427\n",
      "EPOCH 411/2000\n",
      "Average loss: 0.6418\n",
      "EPOCH 412/2000\n",
      "Average loss: 0.6416\n",
      "EPOCH 413/2000\n",
      "Average loss: 0.6391\n",
      "EPOCH 414/2000\n",
      "Average loss: 0.6348\n",
      "EPOCH 415/2000\n",
      "Average loss: 0.6342\n",
      "EPOCH 416/2000\n",
      "Average loss: 0.6309\n",
      "EPOCH 417/2000\n",
      "Average loss: 0.6323\n",
      "EPOCH 418/2000\n",
      "Average loss: 0.6300\n",
      "EPOCH 419/2000\n",
      "Average loss: 0.6296\n",
      "EPOCH 420/2000\n",
      "Average loss: 0.6275\n",
      "EPOCH 421/2000\n",
      "Average loss: 0.6250\n",
      "EPOCH 422/2000\n",
      "Average loss: 0.6226\n",
      "EPOCH 423/2000\n",
      "Average loss: 0.6218\n",
      "EPOCH 424/2000\n",
      "Average loss: 0.6197\n",
      "EPOCH 425/2000\n",
      "Average loss: 0.6186\n",
      "EPOCH 426/2000\n",
      "Average loss: 0.6168\n",
      "EPOCH 427/2000\n",
      "Average loss: 0.6171\n",
      "EPOCH 428/2000\n",
      "Average loss: 0.6152\n",
      "EPOCH 429/2000\n",
      "Average loss: 0.6119\n",
      "EPOCH 430/2000\n",
      "Average loss: 0.6111\n",
      "EPOCH 431/2000\n",
      "Average loss: 0.6099\n",
      "EPOCH 432/2000\n",
      "Average loss: 0.6085\n",
      "EPOCH 433/2000\n",
      "Average loss: 0.6054\n",
      "EPOCH 434/2000\n",
      "Average loss: 0.6037\n",
      "EPOCH 435/2000\n",
      "Average loss: 0.6024\n",
      "EPOCH 436/2000\n",
      "Average loss: 0.6029\n",
      "EPOCH 437/2000\n",
      "Average loss: 0.6005\n",
      "EPOCH 438/2000\n",
      "Average loss: 0.5982\n",
      "EPOCH 439/2000\n",
      "Average loss: 0.5972\n",
      "EPOCH 440/2000\n",
      "Average loss: 0.5965\n",
      "EPOCH 441/2000\n",
      "Average loss: 0.5936\n",
      "EPOCH 442/2000\n",
      "Average loss: 0.5934\n",
      "EPOCH 443/2000\n",
      "Average loss: 0.5926\n",
      "EPOCH 444/2000\n",
      "Average loss: 0.5889\n",
      "EPOCH 445/2000\n",
      "Average loss: 0.5874\n",
      "EPOCH 446/2000\n",
      "Average loss: 0.5867\n",
      "EPOCH 447/2000\n",
      "Average loss: 0.5871\n",
      "EPOCH 448/2000\n",
      "Average loss: 0.5831\n",
      "EPOCH 449/2000\n",
      "Average loss: 0.5824\n",
      "EPOCH 450/2000\n",
      "Average loss: 0.5800\n",
      "EPOCH 451/2000\n",
      "Average loss: 0.5790\n",
      "EPOCH 452/2000\n",
      "Average loss: 0.5783\n",
      "EPOCH 453/2000\n",
      "Average loss: 0.5761\n",
      "EPOCH 454/2000\n",
      "Average loss: 0.5738\n",
      "EPOCH 455/2000\n",
      "Average loss: 0.5737\n",
      "EPOCH 456/2000\n",
      "Average loss: 0.5730\n",
      "EPOCH 457/2000\n",
      "Average loss: 0.5701\n",
      "EPOCH 458/2000\n",
      "Average loss: 0.5689\n",
      "EPOCH 459/2000\n",
      "Average loss: 0.5688\n",
      "EPOCH 460/2000\n",
      "Average loss: 0.5665\n",
      "EPOCH 461/2000\n",
      "Average loss: 0.5653\n",
      "EPOCH 462/2000\n",
      "Average loss: 0.5638\n",
      "EPOCH 463/2000\n",
      "Average loss: 0.5635\n",
      "EPOCH 464/2000\n",
      "Average loss: 0.5611\n",
      "EPOCH 465/2000\n",
      "Average loss: 0.5611\n",
      "EPOCH 466/2000\n",
      "Average loss: 0.5581\n",
      "EPOCH 467/2000\n",
      "Average loss: 0.5568\n",
      "EPOCH 468/2000\n",
      "Average loss: 0.5561\n",
      "EPOCH 469/2000\n",
      "Average loss: 0.5545\n",
      "EPOCH 470/2000\n",
      "Average loss: 0.5539\n",
      "EPOCH 471/2000\n",
      "Average loss: 0.5519\n",
      "EPOCH 472/2000\n",
      "Average loss: 0.5515\n",
      "EPOCH 473/2000\n",
      "Average loss: 0.5500\n",
      "EPOCH 474/2000\n",
      "Average loss: 0.5485\n",
      "EPOCH 475/2000\n",
      "Average loss: 0.5472\n",
      "EPOCH 476/2000\n",
      "Average loss: 0.5458\n",
      "EPOCH 477/2000\n",
      "Average loss: 0.5431\n",
      "EPOCH 478/2000\n",
      "Average loss: 0.5436\n",
      "EPOCH 479/2000\n",
      "Average loss: 0.5409\n",
      "EPOCH 480/2000\n",
      "Average loss: 0.5398\n",
      "EPOCH 481/2000\n",
      "Average loss: 0.5391\n",
      "EPOCH 482/2000\n",
      "Average loss: 0.5376\n",
      "EPOCH 483/2000\n",
      "Average loss: 0.5363\n",
      "EPOCH 484/2000\n",
      "Average loss: 0.5349\n",
      "EPOCH 485/2000\n",
      "Average loss: 0.5335\n",
      "EPOCH 486/2000\n",
      "Average loss: 0.5333\n",
      "EPOCH 487/2000\n",
      "Average loss: 0.5319\n",
      "EPOCH 488/2000\n",
      "Average loss: 0.5307\n",
      "EPOCH 489/2000\n",
      "Average loss: 0.5294\n",
      "EPOCH 490/2000\n",
      "Average loss: 0.5291\n",
      "EPOCH 491/2000\n",
      "Average loss: 0.5269\n",
      "EPOCH 492/2000\n",
      "Average loss: 0.5255\n",
      "EPOCH 493/2000\n",
      "Average loss: 0.5236\n",
      "EPOCH 494/2000\n",
      "Average loss: 0.5239\n",
      "EPOCH 495/2000\n",
      "Average loss: 0.5217\n",
      "EPOCH 496/2000\n",
      "Average loss: 0.5199\n",
      "EPOCH 497/2000\n",
      "Average loss: 0.5180\n",
      "EPOCH 498/2000\n",
      "Average loss: 0.5186\n",
      "EPOCH 499/2000\n",
      "Average loss: 0.5181\n",
      "EPOCH 500/2000\n",
      "Average loss: 0.5136\n",
      "EPOCH 501/2000\n",
      "Average loss: 0.5137\n",
      "EPOCH 502/2000\n",
      "Average loss: 0.5122\n",
      "EPOCH 503/2000\n",
      "Average loss: 0.5107\n",
      "EPOCH 504/2000\n",
      "Average loss: 0.5099\n",
      "EPOCH 505/2000\n",
      "Average loss: 0.5097\n",
      "EPOCH 506/2000\n",
      "Average loss: 0.5081\n",
      "EPOCH 507/2000\n",
      "Average loss: 0.5077\n",
      "EPOCH 508/2000\n",
      "Average loss: 0.5070\n",
      "EPOCH 509/2000\n",
      "Average loss: 0.5053\n",
      "EPOCH 510/2000\n",
      "Average loss: 0.5041\n",
      "EPOCH 511/2000\n",
      "Average loss: 0.5045\n",
      "EPOCH 512/2000\n",
      "Average loss: 0.5031\n",
      "EPOCH 513/2000\n",
      "Average loss: 0.5014\n",
      "EPOCH 514/2000\n",
      "Average loss: 0.5003\n",
      "EPOCH 515/2000\n",
      "Average loss: 0.4996\n",
      "EPOCH 516/2000\n",
      "Average loss: 0.4975\n",
      "EPOCH 517/2000\n",
      "Average loss: 0.4956\n",
      "EPOCH 518/2000\n",
      "Average loss: 0.4952\n",
      "EPOCH 519/2000\n",
      "Average loss: 0.4957\n",
      "EPOCH 520/2000\n",
      "Average loss: 0.4943\n",
      "EPOCH 521/2000\n",
      "Average loss: 0.4922\n",
      "EPOCH 522/2000\n",
      "Average loss: 0.4913\n",
      "EPOCH 523/2000\n",
      "Average loss: 0.4895\n",
      "EPOCH 524/2000\n",
      "Average loss: 0.4891\n",
      "EPOCH 525/2000\n",
      "Average loss: 0.4872\n",
      "EPOCH 526/2000\n",
      "Average loss: 0.4864\n",
      "EPOCH 527/2000\n",
      "Average loss: 0.4854\n",
      "EPOCH 528/2000\n",
      "Average loss: 0.4841\n",
      "EPOCH 529/2000\n",
      "Average loss: 0.4827\n",
      "EPOCH 530/2000\n",
      "Average loss: 0.4840\n",
      "EPOCH 531/2000\n",
      "Average loss: 0.4811\n",
      "EPOCH 532/2000\n",
      "Average loss: 0.4801\n",
      "EPOCH 533/2000\n",
      "Average loss: 0.4790\n",
      "EPOCH 534/2000\n",
      "Average loss: 0.4785\n",
      "EPOCH 535/2000\n",
      "Average loss: 0.4791\n",
      "EPOCH 536/2000\n",
      "Average loss: 0.4761\n",
      "EPOCH 537/2000\n",
      "Average loss: 0.4749\n",
      "EPOCH 538/2000\n",
      "Average loss: 0.4753\n",
      "EPOCH 539/2000\n",
      "Average loss: 0.4744\n",
      "EPOCH 540/2000\n",
      "Average loss: 0.4729\n",
      "EPOCH 541/2000\n",
      "Average loss: 0.4720\n",
      "EPOCH 542/2000\n",
      "Average loss: 0.4703\n",
      "EPOCH 543/2000\n",
      "Average loss: 0.4700\n",
      "EPOCH 544/2000\n",
      "Average loss: 0.4690\n",
      "EPOCH 545/2000\n",
      "Average loss: 0.4676\n",
      "EPOCH 546/2000\n",
      "Average loss: 0.4670\n",
      "EPOCH 547/2000\n",
      "Average loss: 0.4662\n",
      "EPOCH 548/2000\n",
      "Average loss: 0.4645\n",
      "EPOCH 549/2000\n",
      "Average loss: 0.4622\n",
      "EPOCH 550/2000\n",
      "Average loss: 0.4632\n",
      "EPOCH 551/2000\n",
      "Average loss: 0.4632\n",
      "EPOCH 552/2000\n",
      "Average loss: 0.4616\n",
      "EPOCH 553/2000\n",
      "Average loss: 0.4607\n",
      "EPOCH 554/2000\n",
      "Average loss: 0.4601\n",
      "EPOCH 555/2000\n",
      "Average loss: 0.4565\n",
      "EPOCH 556/2000\n",
      "Average loss: 0.4564\n",
      "EPOCH 557/2000\n",
      "Average loss: 0.4561\n",
      "EPOCH 558/2000\n",
      "Average loss: 0.4559\n",
      "EPOCH 559/2000\n",
      "Average loss: 0.4527\n",
      "EPOCH 560/2000\n",
      "Average loss: 0.4544\n",
      "EPOCH 561/2000\n",
      "Average loss: 0.4530\n",
      "EPOCH 562/2000\n",
      "Average loss: 0.4509\n",
      "EPOCH 563/2000\n",
      "Average loss: 0.4488\n",
      "EPOCH 564/2000\n",
      "Average loss: 0.4491\n",
      "EPOCH 565/2000\n",
      "Average loss: 0.4475\n",
      "EPOCH 566/2000\n",
      "Average loss: 0.4472\n",
      "EPOCH 567/2000\n",
      "Average loss: 0.4465\n",
      "EPOCH 568/2000\n",
      "Average loss: 0.4453\n",
      "EPOCH 569/2000\n",
      "Average loss: 0.4456\n",
      "EPOCH 570/2000\n",
      "Average loss: 0.4444\n",
      "EPOCH 571/2000\n",
      "Average loss: 0.4427\n",
      "EPOCH 572/2000\n",
      "Average loss: 0.4418\n",
      "EPOCH 573/2000\n",
      "Average loss: 0.4424\n",
      "EPOCH 574/2000\n",
      "Average loss: 0.4415\n",
      "EPOCH 575/2000\n",
      "Average loss: 0.4396\n",
      "EPOCH 576/2000\n",
      "Average loss: 0.4393\n",
      "EPOCH 577/2000\n",
      "Average loss: 0.4389\n",
      "EPOCH 578/2000\n",
      "Average loss: 0.4359\n",
      "EPOCH 579/2000\n",
      "Average loss: 0.4359\n",
      "EPOCH 580/2000\n",
      "Average loss: 0.4354\n",
      "EPOCH 581/2000\n",
      "Average loss: 0.4337\n",
      "EPOCH 582/2000\n",
      "Average loss: 0.4335\n",
      "EPOCH 583/2000\n",
      "Average loss: 0.4319\n",
      "EPOCH 584/2000\n",
      "Average loss: 0.4331\n",
      "EPOCH 585/2000\n",
      "Average loss: 0.4315\n",
      "EPOCH 586/2000\n",
      "Average loss: 0.4304\n",
      "EPOCH 587/2000\n",
      "Average loss: 0.4296\n",
      "EPOCH 588/2000\n",
      "Average loss: 0.4281\n",
      "EPOCH 589/2000\n",
      "Average loss: 0.4283\n",
      "EPOCH 590/2000\n",
      "Average loss: 0.4266\n",
      "EPOCH 591/2000\n",
      "Average loss: 0.4251\n",
      "EPOCH 592/2000\n",
      "Average loss: 0.4249\n",
      "EPOCH 593/2000\n",
      "Average loss: 0.4251\n",
      "EPOCH 594/2000\n",
      "Average loss: 0.4250\n",
      "EPOCH 595/2000\n",
      "Average loss: 0.4238\n",
      "EPOCH 596/2000\n",
      "Average loss: 0.4224\n",
      "EPOCH 597/2000\n",
      "Average loss: 0.4220\n",
      "EPOCH 598/2000\n",
      "Average loss: 0.4196\n",
      "EPOCH 599/2000\n",
      "Average loss: 0.4187\n",
      "EPOCH 600/2000\n",
      "Average loss: 0.4173\n",
      "EPOCH 601/2000\n",
      "Average loss: 0.4160\n",
      "EPOCH 602/2000\n",
      "Average loss: 0.4158\n",
      "EPOCH 603/2000\n",
      "Average loss: 0.4159\n",
      "EPOCH 604/2000\n",
      "Average loss: 0.4166\n",
      "EPOCH 605/2000\n",
      "Average loss: 0.4157\n",
      "EPOCH 606/2000\n",
      "Average loss: 0.4141\n",
      "EPOCH 607/2000\n",
      "Average loss: 0.4128\n",
      "EPOCH 608/2000\n",
      "Average loss: 0.4121\n",
      "EPOCH 609/2000\n",
      "Average loss: 0.4118\n",
      "EPOCH 610/2000\n",
      "Average loss: 0.4105\n",
      "EPOCH 611/2000\n",
      "Average loss: 0.4111\n",
      "EPOCH 612/2000\n",
      "Average loss: 0.4095\n",
      "EPOCH 613/2000\n",
      "Average loss: 0.4083\n",
      "EPOCH 614/2000\n",
      "Average loss: 0.4090\n",
      "EPOCH 615/2000\n",
      "Average loss: 0.4068\n",
      "EPOCH 616/2000\n",
      "Average loss: 0.4057\n",
      "EPOCH 617/2000\n",
      "Average loss: 0.4055\n",
      "EPOCH 618/2000\n",
      "Average loss: 0.4049\n",
      "EPOCH 619/2000\n",
      "Average loss: 0.4043\n",
      "EPOCH 620/2000\n",
      "Average loss: 0.4037\n",
      "EPOCH 621/2000\n",
      "Average loss: 0.4037\n",
      "EPOCH 622/2000\n",
      "Average loss: 0.4017\n",
      "EPOCH 623/2000\n",
      "Average loss: 0.4013\n",
      "EPOCH 624/2000\n",
      "Average loss: 0.4016\n",
      "EPOCH 625/2000\n",
      "Average loss: 0.3990\n",
      "EPOCH 626/2000\n",
      "Average loss: 0.3987\n",
      "EPOCH 627/2000\n",
      "Average loss: 0.3976\n",
      "EPOCH 628/2000\n",
      "Average loss: 0.3955\n",
      "EPOCH 629/2000\n",
      "Average loss: 0.3966\n",
      "EPOCH 630/2000\n",
      "Average loss: 0.3951\n",
      "EPOCH 631/2000\n",
      "Average loss: 0.3953\n",
      "EPOCH 632/2000\n",
      "Average loss: 0.3941\n",
      "EPOCH 633/2000\n",
      "Average loss: 0.3934\n",
      "EPOCH 634/2000\n",
      "Average loss: 0.3929\n",
      "EPOCH 635/2000\n",
      "Average loss: 0.3920\n",
      "EPOCH 636/2000\n",
      "Average loss: 0.3912\n",
      "EPOCH 637/2000\n",
      "Average loss: 0.3908\n",
      "EPOCH 638/2000\n",
      "Average loss: 0.3909\n",
      "EPOCH 639/2000\n",
      "Average loss: 0.3895\n",
      "EPOCH 640/2000\n",
      "Average loss: 0.3884\n",
      "EPOCH 641/2000\n",
      "Average loss: 0.3876\n",
      "EPOCH 642/2000\n",
      "Average loss: 0.3887\n",
      "EPOCH 643/2000\n",
      "Average loss: 0.3880\n",
      "EPOCH 644/2000\n",
      "Average loss: 0.3862\n",
      "EPOCH 645/2000\n",
      "Average loss: 0.3858\n",
      "EPOCH 646/2000\n",
      "Average loss: 0.3856\n",
      "EPOCH 647/2000\n",
      "Average loss: 0.3856\n",
      "EPOCH 648/2000\n",
      "Average loss: 0.3843\n",
      "EPOCH 649/2000\n",
      "Average loss: 0.3830\n",
      "EPOCH 650/2000\n",
      "Average loss: 0.3816\n",
      "EPOCH 651/2000\n",
      "Average loss: 0.3822\n",
      "EPOCH 652/2000\n",
      "Average loss: 0.3827\n",
      "EPOCH 653/2000\n",
      "Average loss: 0.3821\n",
      "EPOCH 654/2000\n",
      "Average loss: 0.3800\n",
      "EPOCH 655/2000\n",
      "Average loss: 0.3791\n",
      "EPOCH 656/2000\n",
      "Average loss: 0.3784\n",
      "EPOCH 657/2000\n",
      "Average loss: 0.3775\n",
      "EPOCH 658/2000\n",
      "Average loss: 0.3784\n",
      "EPOCH 659/2000\n",
      "Average loss: 0.3781\n",
      "EPOCH 660/2000\n",
      "Average loss: 0.3764\n",
      "EPOCH 661/2000\n",
      "Average loss: 0.3759\n",
      "EPOCH 662/2000\n",
      "Average loss: 0.3760\n",
      "EPOCH 663/2000\n",
      "Average loss: 0.3747\n",
      "EPOCH 664/2000\n",
      "Average loss: 0.3742\n",
      "EPOCH 665/2000\n",
      "Average loss: 0.3727\n",
      "EPOCH 666/2000\n",
      "Average loss: 0.3713\n",
      "EPOCH 667/2000\n",
      "Average loss: 0.3698\n",
      "EPOCH 668/2000\n",
      "Average loss: 0.3720\n",
      "EPOCH 669/2000\n",
      "Average loss: 0.3704\n",
      "EPOCH 670/2000\n",
      "Average loss: 0.3712\n",
      "EPOCH 671/2000\n",
      "Average loss: 0.3682\n",
      "EPOCH 672/2000\n",
      "Average loss: 0.3693\n",
      "EPOCH 673/2000\n",
      "Average loss: 0.3685\n",
      "EPOCH 674/2000\n",
      "Average loss: 0.3677\n",
      "EPOCH 675/2000\n",
      "Average loss: 0.3665\n",
      "EPOCH 676/2000\n",
      "Average loss: 0.3674\n",
      "EPOCH 677/2000\n",
      "Average loss: 0.3668\n",
      "EPOCH 678/2000\n",
      "Average loss: 0.3660\n",
      "EPOCH 679/2000\n",
      "Average loss: 0.3649\n",
      "EPOCH 680/2000\n",
      "Average loss: 0.3632\n",
      "EPOCH 681/2000\n",
      "Average loss: 0.3634\n",
      "EPOCH 682/2000\n",
      "Average loss: 0.3618\n",
      "EPOCH 683/2000\n",
      "Average loss: 0.3606\n",
      "EPOCH 684/2000\n",
      "Average loss: 0.3613\n",
      "EPOCH 685/2000\n",
      "Average loss: 0.3605\n",
      "EPOCH 686/2000\n",
      "Average loss: 0.3601\n",
      "EPOCH 687/2000\n",
      "Average loss: 0.3602\n",
      "EPOCH 688/2000\n",
      "Average loss: 0.3597\n",
      "EPOCH 689/2000\n",
      "Average loss: 0.3575\n",
      "EPOCH 690/2000\n",
      "Average loss: 0.3568\n",
      "EPOCH 691/2000\n",
      "Average loss: 0.3583\n",
      "EPOCH 692/2000\n",
      "Average loss: 0.3561\n",
      "EPOCH 693/2000\n",
      "Average loss: 0.3561\n",
      "EPOCH 694/2000\n",
      "Average loss: 0.3556\n",
      "EPOCH 695/2000\n",
      "Average loss: 0.3540\n",
      "EPOCH 696/2000\n",
      "Average loss: 0.3540\n",
      "EPOCH 697/2000\n",
      "Average loss: 0.3534\n",
      "EPOCH 698/2000\n",
      "Average loss: 0.3519\n",
      "EPOCH 699/2000\n",
      "Average loss: 0.3538\n",
      "EPOCH 700/2000\n",
      "Average loss: 0.3520\n",
      "EPOCH 701/2000\n",
      "Average loss: 0.3510\n",
      "EPOCH 702/2000\n",
      "Average loss: 0.3511\n",
      "EPOCH 703/2000\n",
      "Average loss: 0.3505\n",
      "EPOCH 704/2000\n",
      "Average loss: 0.3501\n",
      "EPOCH 705/2000\n",
      "Average loss: 0.3494\n",
      "EPOCH 706/2000\n",
      "Average loss: 0.3499\n",
      "EPOCH 707/2000\n",
      "Average loss: 0.3485\n",
      "EPOCH 708/2000\n",
      "Average loss: 0.3475\n",
      "EPOCH 709/2000\n",
      "Average loss: 0.3484\n",
      "EPOCH 710/2000\n",
      "Average loss: 0.3463\n",
      "EPOCH 711/2000\n",
      "Average loss: 0.3465\n",
      "EPOCH 712/2000\n",
      "Average loss: 0.3455\n",
      "EPOCH 713/2000\n",
      "Average loss: 0.3439\n",
      "EPOCH 714/2000\n",
      "Average loss: 0.3442\n",
      "EPOCH 715/2000\n",
      "Average loss: 0.3438\n",
      "EPOCH 716/2000\n",
      "Average loss: 0.3429\n",
      "EPOCH 717/2000\n",
      "Average loss: 0.3434\n",
      "EPOCH 718/2000\n",
      "Average loss: 0.3427\n",
      "EPOCH 719/2000\n",
      "Average loss: 0.3420\n",
      "EPOCH 720/2000\n",
      "Average loss: 0.3420\n",
      "EPOCH 721/2000\n",
      "Average loss: 0.3415\n",
      "EPOCH 722/2000\n",
      "Average loss: 0.3418\n",
      "EPOCH 723/2000\n",
      "Average loss: 0.3384\n",
      "EPOCH 724/2000\n",
      "Average loss: 0.3394\n",
      "EPOCH 725/2000\n",
      "Average loss: 0.3388\n",
      "EPOCH 726/2000\n",
      "Average loss: 0.3376\n",
      "EPOCH 727/2000\n",
      "Average loss: 0.3377\n",
      "EPOCH 728/2000\n",
      "Average loss: 0.3383\n",
      "EPOCH 729/2000\n",
      "Average loss: 0.3366\n",
      "EPOCH 730/2000\n",
      "Average loss: 0.3361\n",
      "EPOCH 731/2000\n",
      "Average loss: 0.3362\n",
      "EPOCH 732/2000\n",
      "Average loss: 0.3370\n",
      "EPOCH 733/2000\n",
      "Average loss: 0.3351\n",
      "EPOCH 734/2000\n",
      "Average loss: 0.3353\n",
      "EPOCH 735/2000\n",
      "Average loss: 0.3342\n",
      "EPOCH 736/2000\n",
      "Average loss: 0.3335\n",
      "EPOCH 737/2000\n",
      "Average loss: 0.3341\n",
      "EPOCH 738/2000\n",
      "Average loss: 0.3333\n",
      "EPOCH 739/2000\n",
      "Average loss: 0.3321\n",
      "EPOCH 740/2000\n",
      "Average loss: 0.3309\n",
      "EPOCH 741/2000\n",
      "Average loss: 0.3319\n",
      "EPOCH 742/2000\n",
      "Average loss: 0.3315\n",
      "EPOCH 743/2000\n",
      "Average loss: 0.3295\n",
      "EPOCH 744/2000\n",
      "Average loss: 0.3312\n",
      "EPOCH 745/2000\n",
      "Average loss: 0.3282\n",
      "EPOCH 746/2000\n",
      "Average loss: 0.3274\n",
      "EPOCH 747/2000\n",
      "Average loss: 0.3257\n",
      "EPOCH 748/2000\n",
      "Average loss: 0.3281\n",
      "EPOCH 749/2000\n",
      "Average loss: 0.3269\n",
      "EPOCH 750/2000\n",
      "Average loss: 0.3271\n",
      "EPOCH 751/2000\n",
      "Average loss: 0.3261\n",
      "EPOCH 752/2000\n",
      "Average loss: 0.3247\n",
      "EPOCH 753/2000\n",
      "Average loss: 0.3258\n",
      "EPOCH 754/2000\n",
      "Average loss: 0.3245\n",
      "EPOCH 755/2000\n",
      "Average loss: 0.3247\n",
      "EPOCH 756/2000\n",
      "Average loss: 0.3239\n",
      "EPOCH 757/2000\n",
      "Average loss: 0.3242\n",
      "EPOCH 758/2000\n",
      "Average loss: 0.3238\n",
      "EPOCH 759/2000\n",
      "Average loss: 0.3232\n",
      "EPOCH 760/2000\n",
      "Average loss: 0.3211\n",
      "EPOCH 761/2000\n",
      "Average loss: 0.3215\n",
      "EPOCH 762/2000\n",
      "Average loss: 0.3212\n",
      "EPOCH 763/2000\n",
      "Average loss: 0.3204\n",
      "EPOCH 764/2000\n",
      "Average loss: 0.3200\n",
      "EPOCH 765/2000\n",
      "Average loss: 0.3206\n",
      "EPOCH 766/2000\n",
      "Average loss: 0.3192\n",
      "EPOCH 767/2000\n",
      "Average loss: 0.3184\n",
      "EPOCH 768/2000\n",
      "Average loss: 0.3185\n",
      "EPOCH 769/2000\n",
      "Average loss: 0.3188\n",
      "EPOCH 770/2000\n",
      "Average loss: 0.3176\n",
      "EPOCH 771/2000\n",
      "Average loss: 0.3170\n",
      "EPOCH 772/2000\n",
      "Average loss: 0.3168\n",
      "EPOCH 773/2000\n",
      "Average loss: 0.3160\n",
      "EPOCH 774/2000\n",
      "Average loss: 0.3165\n",
      "EPOCH 775/2000\n",
      "Average loss: 0.3155\n",
      "EPOCH 776/2000\n",
      "Average loss: 0.3157\n",
      "EPOCH 777/2000\n",
      "Average loss: 0.3165\n",
      "EPOCH 778/2000\n",
      "Average loss: 0.3135\n",
      "EPOCH 779/2000\n",
      "Average loss: 0.3135\n",
      "EPOCH 780/2000\n",
      "Average loss: 0.3134\n",
      "EPOCH 781/2000\n",
      "Average loss: 0.3126\n",
      "EPOCH 782/2000\n",
      "Average loss: 0.3127\n",
      "EPOCH 783/2000\n",
      "Average loss: 0.3129\n",
      "EPOCH 784/2000\n",
      "Average loss: 0.3120\n",
      "EPOCH 785/2000\n",
      "Average loss: 0.3107\n",
      "EPOCH 786/2000\n",
      "Average loss: 0.3111\n",
      "EPOCH 787/2000\n",
      "Average loss: 0.3110\n",
      "EPOCH 788/2000\n",
      "Average loss: 0.3102\n",
      "EPOCH 789/2000\n",
      "Average loss: 0.3100\n",
      "EPOCH 790/2000\n",
      "Average loss: 0.3098\n",
      "EPOCH 791/2000\n",
      "Average loss: 0.3081\n",
      "EPOCH 792/2000\n",
      "Average loss: 0.3087\n",
      "EPOCH 793/2000\n",
      "Average loss: 0.3082\n",
      "EPOCH 794/2000\n",
      "Average loss: 0.3079\n",
      "EPOCH 795/2000\n",
      "Average loss: 0.3064\n",
      "EPOCH 796/2000\n",
      "Average loss: 0.3071\n",
      "EPOCH 797/2000\n",
      "Average loss: 0.3088\n",
      "EPOCH 798/2000\n",
      "Average loss: 0.3068\n",
      "EPOCH 799/2000\n",
      "Average loss: 0.3056\n",
      "EPOCH 800/2000\n",
      "Average loss: 0.3050\n",
      "EPOCH 801/2000\n",
      "Average loss: 0.3041\n",
      "EPOCH 802/2000\n",
      "Average loss: 0.3030\n",
      "EPOCH 803/2000\n",
      "Average loss: 0.3040\n",
      "EPOCH 804/2000\n",
      "Average loss: 0.3037\n",
      "EPOCH 805/2000\n",
      "Average loss: 0.3025\n",
      "EPOCH 806/2000\n",
      "Average loss: 0.3024\n",
      "EPOCH 807/2000\n",
      "Average loss: 0.3025\n",
      "EPOCH 808/2000\n",
      "Average loss: 0.3021\n",
      "EPOCH 809/2000\n",
      "Average loss: 0.3010\n",
      "EPOCH 810/2000\n",
      "Average loss: 0.3004\n",
      "EPOCH 811/2000\n",
      "Average loss: 0.3006\n",
      "EPOCH 812/2000\n",
      "Average loss: 0.3003\n",
      "EPOCH 813/2000\n",
      "Average loss: 0.3003\n",
      "EPOCH 814/2000\n",
      "Average loss: 0.3010\n",
      "EPOCH 815/2000\n",
      "Average loss: 0.2986\n",
      "EPOCH 816/2000\n",
      "Average loss: 0.2974\n",
      "EPOCH 817/2000\n",
      "Average loss: 0.2970\n",
      "EPOCH 818/2000\n",
      "Average loss: 0.2988\n",
      "EPOCH 819/2000\n",
      "Average loss: 0.2972\n",
      "EPOCH 820/2000\n",
      "Average loss: 0.2974\n",
      "EPOCH 821/2000\n",
      "Average loss: 0.2980\n",
      "EPOCH 822/2000\n",
      "Average loss: 0.2963\n",
      "EPOCH 823/2000\n",
      "Average loss: 0.2967\n",
      "EPOCH 824/2000\n",
      "Average loss: 0.2962\n",
      "EPOCH 825/2000\n",
      "Average loss: 0.2960\n",
      "EPOCH 826/2000\n",
      "Average loss: 0.2939\n",
      "EPOCH 827/2000\n",
      "Average loss: 0.2940\n",
      "EPOCH 828/2000\n",
      "Average loss: 0.2953\n",
      "EPOCH 829/2000\n",
      "Average loss: 0.2939\n",
      "EPOCH 830/2000\n",
      "Average loss: 0.2936\n",
      "EPOCH 831/2000\n",
      "Average loss: 0.2938\n",
      "EPOCH 832/2000\n",
      "Average loss: 0.2946\n",
      "EPOCH 833/2000\n",
      "Average loss: 0.2940\n",
      "EPOCH 834/2000\n",
      "Average loss: 0.2922\n",
      "EPOCH 835/2000\n",
      "Average loss: 0.2927\n",
      "EPOCH 836/2000\n",
      "Average loss: 0.2916\n",
      "EPOCH 837/2000\n",
      "Average loss: 0.2913\n",
      "EPOCH 838/2000\n",
      "Average loss: 0.2905\n",
      "EPOCH 839/2000\n",
      "Average loss: 0.2910\n",
      "EPOCH 840/2000\n",
      "Average loss: 0.2909\n",
      "EPOCH 841/2000\n",
      "Average loss: 0.2899\n",
      "EPOCH 842/2000\n",
      "Average loss: 0.2896\n",
      "EPOCH 843/2000\n",
      "Average loss: 0.2897\n",
      "EPOCH 844/2000\n",
      "Average loss: 0.2887\n",
      "EPOCH 845/2000\n",
      "Average loss: 0.2897\n",
      "EPOCH 846/2000\n",
      "Average loss: 0.2877\n",
      "EPOCH 847/2000\n",
      "Average loss: 0.2870\n",
      "EPOCH 848/2000\n",
      "Average loss: 0.2878\n",
      "EPOCH 849/2000\n",
      "Average loss: 0.2871\n",
      "EPOCH 850/2000\n",
      "Average loss: 0.2888\n",
      "EPOCH 851/2000\n",
      "Average loss: 0.2864\n",
      "EPOCH 852/2000\n",
      "Average loss: 0.2847\n",
      "EPOCH 853/2000\n",
      "Average loss: 0.2850\n",
      "EPOCH 854/2000\n",
      "Average loss: 0.2869\n",
      "EPOCH 855/2000\n",
      "Average loss: 0.2856\n",
      "EPOCH 856/2000\n",
      "Average loss: 0.2843\n",
      "EPOCH 857/2000\n",
      "Average loss: 0.2845\n",
      "EPOCH 858/2000\n",
      "Average loss: 0.2840\n",
      "EPOCH 859/2000\n",
      "Average loss: 0.2838\n",
      "EPOCH 860/2000\n",
      "Average loss: 0.2846\n",
      "EPOCH 861/2000\n",
      "Average loss: 0.2843\n",
      "EPOCH 862/2000\n",
      "Average loss: 0.2826\n",
      "EPOCH 863/2000\n",
      "Average loss: 0.2842\n",
      "EPOCH 864/2000\n",
      "Average loss: 0.2827\n",
      "EPOCH 865/2000\n",
      "Average loss: 0.2825\n",
      "EPOCH 866/2000\n",
      "Average loss: 0.2818\n",
      "EPOCH 867/2000\n",
      "Average loss: 0.2830\n",
      "EPOCH 868/2000\n",
      "Average loss: 0.2815\n",
      "EPOCH 869/2000\n",
      "Average loss: 0.2806\n",
      "EPOCH 870/2000\n",
      "Average loss: 0.2811\n",
      "EPOCH 871/2000\n",
      "Average loss: 0.2792\n",
      "EPOCH 872/2000\n",
      "Average loss: 0.2793\n",
      "EPOCH 873/2000\n",
      "Average loss: 0.2802\n",
      "EPOCH 874/2000\n",
      "Average loss: 0.2783\n",
      "EPOCH 875/2000\n",
      "Average loss: 0.2784\n",
      "EPOCH 876/2000\n",
      "Average loss: 0.2782\n",
      "EPOCH 877/2000\n",
      "Average loss: 0.2791\n",
      "EPOCH 878/2000\n",
      "Average loss: 0.2778\n",
      "EPOCH 879/2000\n",
      "Average loss: 0.2768\n",
      "EPOCH 880/2000\n",
      "Average loss: 0.2781\n",
      "EPOCH 881/2000\n",
      "Average loss: 0.2767\n",
      "EPOCH 882/2000\n",
      "Average loss: 0.2773\n",
      "EPOCH 883/2000\n",
      "Average loss: 0.2759\n",
      "EPOCH 884/2000\n",
      "Average loss: 0.2757\n",
      "EPOCH 885/2000\n",
      "Average loss: 0.2736\n",
      "EPOCH 886/2000\n",
      "Average loss: 0.2755\n",
      "EPOCH 887/2000\n",
      "Average loss: 0.2751\n",
      "EPOCH 888/2000\n",
      "Average loss: 0.2759\n",
      "EPOCH 889/2000\n",
      "Average loss: 0.2741\n",
      "EPOCH 890/2000\n",
      "Average loss: 0.2738\n",
      "EPOCH 891/2000\n",
      "Average loss: 0.2718\n",
      "EPOCH 892/2000\n",
      "Average loss: 0.2742\n",
      "EPOCH 893/2000\n",
      "Average loss: 0.2729\n",
      "EPOCH 894/2000\n",
      "Average loss: 0.2727\n",
      "EPOCH 895/2000\n",
      "Average loss: 0.2726\n",
      "EPOCH 896/2000\n",
      "Average loss: 0.2713\n",
      "EPOCH 897/2000\n",
      "Average loss: 0.2713\n",
      "EPOCH 898/2000\n",
      "Average loss: 0.2701\n",
      "EPOCH 899/2000\n",
      "Average loss: 0.2713\n",
      "EPOCH 900/2000\n",
      "Average loss: 0.2697\n",
      "EPOCH 901/2000\n",
      "Average loss: 0.2708\n",
      "EPOCH 902/2000\n",
      "Average loss: 0.2709\n",
      "EPOCH 903/2000\n",
      "Average loss: 0.2699\n",
      "EPOCH 904/2000\n",
      "Average loss: 0.2701\n",
      "EPOCH 905/2000\n",
      "Average loss: 0.2690\n",
      "EPOCH 906/2000\n",
      "Average loss: 0.2682\n",
      "EPOCH 907/2000\n",
      "Average loss: 0.2672\n",
      "EPOCH 908/2000\n",
      "Average loss: 0.2685\n",
      "EPOCH 909/2000\n",
      "Average loss: 0.2679\n",
      "EPOCH 910/2000\n",
      "Average loss: 0.2680\n",
      "EPOCH 911/2000\n",
      "Average loss: 0.2679\n",
      "EPOCH 912/2000\n",
      "Average loss: 0.2672\n",
      "EPOCH 913/2000\n",
      "Average loss: 0.2661\n",
      "EPOCH 914/2000\n",
      "Average loss: 0.2664\n",
      "EPOCH 915/2000\n",
      "Average loss: 0.2663\n",
      "EPOCH 916/2000\n",
      "Average loss: 0.2673\n",
      "EPOCH 917/2000\n",
      "Average loss: 0.2674\n",
      "EPOCH 918/2000\n",
      "Average loss: 0.2664\n",
      "EPOCH 919/2000\n",
      "Average loss: 0.2640\n",
      "EPOCH 920/2000\n",
      "Average loss: 0.2655\n",
      "EPOCH 921/2000\n",
      "Average loss: 0.2634\n",
      "EPOCH 922/2000\n",
      "Average loss: 0.2645\n",
      "EPOCH 923/2000\n",
      "Average loss: 0.2642\n",
      "EPOCH 924/2000\n",
      "Average loss: 0.2636\n",
      "EPOCH 925/2000\n",
      "Average loss: 0.2627\n",
      "EPOCH 926/2000\n",
      "Average loss: 0.2631\n",
      "EPOCH 927/2000\n",
      "Average loss: 0.2640\n",
      "EPOCH 928/2000\n",
      "Average loss: 0.2614\n",
      "EPOCH 929/2000\n",
      "Average loss: 0.2617\n",
      "EPOCH 930/2000\n",
      "Average loss: 0.2625\n",
      "EPOCH 931/2000\n",
      "Average loss: 0.2622\n",
      "EPOCH 932/2000\n",
      "Average loss: 0.2620\n",
      "EPOCH 933/2000\n",
      "Average loss: 0.2615\n",
      "EPOCH 934/2000\n",
      "Average loss: 0.2618\n",
      "EPOCH 935/2000\n",
      "Average loss: 0.2605\n",
      "EPOCH 936/2000\n",
      "Average loss: 0.2604\n",
      "EPOCH 937/2000\n",
      "Average loss: 0.2614\n",
      "EPOCH 938/2000\n",
      "Average loss: 0.2613\n",
      "EPOCH 939/2000\n",
      "Average loss: 0.2605\n",
      "EPOCH 940/2000\n",
      "Average loss: 0.2602\n",
      "EPOCH 941/2000\n",
      "Average loss: 0.2588\n",
      "EPOCH 942/2000\n",
      "Average loss: 0.2606\n",
      "EPOCH 943/2000\n",
      "Average loss: 0.2579\n",
      "EPOCH 944/2000\n",
      "Average loss: 0.2587\n",
      "EPOCH 945/2000\n",
      "Average loss: 0.2590\n",
      "EPOCH 946/2000\n",
      "Average loss: 0.2575\n",
      "EPOCH 947/2000\n",
      "Average loss: 0.2577\n",
      "EPOCH 948/2000\n",
      "Average loss: 0.2588\n",
      "EPOCH 949/2000\n",
      "Average loss: 0.2573\n",
      "EPOCH 950/2000\n",
      "Average loss: 0.2562\n",
      "EPOCH 951/2000\n",
      "Average loss: 0.2566\n",
      "EPOCH 952/2000\n",
      "Average loss: 0.2563\n",
      "EPOCH 953/2000\n",
      "Average loss: 0.2571\n",
      "EPOCH 954/2000\n",
      "Average loss: 0.2561\n",
      "EPOCH 955/2000\n",
      "Average loss: 0.2562\n",
      "EPOCH 956/2000\n",
      "Average loss: 0.2559\n",
      "EPOCH 957/2000\n",
      "Average loss: 0.2543\n",
      "EPOCH 958/2000\n",
      "Average loss: 0.2544\n",
      "EPOCH 959/2000\n",
      "Average loss: 0.2558\n",
      "EPOCH 960/2000\n",
      "Average loss: 0.2543\n",
      "EPOCH 961/2000\n",
      "Average loss: 0.2543\n",
      "EPOCH 962/2000\n",
      "Average loss: 0.2545\n",
      "EPOCH 963/2000\n",
      "Average loss: 0.2533\n",
      "EPOCH 964/2000\n",
      "Average loss: 0.2535\n",
      "EPOCH 965/2000\n",
      "Average loss: 0.2518\n",
      "EPOCH 966/2000\n",
      "Average loss: 0.2533\n",
      "EPOCH 967/2000\n",
      "Average loss: 0.2533\n",
      "EPOCH 968/2000\n",
      "Average loss: 0.2533\n",
      "EPOCH 969/2000\n",
      "Average loss: 0.2515\n",
      "EPOCH 970/2000\n",
      "Average loss: 0.2521\n",
      "EPOCH 971/2000\n",
      "Average loss: 0.2516\n",
      "EPOCH 972/2000\n",
      "Average loss: 0.2515\n",
      "EPOCH 973/2000\n",
      "Average loss: 0.2518\n",
      "EPOCH 974/2000\n",
      "Average loss: 0.2507\n",
      "EPOCH 975/2000\n",
      "Average loss: 0.2510\n",
      "EPOCH 976/2000\n",
      "Average loss: 0.2495\n",
      "EPOCH 977/2000\n",
      "Average loss: 0.2501\n",
      "EPOCH 978/2000\n",
      "Average loss: 0.2493\n",
      "EPOCH 979/2000\n",
      "Average loss: 0.2494\n",
      "EPOCH 980/2000\n",
      "Average loss: 0.2489\n",
      "EPOCH 981/2000\n",
      "Average loss: 0.2489\n",
      "EPOCH 982/2000\n",
      "Average loss: 0.2483\n",
      "EPOCH 983/2000\n",
      "Average loss: 0.2482\n",
      "EPOCH 984/2000\n",
      "Average loss: 0.2477\n",
      "EPOCH 985/2000\n",
      "Average loss: 0.2473\n",
      "EPOCH 986/2000\n",
      "Average loss: 0.2472\n",
      "EPOCH 987/2000\n",
      "Average loss: 0.2473\n",
      "EPOCH 988/2000\n",
      "Average loss: 0.2471\n",
      "EPOCH 989/2000\n",
      "Average loss: 0.2484\n",
      "EPOCH 990/2000\n",
      "Average loss: 0.2472\n",
      "EPOCH 991/2000\n",
      "Average loss: 0.2473\n",
      "EPOCH 992/2000\n",
      "Average loss: 0.2471\n",
      "EPOCH 993/2000\n",
      "Average loss: 0.2464\n",
      "EPOCH 994/2000\n",
      "Average loss: 0.2468\n",
      "EPOCH 995/2000\n",
      "Average loss: 0.2467\n",
      "EPOCH 996/2000\n",
      "Average loss: 0.2459\n",
      "EPOCH 997/2000\n",
      "Average loss: 0.2449\n",
      "EPOCH 998/2000\n",
      "Average loss: 0.2448\n",
      "EPOCH 999/2000\n",
      "Average loss: 0.2438\n",
      "EPOCH 1000/2000\n",
      "Average loss: 0.2444\n",
      "EPOCH 1001/2000\n",
      "Average loss: 0.2441\n",
      "EPOCH 1002/2000\n",
      "Average loss: 0.2449\n",
      "EPOCH 1003/2000\n",
      "Average loss: 0.2449\n",
      "EPOCH 1004/2000\n",
      "Average loss: 0.2438\n",
      "EPOCH 1005/2000\n",
      "Average loss: 0.2441\n",
      "EPOCH 1006/2000\n",
      "Average loss: 0.2431\n",
      "EPOCH 1007/2000\n",
      "Average loss: 0.2428\n",
      "EPOCH 1008/2000\n",
      "Average loss: 0.2430\n",
      "EPOCH 1009/2000\n",
      "Average loss: 0.2427\n",
      "EPOCH 1010/2000\n",
      "Average loss: 0.2433\n",
      "EPOCH 1011/2000\n",
      "Average loss: 0.2439\n",
      "EPOCH 1012/2000\n",
      "Average loss: 0.2428\n",
      "EPOCH 1013/2000\n",
      "Average loss: 0.2420\n",
      "EPOCH 1014/2000\n",
      "Average loss: 0.2408\n",
      "EPOCH 1015/2000\n",
      "Average loss: 0.2427\n",
      "EPOCH 1016/2000\n",
      "Average loss: 0.2418\n",
      "EPOCH 1017/2000\n",
      "Average loss: 0.2418\n",
      "EPOCH 1018/2000\n",
      "Average loss: 0.2400\n",
      "EPOCH 1019/2000\n",
      "Average loss: 0.2401\n",
      "EPOCH 1020/2000\n",
      "Average loss: 0.2396\n",
      "EPOCH 1021/2000\n",
      "Average loss: 0.2404\n",
      "EPOCH 1022/2000\n",
      "Average loss: 0.2387\n",
      "EPOCH 1023/2000\n",
      "Average loss: 0.2397\n",
      "EPOCH 1024/2000\n",
      "Average loss: 0.2402\n",
      "EPOCH 1025/2000\n",
      "Average loss: 0.2412\n",
      "EPOCH 1026/2000\n",
      "Average loss: 0.2397\n",
      "EPOCH 1027/2000\n",
      "Average loss: 0.2387\n",
      "EPOCH 1028/2000\n",
      "Average loss: 0.2382\n",
      "EPOCH 1029/2000\n",
      "Average loss: 0.2386\n",
      "EPOCH 1030/2000\n",
      "Average loss: 0.2376\n",
      "EPOCH 1031/2000\n",
      "Average loss: 0.2384\n",
      "EPOCH 1032/2000\n",
      "Average loss: 0.2386\n",
      "EPOCH 1033/2000\n",
      "Average loss: 0.2367\n",
      "EPOCH 1034/2000\n",
      "Average loss: 0.2380\n",
      "EPOCH 1035/2000\n",
      "Average loss: 0.2352\n",
      "EPOCH 1036/2000\n",
      "Average loss: 0.2371\n",
      "EPOCH 1037/2000\n",
      "Average loss: 0.2378\n",
      "EPOCH 1038/2000\n",
      "Average loss: 0.2373\n",
      "EPOCH 1039/2000\n",
      "Average loss: 0.2370\n",
      "EPOCH 1040/2000\n",
      "Average loss: 0.2364\n",
      "EPOCH 1041/2000\n",
      "Average loss: 0.2350\n",
      "EPOCH 1042/2000\n",
      "Average loss: 0.2341\n",
      "EPOCH 1043/2000\n",
      "Average loss: 0.2353\n",
      "EPOCH 1044/2000\n",
      "Average loss: 0.2357\n",
      "EPOCH 1045/2000\n",
      "Average loss: 0.2360\n",
      "EPOCH 1046/2000\n",
      "Average loss: 0.2337\n",
      "EPOCH 1047/2000\n",
      "Average loss: 0.2351\n",
      "EPOCH 1048/2000\n",
      "Average loss: 0.2338\n",
      "EPOCH 1049/2000\n",
      "Average loss: 0.2353\n",
      "EPOCH 1050/2000\n",
      "Average loss: 0.2343\n",
      "EPOCH 1051/2000\n",
      "Average loss: 0.2347\n",
      "EPOCH 1052/2000\n",
      "Average loss: 0.2339\n",
      "EPOCH 1053/2000\n",
      "Average loss: 0.2342\n",
      "EPOCH 1054/2000\n",
      "Average loss: 0.2339\n",
      "EPOCH 1055/2000\n",
      "Average loss: 0.2311\n",
      "EPOCH 1056/2000\n",
      "Average loss: 0.2330\n",
      "EPOCH 1057/2000\n",
      "Average loss: 0.2331\n",
      "EPOCH 1058/2000\n",
      "Average loss: 0.2310\n",
      "EPOCH 1059/2000\n",
      "Average loss: 0.2318\n",
      "EPOCH 1060/2000\n",
      "Average loss: 0.2313\n",
      "EPOCH 1061/2000\n",
      "Average loss: 0.2320\n",
      "EPOCH 1062/2000\n",
      "Average loss: 0.2318\n",
      "EPOCH 1063/2000\n",
      "Average loss: 0.2319\n",
      "EPOCH 1064/2000\n",
      "Average loss: 0.2319\n",
      "EPOCH 1065/2000\n",
      "Average loss: 0.2305\n",
      "EPOCH 1066/2000\n",
      "Average loss: 0.2317\n",
      "EPOCH 1067/2000\n",
      "Average loss: 0.2302\n",
      "EPOCH 1068/2000\n",
      "Average loss: 0.2307\n",
      "EPOCH 1069/2000\n",
      "Average loss: 0.2300\n",
      "EPOCH 1070/2000\n",
      "Average loss: 0.2308\n",
      "EPOCH 1071/2000\n",
      "Average loss: 0.2298\n",
      "EPOCH 1072/2000\n",
      "Average loss: 0.2294\n",
      "EPOCH 1073/2000\n",
      "Average loss: 0.2301\n",
      "EPOCH 1074/2000\n",
      "Average loss: 0.2292\n",
      "EPOCH 1075/2000\n",
      "Average loss: 0.2287\n",
      "EPOCH 1076/2000\n",
      "Average loss: 0.2292\n",
      "EPOCH 1077/2000\n",
      "Average loss: 0.2290\n",
      "EPOCH 1078/2000\n",
      "Average loss: 0.2288\n",
      "EPOCH 1079/2000\n",
      "Average loss: 0.2282\n",
      "EPOCH 1080/2000\n",
      "Average loss: 0.2287\n",
      "EPOCH 1081/2000\n",
      "Average loss: 0.2282\n",
      "EPOCH 1082/2000\n",
      "Average loss: 0.2283\n",
      "EPOCH 1083/2000\n",
      "Average loss: 0.2281\n",
      "EPOCH 1084/2000\n",
      "Average loss: 0.2290\n",
      "EPOCH 1085/2000\n",
      "Average loss: 0.2273\n",
      "EPOCH 1086/2000\n",
      "Average loss: 0.2266\n",
      "EPOCH 1087/2000\n",
      "Average loss: 0.2260\n",
      "EPOCH 1088/2000\n",
      "Average loss: 0.2257\n",
      "EPOCH 1089/2000\n",
      "Average loss: 0.2264\n",
      "EPOCH 1090/2000\n",
      "Average loss: 0.2256\n",
      "EPOCH 1091/2000\n",
      "Average loss: 0.2249\n",
      "EPOCH 1092/2000\n",
      "Average loss: 0.2248\n",
      "EPOCH 1093/2000\n",
      "Average loss: 0.2240\n",
      "EPOCH 1094/2000\n",
      "Average loss: 0.2243\n",
      "EPOCH 1095/2000\n",
      "Average loss: 0.2251\n",
      "EPOCH 1096/2000\n",
      "Average loss: 0.2251\n",
      "EPOCH 1097/2000\n",
      "Average loss: 0.2250\n",
      "EPOCH 1098/2000\n",
      "Average loss: 0.2246\n",
      "EPOCH 1099/2000\n",
      "Average loss: 0.2252\n",
      "EPOCH 1100/2000\n",
      "Average loss: 0.2243\n",
      "EPOCH 1101/2000\n",
      "Average loss: 0.2239\n",
      "EPOCH 1102/2000\n",
      "Average loss: 0.2238\n",
      "EPOCH 1103/2000\n",
      "Average loss: 0.2228\n",
      "EPOCH 1104/2000\n",
      "Average loss: 0.2239\n",
      "EPOCH 1105/2000\n",
      "Average loss: 0.2240\n",
      "EPOCH 1106/2000\n",
      "Average loss: 0.2230\n",
      "EPOCH 1107/2000\n",
      "Average loss: 0.2228\n",
      "EPOCH 1108/2000\n",
      "Average loss: 0.2231\n",
      "EPOCH 1109/2000\n",
      "Average loss: 0.2220\n",
      "EPOCH 1110/2000\n",
      "Average loss: 0.2221\n",
      "EPOCH 1111/2000\n",
      "Average loss: 0.2238\n",
      "EPOCH 1112/2000\n",
      "Average loss: 0.2225\n",
      "EPOCH 1113/2000\n",
      "Average loss: 0.2215\n",
      "EPOCH 1114/2000\n",
      "Average loss: 0.2224\n",
      "EPOCH 1115/2000\n",
      "Average loss: 0.2212\n",
      "EPOCH 1116/2000\n",
      "Average loss: 0.2214\n",
      "EPOCH 1117/2000\n",
      "Average loss: 0.2209\n",
      "EPOCH 1118/2000\n",
      "Average loss: 0.2217\n",
      "EPOCH 1119/2000\n",
      "Average loss: 0.2208\n",
      "EPOCH 1120/2000\n",
      "Average loss: 0.2207\n",
      "EPOCH 1121/2000\n",
      "Average loss: 0.2215\n",
      "EPOCH 1122/2000\n",
      "Average loss: 0.2215\n",
      "EPOCH 1123/2000\n",
      "Average loss: 0.2210\n",
      "EPOCH 1124/2000\n",
      "Average loss: 0.2201\n",
      "EPOCH 1125/2000\n",
      "Average loss: 0.2199\n",
      "EPOCH 1126/2000\n",
      "Average loss: 0.2201\n",
      "EPOCH 1127/2000\n",
      "Average loss: 0.2201\n",
      "EPOCH 1128/2000\n",
      "Average loss: 0.2209\n",
      "EPOCH 1129/2000\n",
      "Average loss: 0.2192\n",
      "EPOCH 1130/2000\n",
      "Average loss: 0.2178\n",
      "EPOCH 1131/2000\n",
      "Average loss: 0.2197\n",
      "EPOCH 1132/2000\n",
      "Average loss: 0.2180\n",
      "EPOCH 1133/2000\n",
      "Average loss: 0.2185\n",
      "EPOCH 1134/2000\n",
      "Average loss: 0.2177\n",
      "EPOCH 1135/2000\n",
      "Average loss: 0.2188\n",
      "EPOCH 1136/2000\n",
      "Average loss: 0.2189\n",
      "EPOCH 1137/2000\n",
      "Average loss: 0.2173\n",
      "EPOCH 1138/2000\n",
      "Average loss: 0.2180\n",
      "EPOCH 1139/2000\n",
      "Average loss: 0.2179\n",
      "EPOCH 1140/2000\n",
      "Average loss: 0.2179\n",
      "EPOCH 1141/2000\n",
      "Average loss: 0.2166\n",
      "EPOCH 1142/2000\n",
      "Average loss: 0.2168\n",
      "EPOCH 1143/2000\n",
      "Average loss: 0.2179\n",
      "EPOCH 1144/2000\n",
      "Average loss: 0.2161\n",
      "EPOCH 1145/2000\n",
      "Average loss: 0.2160\n",
      "EPOCH 1146/2000\n",
      "Average loss: 0.2156\n",
      "EPOCH 1147/2000\n",
      "Average loss: 0.2175\n",
      "EPOCH 1148/2000\n",
      "Average loss: 0.2160\n",
      "EPOCH 1149/2000\n",
      "Average loss: 0.2164\n",
      "EPOCH 1150/2000\n",
      "Average loss: 0.2168\n",
      "EPOCH 1151/2000\n",
      "Average loss: 0.2171\n",
      "EPOCH 1152/2000\n",
      "Average loss: 0.2157\n",
      "EPOCH 1153/2000\n",
      "Average loss: 0.2152\n",
      "EPOCH 1154/2000\n",
      "Average loss: 0.2149\n",
      "EPOCH 1155/2000\n",
      "Average loss: 0.2160\n",
      "EPOCH 1156/2000\n",
      "Average loss: 0.2153\n",
      "EPOCH 1157/2000\n",
      "Average loss: 0.2150\n",
      "EPOCH 1158/2000\n",
      "Average loss: 0.2151\n",
      "EPOCH 1159/2000\n",
      "Average loss: 0.2148\n",
      "EPOCH 1160/2000\n",
      "Average loss: 0.2140\n",
      "EPOCH 1161/2000\n",
      "Average loss: 0.2138\n",
      "EPOCH 1162/2000\n",
      "Average loss: 0.2138\n",
      "EPOCH 1163/2000\n",
      "Average loss: 0.2139\n",
      "EPOCH 1164/2000\n",
      "Average loss: 0.2132\n",
      "EPOCH 1165/2000\n",
      "Average loss: 0.2131\n",
      "EPOCH 1166/2000\n",
      "Average loss: 0.2131\n",
      "EPOCH 1167/2000\n",
      "Average loss: 0.2135\n",
      "EPOCH 1168/2000\n",
      "Average loss: 0.2137\n",
      "EPOCH 1169/2000\n",
      "Average loss: 0.2128\n",
      "EPOCH 1170/2000\n",
      "Average loss: 0.2118\n",
      "EPOCH 1171/2000\n",
      "Average loss: 0.2136\n",
      "EPOCH 1172/2000\n",
      "Average loss: 0.2120\n",
      "EPOCH 1173/2000\n",
      "Average loss: 0.2123\n",
      "EPOCH 1174/2000\n",
      "Average loss: 0.2120\n",
      "EPOCH 1175/2000\n",
      "Average loss: 0.2113\n",
      "EPOCH 1176/2000\n",
      "Average loss: 0.2111\n",
      "EPOCH 1177/2000\n",
      "Average loss: 0.2111\n",
      "EPOCH 1178/2000\n",
      "Average loss: 0.2113\n",
      "EPOCH 1179/2000\n",
      "Average loss: 0.2108\n",
      "EPOCH 1180/2000\n",
      "Average loss: 0.2110\n",
      "EPOCH 1181/2000\n",
      "Average loss: 0.2104\n",
      "EPOCH 1182/2000\n",
      "Average loss: 0.2104\n",
      "EPOCH 1183/2000\n",
      "Average loss: 0.2109\n",
      "EPOCH 1184/2000\n",
      "Average loss: 0.2103\n",
      "EPOCH 1185/2000\n",
      "Average loss: 0.2108\n",
      "EPOCH 1186/2000\n",
      "Average loss: 0.2101\n",
      "EPOCH 1187/2000\n",
      "Average loss: 0.2100\n",
      "EPOCH 1188/2000\n",
      "Average loss: 0.2096\n",
      "EPOCH 1189/2000\n",
      "Average loss: 0.2094\n",
      "EPOCH 1190/2000\n",
      "Average loss: 0.2100\n",
      "EPOCH 1191/2000\n",
      "Average loss: 0.2088\n",
      "EPOCH 1192/2000\n",
      "Average loss: 0.2101\n",
      "EPOCH 1193/2000\n",
      "Average loss: 0.2094\n",
      "EPOCH 1194/2000\n",
      "Average loss: 0.2088\n",
      "EPOCH 1195/2000\n",
      "Average loss: 0.2085\n",
      "EPOCH 1196/2000\n",
      "Average loss: 0.2092\n",
      "EPOCH 1197/2000\n",
      "Average loss: 0.2096\n",
      "EPOCH 1198/2000\n",
      "Average loss: 0.2084\n",
      "EPOCH 1199/2000\n",
      "Average loss: 0.2087\n",
      "EPOCH 1200/2000\n",
      "Average loss: 0.2079\n",
      "EPOCH 1201/2000\n",
      "Average loss: 0.2065\n",
      "EPOCH 1202/2000\n",
      "Average loss: 0.2070\n",
      "EPOCH 1203/2000\n",
      "Average loss: 0.2075\n",
      "EPOCH 1204/2000\n",
      "Average loss: 0.2075\n",
      "EPOCH 1205/2000\n",
      "Average loss: 0.2066\n",
      "EPOCH 1206/2000\n",
      "Average loss: 0.2080\n",
      "EPOCH 1207/2000\n",
      "Average loss: 0.2067\n",
      "EPOCH 1208/2000\n",
      "Average loss: 0.2076\n",
      "EPOCH 1209/2000\n",
      "Average loss: 0.2065\n",
      "EPOCH 1210/2000\n",
      "Average loss: 0.2066\n",
      "EPOCH 1211/2000\n",
      "Average loss: 0.2071\n",
      "EPOCH 1212/2000\n",
      "Average loss: 0.2070\n",
      "EPOCH 1213/2000\n",
      "Average loss: 0.2081\n",
      "EPOCH 1214/2000\n",
      "Average loss: 0.2076\n",
      "EPOCH 1215/2000\n",
      "Average loss: 0.2075\n",
      "EPOCH 1216/2000\n",
      "Average loss: 0.2056\n",
      "EPOCH 1217/2000\n",
      "Average loss: 0.2055\n",
      "EPOCH 1218/2000\n",
      "Average loss: 0.2063\n",
      "EPOCH 1219/2000\n",
      "Average loss: 0.2056\n",
      "EPOCH 1220/2000\n",
      "Average loss: 0.2051\n",
      "EPOCH 1221/2000\n",
      "Average loss: 0.2054\n",
      "EPOCH 1222/2000\n",
      "Average loss: 0.2052\n",
      "EPOCH 1223/2000\n",
      "Average loss: 0.2054\n",
      "EPOCH 1224/2000\n",
      "Average loss: 0.2052\n",
      "EPOCH 1225/2000\n",
      "Average loss: 0.2058\n",
      "EPOCH 1226/2000\n",
      "Average loss: 0.2043\n",
      "EPOCH 1227/2000\n",
      "Average loss: 0.2050\n",
      "EPOCH 1228/2000\n",
      "Average loss: 0.2047\n",
      "EPOCH 1229/2000\n",
      "Average loss: 0.2038\n",
      "EPOCH 1230/2000\n",
      "Average loss: 0.2027\n",
      "EPOCH 1231/2000\n",
      "Average loss: 0.2034\n",
      "EPOCH 1232/2000\n",
      "Average loss: 0.2028\n",
      "EPOCH 1233/2000\n",
      "Average loss: 0.2041\n",
      "EPOCH 1234/2000\n",
      "Average loss: 0.2021\n",
      "EPOCH 1235/2000\n",
      "Average loss: 0.2022\n",
      "EPOCH 1236/2000\n",
      "Average loss: 0.2021\n",
      "EPOCH 1237/2000\n",
      "Average loss: 0.2038\n",
      "EPOCH 1238/2000\n",
      "Average loss: 0.2026\n",
      "EPOCH 1239/2000\n",
      "Average loss: 0.2021\n",
      "EPOCH 1240/2000\n",
      "Average loss: 0.2023\n",
      "EPOCH 1241/2000\n",
      "Average loss: 0.2022\n",
      "EPOCH 1242/2000\n",
      "Average loss: 0.2035\n",
      "EPOCH 1243/2000\n",
      "Average loss: 0.2018\n",
      "EPOCH 1244/2000\n",
      "Average loss: 0.2022\n",
      "EPOCH 1245/2000\n",
      "Average loss: 0.2021\n",
      "EPOCH 1246/2000\n",
      "Average loss: 0.2027\n",
      "EPOCH 1247/2000\n",
      "Average loss: 0.2014\n",
      "EPOCH 1248/2000\n",
      "Average loss: 0.2016\n",
      "EPOCH 1249/2000\n",
      "Average loss: 0.2010\n",
      "EPOCH 1250/2000\n",
      "Average loss: 0.2012\n",
      "EPOCH 1251/2000\n",
      "Average loss: 0.2013\n",
      "EPOCH 1252/2000\n",
      "Average loss: 0.2015\n",
      "EPOCH 1253/2000\n",
      "Average loss: 0.2002\n",
      "EPOCH 1254/2000\n",
      "Average loss: 0.2005\n",
      "EPOCH 1255/2000\n",
      "Average loss: 0.2005\n",
      "EPOCH 1256/2000\n",
      "Average loss: 0.2011\n",
      "EPOCH 1257/2000\n",
      "Average loss: 0.2008\n",
      "EPOCH 1258/2000\n",
      "Average loss: 0.2009\n",
      "EPOCH 1259/2000\n",
      "Average loss: 0.2008\n",
      "EPOCH 1260/2000\n",
      "Average loss: 0.2012\n",
      "EPOCH 1261/2000\n",
      "Average loss: 0.2007\n",
      "EPOCH 1262/2000\n",
      "Average loss: 0.1996\n",
      "EPOCH 1263/2000\n",
      "Average loss: 0.1996\n",
      "EPOCH 1264/2000\n",
      "Average loss: 0.1985\n",
      "EPOCH 1265/2000\n",
      "Average loss: 0.1975\n",
      "EPOCH 1266/2000\n",
      "Average loss: 0.2001\n",
      "EPOCH 1267/2000\n",
      "Average loss: 0.1985\n",
      "EPOCH 1268/2000\n",
      "Average loss: 0.1982\n",
      "EPOCH 1269/2000\n",
      "Average loss: 0.2006\n",
      "EPOCH 1270/2000\n",
      "Average loss: 0.1993\n",
      "EPOCH 1271/2000\n",
      "Average loss: 0.1986\n",
      "EPOCH 1272/2000\n",
      "Average loss: 0.1980\n",
      "EPOCH 1273/2000\n",
      "Average loss: 0.1977\n",
      "EPOCH 1274/2000\n",
      "Average loss: 0.1972\n",
      "EPOCH 1275/2000\n",
      "Average loss: 0.1986\n",
      "EPOCH 1276/2000\n",
      "Average loss: 0.1974\n",
      "EPOCH 1277/2000\n",
      "Average loss: 0.1976\n",
      "EPOCH 1278/2000\n",
      "Average loss: 0.1975\n",
      "EPOCH 1279/2000\n",
      "Average loss: 0.1979\n",
      "EPOCH 1280/2000\n",
      "Average loss: 0.1975\n",
      "EPOCH 1281/2000\n",
      "Average loss: 0.1979\n",
      "EPOCH 1282/2000\n",
      "Average loss: 0.1973\n",
      "EPOCH 1283/2000\n",
      "Average loss: 0.1966\n",
      "EPOCH 1284/2000\n",
      "Average loss: 0.1971\n",
      "EPOCH 1285/2000\n",
      "Average loss: 0.1963\n",
      "EPOCH 1286/2000\n",
      "Average loss: 0.1968\n",
      "EPOCH 1287/2000\n",
      "Average loss: 0.1966\n",
      "EPOCH 1288/2000\n",
      "Average loss: 0.1960\n",
      "EPOCH 1289/2000\n",
      "Average loss: 0.1971\n",
      "EPOCH 1290/2000\n",
      "Average loss: 0.1946\n",
      "EPOCH 1291/2000\n",
      "Average loss: 0.1957\n",
      "EPOCH 1292/2000\n",
      "Average loss: 0.1959\n",
      "EPOCH 1293/2000\n",
      "Average loss: 0.1955\n",
      "EPOCH 1294/2000\n",
      "Average loss: 0.1948\n",
      "EPOCH 1295/2000\n",
      "Average loss: 0.1963\n",
      "EPOCH 1296/2000\n",
      "Average loss: 0.1948\n",
      "EPOCH 1297/2000\n",
      "Average loss: 0.1955\n",
      "EPOCH 1298/2000\n",
      "Average loss: 0.1958\n",
      "EPOCH 1299/2000\n",
      "Average loss: 0.1942\n",
      "EPOCH 1300/2000\n",
      "Average loss: 0.1936\n",
      "EPOCH 1301/2000\n",
      "Average loss: 0.1961\n",
      "EPOCH 1302/2000\n",
      "Average loss: 0.1944\n",
      "EPOCH 1303/2000\n",
      "Average loss: 0.1944\n",
      "EPOCH 1304/2000\n",
      "Average loss: 0.1937\n",
      "EPOCH 1305/2000\n",
      "Average loss: 0.1936\n",
      "EPOCH 1306/2000\n",
      "Average loss: 0.1928\n",
      "EPOCH 1307/2000\n",
      "Average loss: 0.1939\n",
      "EPOCH 1308/2000\n",
      "Average loss: 0.1948\n",
      "EPOCH 1309/2000\n",
      "Average loss: 0.1931\n",
      "EPOCH 1310/2000\n",
      "Average loss: 0.1935\n",
      "EPOCH 1311/2000\n",
      "Average loss: 0.1936\n",
      "EPOCH 1312/2000\n",
      "Average loss: 0.1928\n",
      "EPOCH 1313/2000\n",
      "Average loss: 0.1938\n",
      "EPOCH 1314/2000\n",
      "Average loss: 0.1935\n",
      "EPOCH 1315/2000\n",
      "Average loss: 0.1929\n",
      "EPOCH 1316/2000\n",
      "Average loss: 0.1930\n",
      "EPOCH 1317/2000\n",
      "Average loss: 0.1925\n",
      "EPOCH 1318/2000\n",
      "Average loss: 0.1930\n",
      "EPOCH 1319/2000\n",
      "Average loss: 0.1925\n",
      "EPOCH 1320/2000\n",
      "Average loss: 0.1932\n",
      "EPOCH 1321/2000\n",
      "Average loss: 0.1934\n",
      "EPOCH 1322/2000\n",
      "Average loss: 0.1919\n",
      "EPOCH 1323/2000\n",
      "Average loss: 0.1913\n",
      "EPOCH 1324/2000\n",
      "Average loss: 0.1915\n",
      "EPOCH 1325/2000\n",
      "Average loss: 0.1910\n",
      "EPOCH 1326/2000\n",
      "Average loss: 0.1912\n",
      "EPOCH 1327/2000\n",
      "Average loss: 0.1915\n",
      "EPOCH 1328/2000\n",
      "Average loss: 0.1914\n",
      "EPOCH 1329/2000\n",
      "Average loss: 0.1927\n",
      "EPOCH 1330/2000\n",
      "Average loss: 0.1911\n",
      "EPOCH 1331/2000\n",
      "Average loss: 0.1910\n",
      "EPOCH 1332/2000\n",
      "Average loss: 0.1909\n",
      "EPOCH 1333/2000\n",
      "Average loss: 0.1905\n",
      "EPOCH 1334/2000\n",
      "Average loss: 0.1904\n",
      "EPOCH 1335/2000\n",
      "Average loss: 0.1906\n",
      "EPOCH 1336/2000\n",
      "Average loss: 0.1900\n",
      "EPOCH 1337/2000\n",
      "Average loss: 0.1907\n",
      "EPOCH 1338/2000\n",
      "Average loss: 0.1904\n",
      "EPOCH 1339/2000\n",
      "Average loss: 0.1904\n",
      "EPOCH 1340/2000\n",
      "Average loss: 0.1904\n",
      "EPOCH 1341/2000\n",
      "Average loss: 0.1895\n",
      "EPOCH 1342/2000\n",
      "Average loss: 0.1893\n",
      "EPOCH 1343/2000\n",
      "Average loss: 0.1904\n",
      "EPOCH 1344/2000\n",
      "Average loss: 0.1887\n",
      "EPOCH 1345/2000\n",
      "Average loss: 0.1885\n",
      "EPOCH 1346/2000\n",
      "Average loss: 0.1885\n",
      "EPOCH 1347/2000\n",
      "Average loss: 0.1896\n",
      "EPOCH 1348/2000\n",
      "Average loss: 0.1900\n",
      "EPOCH 1349/2000\n",
      "Average loss: 0.1889\n",
      "EPOCH 1350/2000\n",
      "Average loss: 0.1885\n",
      "EPOCH 1351/2000\n",
      "Average loss: 0.1889\n",
      "EPOCH 1352/2000\n",
      "Average loss: 0.1891\n",
      "EPOCH 1353/2000\n",
      "Average loss: 0.1880\n",
      "EPOCH 1354/2000\n",
      "Average loss: 0.1893\n",
      "EPOCH 1355/2000\n",
      "Average loss: 0.1896\n",
      "EPOCH 1356/2000\n",
      "Average loss: 0.1897\n",
      "EPOCH 1357/2000\n",
      "Average loss: 0.1883\n",
      "EPOCH 1358/2000\n",
      "Average loss: 0.1881\n",
      "EPOCH 1359/2000\n",
      "Average loss: 0.1891\n",
      "EPOCH 1360/2000\n",
      "Average loss: 0.1887\n",
      "EPOCH 1361/2000\n",
      "Average loss: 0.1877\n",
      "EPOCH 1362/2000\n",
      "Average loss: 0.1887\n",
      "EPOCH 1363/2000\n",
      "Average loss: 0.1878\n",
      "EPOCH 1364/2000\n",
      "Average loss: 0.1876\n",
      "EPOCH 1365/2000\n",
      "Average loss: 0.1860\n",
      "EPOCH 1366/2000\n",
      "Average loss: 0.1874\n",
      "EPOCH 1367/2000\n",
      "Average loss: 0.1866\n",
      "EPOCH 1368/2000\n",
      "Average loss: 0.1864\n",
      "EPOCH 1369/2000\n",
      "Average loss: 0.1859\n",
      "EPOCH 1370/2000\n",
      "Average loss: 0.1864\n",
      "EPOCH 1371/2000\n",
      "Average loss: 0.1870\n",
      "EPOCH 1372/2000\n",
      "Average loss: 0.1873\n",
      "EPOCH 1373/2000\n",
      "Average loss: 0.1868\n",
      "EPOCH 1374/2000\n",
      "Average loss: 0.1871\n",
      "EPOCH 1375/2000\n",
      "Average loss: 0.1882\n",
      "EPOCH 1376/2000\n",
      "Average loss: 0.1870\n",
      "EPOCH 1377/2000\n",
      "Average loss: 0.1876\n",
      "EPOCH 1378/2000\n",
      "Average loss: 0.1864\n",
      "EPOCH 1379/2000\n",
      "Average loss: 0.1853\n",
      "EPOCH 1380/2000\n",
      "Average loss: 0.1856\n",
      "EPOCH 1381/2000\n",
      "Average loss: 0.1862\n",
      "EPOCH 1382/2000\n",
      "Average loss: 0.1860\n",
      "EPOCH 1383/2000\n",
      "Average loss: 0.1864\n",
      "EPOCH 1384/2000\n",
      "Average loss: 0.1859\n",
      "EPOCH 1385/2000\n",
      "Average loss: 0.1862\n",
      "EPOCH 1386/2000\n",
      "Average loss: 0.1849\n",
      "EPOCH 1387/2000\n",
      "Average loss: 0.1855\n",
      "EPOCH 1388/2000\n",
      "Average loss: 0.1850\n",
      "EPOCH 1389/2000\n",
      "Average loss: 0.1853\n",
      "EPOCH 1390/2000\n",
      "Average loss: 0.1854\n",
      "EPOCH 1391/2000\n",
      "Average loss: 0.1842\n",
      "EPOCH 1392/2000\n",
      "Average loss: 0.1853\n",
      "EPOCH 1393/2000\n",
      "Average loss: 0.1845\n",
      "EPOCH 1394/2000\n",
      "Average loss: 0.1859\n",
      "EPOCH 1395/2000\n",
      "Average loss: 0.1845\n",
      "EPOCH 1396/2000\n",
      "Average loss: 0.1840\n",
      "EPOCH 1397/2000\n",
      "Average loss: 0.1834\n",
      "EPOCH 1398/2000\n",
      "Average loss: 0.1848\n",
      "EPOCH 1399/2000\n",
      "Average loss: 0.1828\n",
      "EPOCH 1400/2000\n",
      "Average loss: 0.1835\n",
      "EPOCH 1401/2000\n",
      "Average loss: 0.1850\n",
      "EPOCH 1402/2000\n",
      "Average loss: 0.1841\n",
      "EPOCH 1403/2000\n",
      "Average loss: 0.1831\n",
      "EPOCH 1404/2000\n",
      "Average loss: 0.1825\n",
      "EPOCH 1405/2000\n",
      "Average loss: 0.1835\n",
      "EPOCH 1406/2000\n",
      "Average loss: 0.1840\n",
      "EPOCH 1407/2000\n",
      "Average loss: 0.1827\n",
      "EPOCH 1408/2000\n",
      "Average loss: 0.1840\n",
      "EPOCH 1409/2000\n",
      "Average loss: 0.1815\n",
      "EPOCH 1410/2000\n",
      "Average loss: 0.1819\n",
      "EPOCH 1411/2000\n",
      "Average loss: 0.1830\n",
      "EPOCH 1412/2000\n",
      "Average loss: 0.1818\n",
      "EPOCH 1413/2000\n",
      "Average loss: 0.1813\n",
      "EPOCH 1414/2000\n",
      "Average loss: 0.1818\n",
      "EPOCH 1415/2000\n",
      "Average loss: 0.1826\n",
      "EPOCH 1416/2000\n",
      "Average loss: 0.1815\n",
      "EPOCH 1417/2000\n",
      "Average loss: 0.1813\n",
      "EPOCH 1418/2000\n",
      "Average loss: 0.1816\n",
      "EPOCH 1419/2000\n",
      "Average loss: 0.1818\n",
      "EPOCH 1420/2000\n",
      "Average loss: 0.1824\n",
      "EPOCH 1421/2000\n",
      "Average loss: 0.1816\n",
      "EPOCH 1422/2000\n",
      "Average loss: 0.1814\n",
      "EPOCH 1423/2000\n",
      "Average loss: 0.1813\n",
      "EPOCH 1424/2000\n",
      "Average loss: 0.1813\n",
      "EPOCH 1425/2000\n",
      "Average loss: 0.1802\n",
      "EPOCH 1426/2000\n",
      "Average loss: 0.1809\n",
      "EPOCH 1427/2000\n",
      "Average loss: 0.1811\n",
      "EPOCH 1428/2000\n",
      "Average loss: 0.1814\n",
      "EPOCH 1429/2000\n",
      "Average loss: 0.1811\n",
      "EPOCH 1430/2000\n",
      "Average loss: 0.1820\n",
      "EPOCH 1431/2000\n",
      "Average loss: 0.1812\n",
      "EPOCH 1432/2000\n",
      "Average loss: 0.1815\n",
      "EPOCH 1433/2000\n",
      "Average loss: 0.1810\n",
      "EPOCH 1434/2000\n",
      "Average loss: 0.1807\n",
      "EPOCH 1435/2000\n",
      "Average loss: 0.1792\n",
      "EPOCH 1436/2000\n",
      "Average loss: 0.1807\n",
      "EPOCH 1437/2000\n",
      "Average loss: 0.1798\n",
      "EPOCH 1438/2000\n",
      "Average loss: 0.1814\n",
      "EPOCH 1439/2000\n",
      "Average loss: 0.1800\n",
      "EPOCH 1440/2000\n",
      "Average loss: 0.1800\n",
      "EPOCH 1441/2000\n",
      "Average loss: 0.1784\n",
      "EPOCH 1442/2000\n",
      "Average loss: 0.1794\n",
      "EPOCH 1443/2000\n",
      "Average loss: 0.1796\n",
      "EPOCH 1444/2000\n",
      "Average loss: 0.1795\n",
      "EPOCH 1445/2000\n",
      "Average loss: 0.1798\n",
      "EPOCH 1446/2000\n",
      "Average loss: 0.1802\n",
      "EPOCH 1447/2000\n",
      "Average loss: 0.1797\n",
      "EPOCH 1448/2000\n",
      "Average loss: 0.1784\n",
      "EPOCH 1449/2000\n",
      "Average loss: 0.1796\n",
      "EPOCH 1450/2000\n",
      "Average loss: 0.1800\n",
      "EPOCH 1451/2000\n",
      "Average loss: 0.1783\n",
      "EPOCH 1452/2000\n",
      "Average loss: 0.1792\n",
      "EPOCH 1453/2000\n",
      "Average loss: 0.1798\n",
      "EPOCH 1454/2000\n",
      "Average loss: 0.1783\n",
      "EPOCH 1455/2000\n",
      "Average loss: 0.1784\n",
      "EPOCH 1456/2000\n",
      "Average loss: 0.1780\n",
      "EPOCH 1457/2000\n",
      "Average loss: 0.1786\n",
      "EPOCH 1458/2000\n",
      "Average loss: 0.1788\n",
      "EPOCH 1459/2000\n",
      "Average loss: 0.1776\n",
      "EPOCH 1460/2000\n",
      "Average loss: 0.1774\n",
      "EPOCH 1461/2000\n",
      "Average loss: 0.1771\n",
      "EPOCH 1462/2000\n",
      "Average loss: 0.1769\n",
      "EPOCH 1463/2000\n",
      "Average loss: 0.1789\n",
      "EPOCH 1464/2000\n",
      "Average loss: 0.1775\n",
      "EPOCH 1465/2000\n",
      "Average loss: 0.1781\n",
      "EPOCH 1466/2000\n",
      "Average loss: 0.1773\n",
      "EPOCH 1467/2000\n",
      "Average loss: 0.1765\n",
      "EPOCH 1468/2000\n",
      "Average loss: 0.1779\n",
      "EPOCH 1469/2000\n",
      "Average loss: 0.1763\n",
      "EPOCH 1470/2000\n",
      "Average loss: 0.1765\n",
      "EPOCH 1471/2000\n",
      "Average loss: 0.1758\n",
      "EPOCH 1472/2000\n",
      "Average loss: 0.1757\n",
      "EPOCH 1473/2000\n",
      "Average loss: 0.1766\n",
      "EPOCH 1474/2000\n",
      "Average loss: 0.1774\n",
      "EPOCH 1475/2000\n",
      "Average loss: 0.1770\n",
      "EPOCH 1476/2000\n",
      "Average loss: 0.1760\n",
      "EPOCH 1477/2000\n",
      "Average loss: 0.1773\n",
      "EPOCH 1478/2000\n",
      "Average loss: 0.1759\n",
      "EPOCH 1479/2000\n",
      "Average loss: 0.1773\n",
      "EPOCH 1480/2000\n",
      "Average loss: 0.1761\n",
      "EPOCH 1481/2000\n",
      "Average loss: 0.1756\n",
      "EPOCH 1482/2000\n",
      "Average loss: 0.1764\n",
      "EPOCH 1483/2000\n",
      "Average loss: 0.1761\n",
      "EPOCH 1484/2000\n",
      "Average loss: 0.1753\n",
      "EPOCH 1485/2000\n",
      "Average loss: 0.1759\n",
      "EPOCH 1486/2000\n",
      "Average loss: 0.1755\n",
      "EPOCH 1487/2000\n",
      "Average loss: 0.1759\n",
      "EPOCH 1488/2000\n",
      "Average loss: 0.1745\n",
      "EPOCH 1489/2000\n",
      "Average loss: 0.1747\n",
      "EPOCH 1490/2000\n",
      "Average loss: 0.1742\n",
      "EPOCH 1491/2000\n",
      "Average loss: 0.1755\n",
      "EPOCH 1492/2000\n",
      "Average loss: 0.1757\n",
      "EPOCH 1493/2000\n",
      "Average loss: 0.1757\n",
      "EPOCH 1494/2000\n",
      "Average loss: 0.1765\n",
      "EPOCH 1495/2000\n",
      "Average loss: 0.1754\n",
      "EPOCH 1496/2000\n",
      "Average loss: 0.1742\n",
      "EPOCH 1497/2000\n",
      "Average loss: 0.1737\n",
      "EPOCH 1498/2000\n",
      "Average loss: 0.1740\n",
      "EPOCH 1499/2000\n",
      "Average loss: 0.1745\n",
      "EPOCH 1500/2000\n",
      "Average loss: 0.1737\n",
      "EPOCH 1501/2000\n",
      "Average loss: 0.1735\n",
      "EPOCH 1502/2000\n",
      "Average loss: 0.1745\n",
      "EPOCH 1503/2000\n",
      "Average loss: 0.1736\n",
      "EPOCH 1504/2000\n",
      "Average loss: 0.1742\n",
      "EPOCH 1505/2000\n",
      "Average loss: 0.1726\n",
      "EPOCH 1506/2000\n",
      "Average loss: 0.1751\n",
      "EPOCH 1507/2000\n",
      "Average loss: 0.1726\n",
      "EPOCH 1508/2000\n",
      "Average loss: 0.1738\n",
      "EPOCH 1509/2000\n",
      "Average loss: 0.1748\n",
      "EPOCH 1510/2000\n",
      "Average loss: 0.1734\n",
      "EPOCH 1511/2000\n",
      "Average loss: 0.1735\n",
      "EPOCH 1512/2000\n",
      "Average loss: 0.1737\n",
      "EPOCH 1513/2000\n",
      "Average loss: 0.1739\n",
      "EPOCH 1514/2000\n",
      "Average loss: 0.1731\n",
      "EPOCH 1515/2000\n",
      "Average loss: 0.1720\n",
      "EPOCH 1516/2000\n",
      "Average loss: 0.1731\n",
      "EPOCH 1517/2000\n",
      "Average loss: 0.1726\n",
      "EPOCH 1518/2000\n",
      "Average loss: 0.1733\n",
      "EPOCH 1519/2000\n",
      "Average loss: 0.1735\n",
      "EPOCH 1520/2000\n",
      "Average loss: 0.1720\n",
      "EPOCH 1521/2000\n",
      "Average loss: 0.1729\n",
      "EPOCH 1522/2000\n",
      "Average loss: 0.1726\n",
      "EPOCH 1523/2000\n",
      "Average loss: 0.1733\n",
      "EPOCH 1524/2000\n",
      "Average loss: 0.1719\n",
      "EPOCH 1525/2000\n",
      "Average loss: 0.1728\n",
      "EPOCH 1526/2000\n",
      "Average loss: 0.1734\n",
      "EPOCH 1527/2000\n",
      "Average loss: 0.1730\n",
      "EPOCH 1528/2000\n",
      "Average loss: 0.1722\n",
      "EPOCH 1529/2000\n",
      "Average loss: 0.1722\n",
      "EPOCH 1530/2000\n",
      "Average loss: 0.1724\n",
      "EPOCH 1531/2000\n",
      "Average loss: 0.1724\n",
      "EPOCH 1532/2000\n",
      "Average loss: 0.1716\n",
      "EPOCH 1533/2000\n",
      "Average loss: 0.1712\n",
      "EPOCH 1534/2000\n",
      "Average loss: 0.1709\n",
      "EPOCH 1535/2000\n",
      "Average loss: 0.1711\n",
      "EPOCH 1536/2000\n",
      "Average loss: 0.1715\n",
      "EPOCH 1537/2000\n",
      "Average loss: 0.1717\n",
      "EPOCH 1538/2000\n",
      "Average loss: 0.1710\n",
      "EPOCH 1539/2000\n",
      "Average loss: 0.1717\n",
      "EPOCH 1540/2000\n",
      "Average loss: 0.1724\n",
      "EPOCH 1541/2000\n",
      "Average loss: 0.1717\n",
      "EPOCH 1542/2000\n",
      "Average loss: 0.1709\n",
      "EPOCH 1543/2000\n",
      "Average loss: 0.1717\n",
      "EPOCH 1544/2000\n",
      "Average loss: 0.1701\n",
      "EPOCH 1545/2000\n",
      "Average loss: 0.1714\n",
      "EPOCH 1546/2000\n",
      "Average loss: 0.1697\n",
      "EPOCH 1547/2000\n",
      "Average loss: 0.1707\n",
      "EPOCH 1548/2000\n",
      "Average loss: 0.1690\n",
      "EPOCH 1549/2000\n",
      "Average loss: 0.1707\n",
      "EPOCH 1550/2000\n",
      "Average loss: 0.1699\n",
      "EPOCH 1551/2000\n",
      "Average loss: 0.1695\n",
      "EPOCH 1552/2000\n",
      "Average loss: 0.1706\n",
      "EPOCH 1553/2000\n",
      "Average loss: 0.1701\n",
      "EPOCH 1554/2000\n",
      "Average loss: 0.1694\n",
      "EPOCH 1555/2000\n",
      "Average loss: 0.1689\n",
      "EPOCH 1556/2000\n",
      "Average loss: 0.1702\n",
      "EPOCH 1557/2000\n",
      "Average loss: 0.1706\n",
      "EPOCH 1558/2000\n",
      "Average loss: 0.1688\n",
      "EPOCH 1559/2000\n",
      "Average loss: 0.1712\n",
      "EPOCH 1560/2000\n",
      "Average loss: 0.1703\n",
      "EPOCH 1561/2000\n",
      "Average loss: 0.1688\n",
      "EPOCH 1562/2000\n",
      "Average loss: 0.1696\n",
      "EPOCH 1563/2000\n",
      "Average loss: 0.1682\n",
      "EPOCH 1564/2000\n",
      "Average loss: 0.1699\n",
      "EPOCH 1565/2000\n",
      "Average loss: 0.1688\n",
      "EPOCH 1566/2000\n",
      "Average loss: 0.1685\n",
      "EPOCH 1567/2000\n",
      "Average loss: 0.1685\n",
      "EPOCH 1568/2000\n",
      "Average loss: 0.1683\n",
      "EPOCH 1569/2000\n",
      "Average loss: 0.1689\n",
      "EPOCH 1570/2000\n",
      "Average loss: 0.1682\n",
      "EPOCH 1571/2000\n",
      "Average loss: 0.1684\n",
      "EPOCH 1572/2000\n",
      "Average loss: 0.1680\n",
      "EPOCH 1573/2000\n",
      "Average loss: 0.1693\n",
      "EPOCH 1574/2000\n",
      "Average loss: 0.1677\n",
      "EPOCH 1575/2000\n",
      "Average loss: 0.1686\n",
      "EPOCH 1576/2000\n",
      "Average loss: 0.1677\n",
      "EPOCH 1577/2000\n",
      "Average loss: 0.1680\n",
      "EPOCH 1578/2000\n",
      "Average loss: 0.1677\n",
      "EPOCH 1579/2000\n",
      "Average loss: 0.1682\n",
      "EPOCH 1580/2000\n",
      "Average loss: 0.1673\n",
      "EPOCH 1581/2000\n",
      "Average loss: 0.1693\n",
      "EPOCH 1582/2000\n",
      "Average loss: 0.1682\n",
      "EPOCH 1583/2000\n",
      "Average loss: 0.1682\n",
      "EPOCH 1584/2000\n",
      "Average loss: 0.1682\n",
      "EPOCH 1585/2000\n",
      "Average loss: 0.1672\n",
      "EPOCH 1586/2000\n",
      "Average loss: 0.1677\n",
      "EPOCH 1587/2000\n",
      "Average loss: 0.1672\n",
      "EPOCH 1588/2000\n",
      "Average loss: 0.1666\n",
      "EPOCH 1589/2000\n",
      "Average loss: 0.1672\n",
      "EPOCH 1590/2000\n",
      "Average loss: 0.1687\n",
      "EPOCH 1591/2000\n",
      "Average loss: 0.1677\n",
      "EPOCH 1592/2000\n",
      "Average loss: 0.1671\n",
      "EPOCH 1593/2000\n",
      "Average loss: 0.1668\n",
      "EPOCH 1594/2000\n",
      "Average loss: 0.1673\n",
      "EPOCH 1595/2000\n",
      "Average loss: 0.1672\n",
      "EPOCH 1596/2000\n",
      "Average loss: 0.1668\n",
      "EPOCH 1597/2000\n",
      "Average loss: 0.1661\n",
      "EPOCH 1598/2000\n",
      "Average loss: 0.1674\n",
      "EPOCH 1599/2000\n",
      "Average loss: 0.1661\n",
      "EPOCH 1600/2000\n",
      "Average loss: 0.1656\n",
      "EPOCH 1601/2000\n",
      "Average loss: 0.1668\n",
      "EPOCH 1602/2000\n",
      "Average loss: 0.1658\n",
      "EPOCH 1603/2000\n",
      "Average loss: 0.1666\n",
      "EPOCH 1604/2000\n",
      "Average loss: 0.1674\n",
      "EPOCH 1605/2000\n",
      "Average loss: 0.1668\n",
      "EPOCH 1606/2000\n",
      "Average loss: 0.1667\n",
      "EPOCH 1607/2000\n",
      "Average loss: 0.1657\n",
      "EPOCH 1608/2000\n",
      "Average loss: 0.1655\n",
      "EPOCH 1609/2000\n",
      "Average loss: 0.1658\n",
      "EPOCH 1610/2000\n",
      "Average loss: 0.1657\n",
      "EPOCH 1611/2000\n",
      "Average loss: 0.1668\n",
      "EPOCH 1612/2000\n",
      "Average loss: 0.1655\n",
      "EPOCH 1613/2000\n",
      "Average loss: 0.1665\n",
      "EPOCH 1614/2000\n",
      "Average loss: 0.1666\n",
      "EPOCH 1615/2000\n",
      "Average loss: 0.1651\n",
      "EPOCH 1616/2000\n",
      "Average loss: 0.1660\n",
      "EPOCH 1617/2000\n",
      "Average loss: 0.1654\n",
      "EPOCH 1618/2000\n",
      "Average loss: 0.1651\n",
      "EPOCH 1619/2000\n",
      "Average loss: 0.1658\n",
      "EPOCH 1620/2000\n",
      "Average loss: 0.1656\n",
      "EPOCH 1621/2000\n",
      "Average loss: 0.1648\n",
      "EPOCH 1622/2000\n",
      "Average loss: 0.1656\n",
      "EPOCH 1623/2000\n",
      "Average loss: 0.1639\n",
      "EPOCH 1624/2000\n",
      "Average loss: 0.1652\n",
      "EPOCH 1625/2000\n",
      "Average loss: 0.1648\n",
      "EPOCH 1626/2000\n",
      "Average loss: 0.1646\n",
      "EPOCH 1627/2000\n",
      "Average loss: 0.1650\n",
      "EPOCH 1628/2000\n",
      "Average loss: 0.1639\n",
      "EPOCH 1629/2000\n",
      "Average loss: 0.1657\n",
      "EPOCH 1630/2000\n",
      "Average loss: 0.1658\n",
      "EPOCH 1631/2000\n",
      "Average loss: 0.1624\n",
      "EPOCH 1632/2000\n",
      "Average loss: 0.1637\n",
      "EPOCH 1633/2000\n",
      "Average loss: 0.1643\n",
      "EPOCH 1634/2000\n",
      "Average loss: 0.1656\n",
      "EPOCH 1635/2000\n",
      "Average loss: 0.1645\n",
      "EPOCH 1636/2000\n",
      "Average loss: 0.1637\n",
      "EPOCH 1637/2000\n",
      "Average loss: 0.1643\n",
      "EPOCH 1638/2000\n",
      "Average loss: 0.1633\n",
      "EPOCH 1639/2000\n",
      "Average loss: 0.1640\n",
      "EPOCH 1640/2000\n",
      "Average loss: 0.1630\n",
      "EPOCH 1641/2000\n",
      "Average loss: 0.1637\n",
      "EPOCH 1642/2000\n",
      "Average loss: 0.1637\n",
      "EPOCH 1643/2000\n",
      "Average loss: 0.1635\n",
      "EPOCH 1644/2000\n",
      "Average loss: 0.1639\n",
      "EPOCH 1645/2000\n",
      "Average loss: 0.1631\n",
      "EPOCH 1646/2000\n",
      "Average loss: 0.1627\n",
      "EPOCH 1647/2000\n",
      "Average loss: 0.1639\n",
      "EPOCH 1648/2000\n",
      "Average loss: 0.1627\n",
      "EPOCH 1649/2000\n",
      "Average loss: 0.1616\n",
      "EPOCH 1650/2000\n",
      "Average loss: 0.1625\n",
      "EPOCH 1651/2000\n",
      "Average loss: 0.1621\n",
      "EPOCH 1652/2000\n",
      "Average loss: 0.1636\n",
      "EPOCH 1653/2000\n",
      "Average loss: 0.1626\n",
      "EPOCH 1654/2000\n",
      "Average loss: 0.1632\n",
      "EPOCH 1655/2000\n",
      "Average loss: 0.1634\n",
      "EPOCH 1656/2000\n",
      "Average loss: 0.1625\n",
      "EPOCH 1657/2000\n",
      "Average loss: 0.1620\n",
      "EPOCH 1658/2000\n",
      "Average loss: 0.1613\n",
      "EPOCH 1659/2000\n",
      "Average loss: 0.1623\n",
      "EPOCH 1660/2000\n",
      "Average loss: 0.1617\n",
      "EPOCH 1661/2000\n",
      "Average loss: 0.1606\n",
      "EPOCH 1662/2000\n",
      "Average loss: 0.1616\n",
      "EPOCH 1663/2000\n",
      "Average loss: 0.1614\n",
      "EPOCH 1664/2000\n",
      "Average loss: 0.1608\n",
      "EPOCH 1665/2000\n",
      "Average loss: 0.1619\n",
      "EPOCH 1666/2000\n",
      "Average loss: 0.1626\n",
      "EPOCH 1667/2000\n",
      "Average loss: 0.1619\n",
      "EPOCH 1668/2000\n",
      "Average loss: 0.1619\n",
      "EPOCH 1669/2000\n",
      "Average loss: 0.1608\n",
      "EPOCH 1670/2000\n",
      "Average loss: 0.1611\n",
      "EPOCH 1671/2000\n",
      "Average loss: 0.1615\n",
      "EPOCH 1672/2000\n",
      "Average loss: 0.1612\n",
      "EPOCH 1673/2000\n",
      "Average loss: 0.1602\n",
      "EPOCH 1674/2000\n",
      "Average loss: 0.1603\n",
      "EPOCH 1675/2000\n",
      "Average loss: 0.1599\n",
      "EPOCH 1676/2000\n",
      "Average loss: 0.1624\n",
      "EPOCH 1677/2000\n",
      "Average loss: 0.1610\n",
      "EPOCH 1678/2000\n",
      "Average loss: 0.1609\n",
      "EPOCH 1679/2000\n",
      "Average loss: 0.1609\n",
      "EPOCH 1680/2000\n",
      "Average loss: 0.1628\n",
      "EPOCH 1681/2000\n",
      "Average loss: 0.1616\n",
      "EPOCH 1682/2000\n",
      "Average loss: 0.1621\n",
      "EPOCH 1683/2000\n",
      "Average loss: 0.1614\n",
      "EPOCH 1684/2000\n",
      "Average loss: 0.1608\n",
      "EPOCH 1685/2000\n",
      "Average loss: 0.1600\n",
      "EPOCH 1686/2000\n",
      "Average loss: 0.1605\n",
      "EPOCH 1687/2000\n",
      "Average loss: 0.1601\n",
      "EPOCH 1688/2000\n",
      "Average loss: 0.1620\n",
      "EPOCH 1689/2000\n",
      "Average loss: 0.1604\n",
      "EPOCH 1690/2000\n",
      "Average loss: 0.1606\n",
      "EPOCH 1691/2000\n",
      "Average loss: 0.1601\n",
      "EPOCH 1692/2000\n",
      "Average loss: 0.1593\n",
      "EPOCH 1693/2000\n",
      "Average loss: 0.1599\n",
      "EPOCH 1694/2000\n",
      "Average loss: 0.1589\n",
      "EPOCH 1695/2000\n",
      "Average loss: 0.1611\n",
      "EPOCH 1696/2000\n",
      "Average loss: 0.1598\n",
      "EPOCH 1697/2000\n",
      "Average loss: 0.1594\n",
      "EPOCH 1698/2000\n",
      "Average loss: 0.1589\n",
      "EPOCH 1699/2000\n",
      "Average loss: 0.1587\n",
      "EPOCH 1700/2000\n",
      "Average loss: 0.1595\n",
      "EPOCH 1701/2000\n",
      "Average loss: 0.1600\n",
      "EPOCH 1702/2000\n",
      "Average loss: 0.1586\n",
      "EPOCH 1703/2000\n",
      "Average loss: 0.1581\n",
      "EPOCH 1704/2000\n",
      "Average loss: 0.1592\n",
      "EPOCH 1705/2000\n",
      "Average loss: 0.1590\n",
      "EPOCH 1706/2000\n",
      "Average loss: 0.1587\n",
      "EPOCH 1707/2000\n",
      "Average loss: 0.1592\n",
      "EPOCH 1708/2000\n",
      "Average loss: 0.1592\n",
      "EPOCH 1709/2000\n",
      "Average loss: 0.1602\n",
      "EPOCH 1710/2000\n",
      "Average loss: 0.1592\n",
      "EPOCH 1711/2000\n",
      "Average loss: 0.1585\n",
      "EPOCH 1712/2000\n",
      "Average loss: 0.1595\n",
      "EPOCH 1713/2000\n",
      "Average loss: 0.1597\n",
      "EPOCH 1714/2000\n",
      "Average loss: 0.1592\n",
      "EPOCH 1715/2000\n",
      "Average loss: 0.1591\n",
      "EPOCH 1716/2000\n",
      "Average loss: 0.1589\n",
      "EPOCH 1717/2000\n",
      "Average loss: 0.1585\n",
      "EPOCH 1718/2000\n",
      "Average loss: 0.1581\n",
      "EPOCH 1719/2000\n",
      "Average loss: 0.1581\n",
      "EPOCH 1720/2000\n",
      "Average loss: 0.1574\n",
      "EPOCH 1721/2000\n",
      "Average loss: 0.1580\n",
      "EPOCH 1722/2000\n",
      "Average loss: 0.1580\n",
      "EPOCH 1723/2000\n",
      "Average loss: 0.1583\n",
      "EPOCH 1724/2000\n",
      "Average loss: 0.1579\n",
      "EPOCH 1725/2000\n",
      "Average loss: 0.1579\n",
      "EPOCH 1726/2000\n",
      "Average loss: 0.1589\n",
      "EPOCH 1727/2000\n",
      "Average loss: 0.1585\n",
      "EPOCH 1728/2000\n",
      "Average loss: 0.1578\n",
      "EPOCH 1729/2000\n",
      "Average loss: 0.1574\n",
      "EPOCH 1730/2000\n",
      "Average loss: 0.1574\n",
      "EPOCH 1731/2000\n",
      "Average loss: 0.1571\n",
      "EPOCH 1732/2000\n",
      "Average loss: 0.1583\n",
      "EPOCH 1733/2000\n",
      "Average loss: 0.1576\n",
      "EPOCH 1734/2000\n",
      "Average loss: 0.1578\n",
      "EPOCH 1735/2000\n",
      "Average loss: 0.1572\n",
      "EPOCH 1736/2000\n",
      "Average loss: 0.1569\n",
      "EPOCH 1737/2000\n",
      "Average loss: 0.1578\n",
      "EPOCH 1738/2000\n",
      "Average loss: 0.1572\n",
      "EPOCH 1739/2000\n",
      "Average loss: 0.1570\n",
      "EPOCH 1740/2000\n",
      "Average loss: 0.1575\n",
      "EPOCH 1741/2000\n",
      "Average loss: 0.1586\n",
      "EPOCH 1742/2000\n",
      "Average loss: 0.1573\n",
      "EPOCH 1743/2000\n",
      "Average loss: 0.1563\n",
      "EPOCH 1744/2000\n",
      "Average loss: 0.1567\n",
      "EPOCH 1745/2000\n",
      "Average loss: 0.1564\n",
      "EPOCH 1746/2000\n",
      "Average loss: 0.1560\n",
      "EPOCH 1747/2000\n",
      "Average loss: 0.1567\n",
      "EPOCH 1748/2000\n",
      "Average loss: 0.1563\n",
      "EPOCH 1749/2000\n",
      "Average loss: 0.1558\n",
      "EPOCH 1750/2000\n",
      "Average loss: 0.1565\n",
      "EPOCH 1751/2000\n",
      "Average loss: 0.1571\n",
      "EPOCH 1752/2000\n",
      "Average loss: 0.1555\n",
      "EPOCH 1753/2000\n",
      "Average loss: 0.1567\n",
      "EPOCH 1754/2000\n",
      "Average loss: 0.1567\n",
      "EPOCH 1755/2000\n",
      "Average loss: 0.1560\n",
      "EPOCH 1756/2000\n",
      "Average loss: 0.1572\n",
      "EPOCH 1757/2000\n",
      "Average loss: 0.1560\n",
      "EPOCH 1758/2000\n",
      "Average loss: 0.1570\n",
      "EPOCH 1759/2000\n",
      "Average loss: 0.1563\n",
      "EPOCH 1760/2000\n",
      "Average loss: 0.1550\n",
      "EPOCH 1761/2000\n",
      "Average loss: 0.1553\n",
      "EPOCH 1762/2000\n",
      "Average loss: 0.1559\n",
      "EPOCH 1763/2000\n",
      "Average loss: 0.1564\n",
      "EPOCH 1764/2000\n",
      "Average loss: 0.1552\n",
      "EPOCH 1765/2000\n",
      "Average loss: 0.1552\n",
      "EPOCH 1766/2000\n",
      "Average loss: 0.1553\n",
      "EPOCH 1767/2000\n",
      "Average loss: 0.1557\n",
      "EPOCH 1768/2000\n",
      "Average loss: 0.1545\n",
      "EPOCH 1769/2000\n",
      "Average loss: 0.1551\n",
      "EPOCH 1770/2000\n",
      "Average loss: 0.1558\n",
      "EPOCH 1771/2000\n",
      "Average loss: 0.1552\n",
      "EPOCH 1772/2000\n",
      "Average loss: 0.1548\n",
      "EPOCH 1773/2000\n",
      "Average loss: 0.1554\n",
      "EPOCH 1774/2000\n",
      "Average loss: 0.1562\n",
      "EPOCH 1775/2000\n",
      "Average loss: 0.1560\n",
      "EPOCH 1776/2000\n",
      "Average loss: 0.1551\n",
      "EPOCH 1777/2000\n",
      "Average loss: 0.1561\n",
      "EPOCH 1778/2000\n",
      "Average loss: 0.1556\n",
      "EPOCH 1779/2000\n",
      "Average loss: 0.1548\n",
      "EPOCH 1780/2000\n",
      "Average loss: 0.1542\n",
      "EPOCH 1781/2000\n",
      "Average loss: 0.1542\n",
      "EPOCH 1782/2000\n",
      "Average loss: 0.1543\n",
      "EPOCH 1783/2000\n",
      "Average loss: 0.1544\n",
      "EPOCH 1784/2000\n",
      "Average loss: 0.1543\n",
      "EPOCH 1785/2000\n",
      "Average loss: 0.1545\n",
      "EPOCH 1786/2000\n",
      "Average loss: 0.1546\n",
      "EPOCH 1787/2000\n",
      "Average loss: 0.1542\n",
      "EPOCH 1788/2000\n",
      "Average loss: 0.1547\n",
      "EPOCH 1789/2000\n",
      "Average loss: 0.1549\n",
      "EPOCH 1790/2000\n",
      "Average loss: 0.1545\n",
      "EPOCH 1791/2000\n",
      "Average loss: 0.1540\n",
      "EPOCH 1792/2000\n",
      "Average loss: 0.1537\n",
      "EPOCH 1793/2000\n",
      "Average loss: 0.1544\n",
      "EPOCH 1794/2000\n",
      "Average loss: 0.1540\n",
      "EPOCH 1795/2000\n",
      "Average loss: 0.1540\n",
      "EPOCH 1796/2000\n",
      "Average loss: 0.1540\n",
      "EPOCH 1797/2000\n",
      "Average loss: 0.1537\n",
      "EPOCH 1798/2000\n",
      "Average loss: 0.1534\n",
      "EPOCH 1799/2000\n",
      "Average loss: 0.1541\n",
      "EPOCH 1800/2000\n",
      "Average loss: 0.1538\n",
      "EPOCH 1801/2000\n",
      "Average loss: 0.1533\n",
      "EPOCH 1802/2000\n",
      "Average loss: 0.1524\n",
      "EPOCH 1803/2000\n",
      "Average loss: 0.1531\n",
      "EPOCH 1804/2000\n",
      "Average loss: 0.1533\n",
      "EPOCH 1805/2000\n",
      "Average loss: 0.1521\n",
      "EPOCH 1806/2000\n",
      "Average loss: 0.1520\n",
      "EPOCH 1807/2000\n",
      "Average loss: 0.1527\n",
      "EPOCH 1808/2000\n",
      "Average loss: 0.1529\n",
      "EPOCH 1809/2000\n",
      "Average loss: 0.1523\n",
      "EPOCH 1810/2000\n",
      "Average loss: 0.1521\n",
      "EPOCH 1811/2000\n",
      "Average loss: 0.1526\n",
      "EPOCH 1812/2000\n",
      "Average loss: 0.1527\n",
      "EPOCH 1813/2000\n",
      "Average loss: 0.1528\n",
      "EPOCH 1814/2000\n",
      "Average loss: 0.1518\n",
      "EPOCH 1815/2000\n",
      "Average loss: 0.1518\n",
      "EPOCH 1816/2000\n",
      "Average loss: 0.1527\n",
      "EPOCH 1817/2000\n",
      "Average loss: 0.1516\n",
      "EPOCH 1818/2000\n",
      "Average loss: 0.1518\n",
      "EPOCH 1819/2000\n",
      "Average loss: 0.1524\n",
      "EPOCH 1820/2000\n",
      "Average loss: 0.1516\n",
      "EPOCH 1821/2000\n",
      "Average loss: 0.1532\n",
      "EPOCH 1822/2000\n",
      "Average loss: 0.1530\n",
      "EPOCH 1823/2000\n",
      "Average loss: 0.1529\n",
      "EPOCH 1824/2000\n",
      "Average loss: 0.1525\n",
      "EPOCH 1825/2000\n",
      "Average loss: 0.1530\n",
      "EPOCH 1826/2000\n",
      "Average loss: 0.1524\n",
      "EPOCH 1827/2000\n",
      "Average loss: 0.1520\n",
      "EPOCH 1828/2000\n",
      "Average loss: 0.1532\n",
      "EPOCH 1829/2000\n",
      "Average loss: 0.1516\n",
      "EPOCH 1830/2000\n",
      "Average loss: 0.1529\n",
      "EPOCH 1831/2000\n",
      "Average loss: 0.1521\n",
      "EPOCH 1832/2000\n",
      "Average loss: 0.1520\n",
      "EPOCH 1833/2000\n",
      "Average loss: 0.1530\n",
      "EPOCH 1834/2000\n",
      "Average loss: 0.1519\n",
      "EPOCH 1835/2000\n",
      "Average loss: 0.1510\n",
      "EPOCH 1836/2000\n",
      "Average loss: 0.1514\n",
      "EPOCH 1837/2000\n",
      "Average loss: 0.1521\n",
      "EPOCH 1838/2000\n",
      "Average loss: 0.1522\n",
      "EPOCH 1839/2000\n",
      "Average loss: 0.1505\n",
      "EPOCH 1840/2000\n",
      "Average loss: 0.1514\n",
      "EPOCH 1841/2000\n",
      "Average loss: 0.1521\n",
      "EPOCH 1842/2000\n",
      "Average loss: 0.1511\n",
      "EPOCH 1843/2000\n",
      "Average loss: 0.1524\n",
      "EPOCH 1844/2000\n",
      "Average loss: 0.1516\n",
      "EPOCH 1845/2000\n",
      "Average loss: 0.1516\n",
      "EPOCH 1846/2000\n",
      "Average loss: 0.1509\n",
      "EPOCH 1847/2000\n",
      "Average loss: 0.1509\n",
      "EPOCH 1848/2000\n",
      "Average loss: 0.1499\n",
      "EPOCH 1849/2000\n",
      "Average loss: 0.1496\n",
      "EPOCH 1850/2000\n",
      "Average loss: 0.1515\n",
      "EPOCH 1851/2000\n",
      "Average loss: 0.1500\n",
      "EPOCH 1852/2000\n",
      "Average loss: 0.1506\n",
      "EPOCH 1853/2000\n",
      "Average loss: 0.1511\n",
      "EPOCH 1854/2000\n",
      "Average loss: 0.1502\n",
      "EPOCH 1855/2000\n",
      "Average loss: 0.1499\n",
      "EPOCH 1856/2000\n",
      "Average loss: 0.1507\n",
      "EPOCH 1857/2000\n",
      "Average loss: 0.1506\n",
      "EPOCH 1858/2000\n",
      "Average loss: 0.1497\n",
      "EPOCH 1859/2000\n",
      "Average loss: 0.1501\n",
      "EPOCH 1860/2000\n",
      "Average loss: 0.1511\n",
      "EPOCH 1861/2000\n",
      "Average loss: 0.1506\n",
      "EPOCH 1862/2000\n",
      "Average loss: 0.1502\n",
      "EPOCH 1863/2000\n",
      "Average loss: 0.1489\n",
      "EPOCH 1864/2000\n",
      "Average loss: 0.1494\n",
      "EPOCH 1865/2000\n",
      "Average loss: 0.1490\n",
      "EPOCH 1866/2000\n",
      "Average loss: 0.1500\n",
      "EPOCH 1867/2000\n",
      "Average loss: 0.1496\n",
      "EPOCH 1868/2000\n",
      "Average loss: 0.1494\n",
      "EPOCH 1869/2000\n",
      "Average loss: 0.1506\n",
      "EPOCH 1870/2000\n",
      "Average loss: 0.1501\n",
      "EPOCH 1871/2000\n",
      "Average loss: 0.1489\n",
      "EPOCH 1872/2000\n",
      "Average loss: 0.1480\n",
      "EPOCH 1873/2000\n",
      "Average loss: 0.1489\n",
      "EPOCH 1874/2000\n",
      "Average loss: 0.1492\n",
      "EPOCH 1875/2000\n",
      "Average loss: 0.1499\n",
      "EPOCH 1876/2000\n",
      "Average loss: 0.1481\n",
      "EPOCH 1877/2000\n",
      "Average loss: 0.1489\n",
      "EPOCH 1878/2000\n",
      "Average loss: 0.1492\n",
      "EPOCH 1879/2000\n",
      "Average loss: 0.1497\n",
      "EPOCH 1880/2000\n",
      "Average loss: 0.1493\n",
      "EPOCH 1881/2000\n",
      "Average loss: 0.1489\n",
      "EPOCH 1882/2000\n",
      "Average loss: 0.1482\n",
      "EPOCH 1883/2000\n",
      "Average loss: 0.1493\n",
      "EPOCH 1884/2000\n",
      "Average loss: 0.1480\n",
      "EPOCH 1885/2000\n",
      "Average loss: 0.1493\n",
      "EPOCH 1886/2000\n",
      "Average loss: 0.1487\n",
      "EPOCH 1887/2000\n",
      "Average loss: 0.1485\n",
      "EPOCH 1888/2000\n",
      "Average loss: 0.1495\n",
      "EPOCH 1889/2000\n",
      "Average loss: 0.1485\n",
      "EPOCH 1890/2000\n",
      "Average loss: 0.1474\n",
      "EPOCH 1891/2000\n",
      "Average loss: 0.1490\n",
      "EPOCH 1892/2000\n",
      "Average loss: 0.1487\n",
      "EPOCH 1893/2000\n",
      "Average loss: 0.1482\n",
      "EPOCH 1894/2000\n",
      "Average loss: 0.1491\n",
      "EPOCH 1895/2000\n",
      "Average loss: 0.1484\n",
      "EPOCH 1896/2000\n",
      "Average loss: 0.1483\n",
      "EPOCH 1897/2000\n",
      "Average loss: 0.1485\n",
      "EPOCH 1898/2000\n",
      "Average loss: 0.1485\n",
      "EPOCH 1899/2000\n",
      "Average loss: 0.1481\n",
      "EPOCH 1900/2000\n",
      "Average loss: 0.1477\n",
      "EPOCH 1901/2000\n",
      "Average loss: 0.1471\n",
      "EPOCH 1902/2000\n",
      "Average loss: 0.1485\n",
      "EPOCH 1903/2000\n",
      "Average loss: 0.1472\n",
      "EPOCH 1904/2000\n",
      "Average loss: 0.1470\n",
      "EPOCH 1905/2000\n",
      "Average loss: 0.1472\n",
      "EPOCH 1906/2000\n",
      "Average loss: 0.1477\n",
      "EPOCH 1907/2000\n",
      "Average loss: 0.1467\n",
      "EPOCH 1908/2000\n",
      "Average loss: 0.1473\n",
      "EPOCH 1909/2000\n",
      "Average loss: 0.1472\n",
      "EPOCH 1910/2000\n",
      "Average loss: 0.1478\n",
      "EPOCH 1911/2000\n",
      "Average loss: 0.1460\n",
      "EPOCH 1912/2000\n",
      "Average loss: 0.1482\n",
      "EPOCH 1913/2000\n",
      "Average loss: 0.1480\n",
      "EPOCH 1914/2000\n",
      "Average loss: 0.1473\n",
      "EPOCH 1915/2000\n",
      "Average loss: 0.1475\n",
      "EPOCH 1916/2000\n",
      "Average loss: 0.1474\n",
      "EPOCH 1917/2000\n",
      "Average loss: 0.1473\n",
      "EPOCH 1918/2000\n",
      "Average loss: 0.1479\n",
      "EPOCH 1919/2000\n",
      "Average loss: 0.1475\n",
      "EPOCH 1920/2000\n",
      "Average loss: 0.1463\n",
      "EPOCH 1921/2000\n",
      "Average loss: 0.1467\n",
      "EPOCH 1922/2000\n",
      "Average loss: 0.1466\n",
      "EPOCH 1923/2000\n",
      "Average loss: 0.1469\n",
      "EPOCH 1924/2000\n",
      "Average loss: 0.1470\n",
      "EPOCH 1925/2000\n",
      "Average loss: 0.1473\n",
      "EPOCH 1926/2000\n",
      "Average loss: 0.1469\n",
      "EPOCH 1927/2000\n",
      "Average loss: 0.1462\n",
      "EPOCH 1928/2000\n",
      "Average loss: 0.1464\n",
      "EPOCH 1929/2000\n",
      "Average loss: 0.1458\n",
      "EPOCH 1930/2000\n",
      "Average loss: 0.1463\n",
      "EPOCH 1931/2000\n",
      "Average loss: 0.1466\n",
      "EPOCH 1932/2000\n",
      "Average loss: 0.1460\n",
      "EPOCH 1933/2000\n",
      "Average loss: 0.1463\n",
      "EPOCH 1934/2000\n",
      "Average loss: 0.1465\n",
      "EPOCH 1935/2000\n",
      "Average loss: 0.1459\n",
      "EPOCH 1936/2000\n",
      "Average loss: 0.1453\n",
      "EPOCH 1937/2000\n",
      "Average loss: 0.1460\n",
      "EPOCH 1938/2000\n",
      "Average loss: 0.1463\n",
      "EPOCH 1939/2000\n",
      "Average loss: 0.1462\n",
      "EPOCH 1940/2000\n",
      "Average loss: 0.1460\n",
      "EPOCH 1941/2000\n",
      "Average loss: 0.1457\n",
      "EPOCH 1942/2000\n",
      "Average loss: 0.1456\n",
      "EPOCH 1943/2000\n",
      "Average loss: 0.1460\n",
      "EPOCH 1944/2000\n",
      "Average loss: 0.1448\n",
      "EPOCH 1945/2000\n",
      "Average loss: 0.1461\n",
      "EPOCH 1946/2000\n",
      "Average loss: 0.1449\n",
      "EPOCH 1947/2000\n",
      "Average loss: 0.1460\n",
      "EPOCH 1948/2000\n",
      "Average loss: 0.1454\n",
      "EPOCH 1949/2000\n",
      "Average loss: 0.1454\n",
      "EPOCH 1950/2000\n",
      "Average loss: 0.1447\n",
      "EPOCH 1951/2000\n",
      "Average loss: 0.1442\n",
      "EPOCH 1952/2000\n",
      "Average loss: 0.1468\n",
      "EPOCH 1953/2000\n",
      "Average loss: 0.1450\n",
      "EPOCH 1954/2000\n",
      "Average loss: 0.1445\n",
      "EPOCH 1955/2000\n",
      "Average loss: 0.1456\n",
      "EPOCH 1956/2000\n",
      "Average loss: 0.1450\n",
      "EPOCH 1957/2000\n",
      "Average loss: 0.1450\n",
      "EPOCH 1958/2000\n",
      "Average loss: 0.1461\n",
      "EPOCH 1959/2000\n",
      "Average loss: 0.1451\n",
      "EPOCH 1960/2000\n",
      "Average loss: 0.1458\n",
      "EPOCH 1961/2000\n",
      "Average loss: 0.1462\n",
      "EPOCH 1962/2000\n",
      "Average loss: 0.1455\n",
      "EPOCH 1963/2000\n",
      "Average loss: 0.1458\n",
      "EPOCH 1964/2000\n",
      "Average loss: 0.1453\n",
      "EPOCH 1965/2000\n",
      "Average loss: 0.1448\n",
      "EPOCH 1966/2000\n",
      "Average loss: 0.1456\n",
      "EPOCH 1967/2000\n",
      "Average loss: 0.1456\n",
      "EPOCH 1968/2000\n",
      "Average loss: 0.1442\n",
      "EPOCH 1969/2000\n",
      "Average loss: 0.1444\n",
      "EPOCH 1970/2000\n",
      "Average loss: 0.1438\n",
      "EPOCH 1971/2000\n",
      "Average loss: 0.1445\n",
      "EPOCH 1972/2000\n",
      "Average loss: 0.1446\n",
      "EPOCH 1973/2000\n",
      "Average loss: 0.1436\n",
      "EPOCH 1974/2000\n",
      "Average loss: 0.1450\n",
      "EPOCH 1975/2000\n",
      "Average loss: 0.1446\n",
      "EPOCH 1976/2000\n",
      "Average loss: 0.1442\n",
      "EPOCH 1977/2000\n",
      "Average loss: 0.1425\n",
      "EPOCH 1978/2000\n",
      "Average loss: 0.1437\n",
      "EPOCH 1979/2000\n",
      "Average loss: 0.1438\n",
      "EPOCH 1980/2000\n",
      "Average loss: 0.1444\n",
      "EPOCH 1981/2000\n",
      "Average loss: 0.1441\n",
      "EPOCH 1982/2000\n",
      "Average loss: 0.1445\n",
      "EPOCH 1983/2000\n",
      "Average loss: 0.1424\n",
      "EPOCH 1984/2000\n",
      "Average loss: 0.1442\n",
      "EPOCH 1985/2000\n",
      "Average loss: 0.1440\n",
      "EPOCH 1986/2000\n",
      "Average loss: 0.1430\n",
      "EPOCH 1987/2000\n",
      "Average loss: 0.1430\n",
      "EPOCH 1988/2000\n",
      "Average loss: 0.1437\n",
      "EPOCH 1989/2000\n",
      "Average loss: 0.1430\n",
      "EPOCH 1990/2000\n",
      "Average loss: 0.1426\n",
      "EPOCH 1991/2000\n",
      "Average loss: 0.1444\n",
      "EPOCH 1992/2000\n",
      "Average loss: 0.1439\n",
      "EPOCH 1993/2000\n",
      "Average loss: 0.1433\n",
      "EPOCH 1994/2000\n",
      "Average loss: 0.1436\n",
      "EPOCH 1995/2000\n",
      "Average loss: 0.1441\n",
      "EPOCH 1996/2000\n",
      "Average loss: 0.1438\n",
      "EPOCH 1997/2000\n",
      "Average loss: 0.1438\n",
      "EPOCH 1998/2000\n",
      "Average loss: 0.1428\n",
      "EPOCH 1999/2000\n",
      "Average loss: 0.1428\n",
      "EPOCH 2000/2000\n",
      "Average loss: 0.1433\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "#get index every seq_len, starting at idx 0\n",
    "idx = list(range(0, len(train_data)-1, seq_len))\n",
    "num_batches = len(idx) // batch_size\n",
    "if len(idx) % batch_size != 0:\n",
    "    num_batches += 1\n",
    "\n",
    "SHUFFLE_BATCHES = True\n",
    "random.seed(42)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print(f\"EPOCH {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    if SHUFFLE_BATCHES:\n",
    "        random.shuffle(idx)\n",
    "\n",
    "    batch_nums = list(range(num_batches))\n",
    "    for i in batch_nums:\n",
    "        \n",
    "        batch_start = i * batch_size\n",
    "        batch_end = min((i+1) * batch_size, len(idx))\n",
    "        batch_idx = idx[batch_start:batch_end]\n",
    "\n",
    "        # Get the batch data\n",
    "        batch = torch.stack([train_data[i:i+seq_len+1] for i in batch_idx], dim=0)\n",
    "        x = batch[:, :-1].contiguous()\n",
    "        y = batch[:, 1:].contiguous()\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss, output = model(x, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    avg_loss = sum(losses[-num_batches:]) / num_batches\n",
    "    print(f\"Average loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved with loss 0.1467\n"
     ]
    }
   ],
   "source": [
    "save_checkpoint(model, {\n",
    "    'vocab_size': vocab_size,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'seq_len': seq_len,\n",
    "    'num_blocks': num_blocks\n",
    "}, optimizer, losses, filename=\"shakespeare/checkpoints/mixer/checkpoint_tMLP.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 65, 'hidden_dim': 512, 'seq_len': 512, 'num_blocks': 6}\n",
      "Checkpoint loaded: loss 0.1467\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, losses = load_checkpoint(\"shakespeare/checkpoints/mixer/checkpoint_tMLP.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "downsample = 1\n",
    "window = 10\n",
    "\n",
    "temp = []\n",
    "for epoch in losses[num_batches*2::downsample]:\n",
    "    avg = np.mean(epoch)\n",
    "    temp.append(avg)\n",
    "temp = np.convolve(temp, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(temp, label=\"Training Loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n",
      "Generated text:\n",
      "We are accounted poor citizens, the patricians good. What authority surfeits on would relieve us: if they would yield us but the superfluity, while it were wholesome, we might guess they relieved us humanely;\n",
      "but there's come to be a ll hip ell what show.\n",
      "\n",
      "LUCIO:\n",
      "There was not at the trial desperate,\n",
      "My words be I have show'd to heaven grant's hours\n",
      "Are pleasure to be done, to ste the city affect.\n",
      "\n",
      "GLOUCESTER:\n",
      "You take my leave to make my voice of all,\n",
      "Provoke me for the state.\n",
      "\n",
      "MERCUTIO:\n",
      "The solemn of \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate(model, prompt, generation_length=100, seq_len=128):\n",
    "    \"\"\"\n",
    "    Generates text from the model given a prompt.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained MLPMixer model.\n",
    "        prompt (str): The prompt to condition generation on.\n",
    "        generation_length (int): Number of characters to generate.\n",
    "        seq_len (int): The sequence length the model expects.\n",
    "        temperature (float): Temperature parameter for sampling.\n",
    "    \n",
    "    Returns:\n",
    "        str: The prompt plus the generated text.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    # Convert prompt into a list of token ids\n",
    "    tokens = [char2id[c] for c in prompt]\n",
    "    \n",
    "    # Autoregressive generation loop\n",
    "    with torch.no_grad():\n",
    "        for _ in range(generation_length):\n",
    "            # Prepare the input context:\n",
    "            # If the context is shorter than seq_len, pad to the left (using the token for space)\n",
    "            if len(tokens) < seq_len:\n",
    "                # pad with the space character (or any chosen pad token)\n",
    "                padded = [char2id[' ']] * (seq_len - len(tokens)) + tokens\n",
    "            else:\n",
    "                padded = tokens[-seq_len:]\n",
    "            \n",
    "            # Convert to a tensor and add batch dimension\n",
    "            x = torch.tensor(padded, dtype=torch.long, device=device).unsqueeze(0)  # shape (1, seq_len)\n",
    "            \n",
    "            # Get logits from the model; output shape is (1, seq_len, vocab_size)\n",
    "            logits = model(x)\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            #intermediate string output\n",
    "            temp = [id2char[t] for t in predictions[0].tolist()]\n",
    "            # print(''.join(temp))\n",
    "\n",
    "            next_token = predictions[0][-1].item()\n",
    "            tokens.append(next_token)\n",
    "    \n",
    "    # Convert the list of tokens back to a string\n",
    "    generated_text = ''.join([id2char[t] for t in tokens])\n",
    "    return generated_text\n",
    "\n",
    "# Example usage:\n",
    "prompt = \"We are accounted poor citizens, the patricians good. What authority surfeits on would relieve us: if they would yield us but the superfluity, while it were wholesome, we might guess they relieved us humanely;\"\n",
    "print(len(prompt))\n",
    "generated = evaluate(model, prompt, generation_length=300, seq_len=seq_len)\n",
    "print(\"Generated text:\")\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
