{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Classification / Generation using AUNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#deallocate all cuda memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#print cuda memory\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_dim:int, \n",
    "        output_dim:int, \n",
    "        num_layers:int, \n",
    "        hidden_dim:int):        \n",
    "\n",
    "        assert num_layers >= 2, \"Number of layers must be at least 2\"\n",
    "\n",
    "        super(AUNNModel, self).__init__() \n",
    "    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # # Initialize fourier embedding parameters\n",
    "        # thresh = 0.1\n",
    "        # context_len = 4\n",
    "        # alpha = -np.log(thresh) / (context_len ** 2)\n",
    "        # M = embedding_dim // 2\n",
    "        # w_init = np.random.normal(0, np.sqrt(2 * alpha), size=M).astype(np.float32)\n",
    "        # b_init = np.random.uniform(0, 2 * np.pi, size=M).astype(np.float32)\n",
    "        # self.register_buffer(\"w\", torch.tensor(w_init, device=device))\n",
    "        # self.register_buffer(\"b\", torch.tensor(b_init, device=device))\n",
    "\n",
    "        # Input Layer\n",
    "        self.input_layer =  nn.Linear(self.embedding_dim, self.hidden_dim)\n",
    "\n",
    "        # Hidden Layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(self.num_layers - 2):  # Exclude input and output layers\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "                nn.SiLU()\n",
    "            ))\n",
    "\n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # Kaiming He initialization for Swish activation\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    #transformer style sinuisoidal\n",
    "    def encode(self, x: torch.Tensor):\n",
    "        \"\"\"Apply fourier positional encoding to the input.\n",
    "\n",
    "        Args:\n",
    "        x (torch.Tensor): a 1 dimension tensor of indices\n",
    "        dim (optional, int): dimension of positional encoding. max index representable is 2^(dim//2+1). Default: 64.\n",
    "        Returns:\n",
    "        (torch.Tensor): Positional encoding of the input tensor. dimension: [x.size(0), dim]\n",
    "        \"\"\"\n",
    "        dim = self.embedding_dim\n",
    "        position = x.unsqueeze(1)\n",
    "        device = x.device  # Get the device of x\n",
    "\n",
    "        # Create div_term on the same device as x\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, dim, 2, dtype=torch.float32, device=device) *\\\n",
    "                (-np.log(10000.0) / dim)\n",
    "        )\n",
    "\n",
    "        # Create pe on the same device as x\n",
    "        pe = torch.zeros(x.size(0), dim, device=device)\n",
    "\n",
    "        # Perform computations\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "    # #sinusoidal binary\n",
    "    # def encode(self, x: torch.Tensor):\n",
    "\n",
    "    #     dim = self.embedding_dim\n",
    "    #     assert dim % 2 == 0, \"Encoding dimension (dim) must be even.\"\n",
    "\n",
    "    #     # Determine the number of frequencies\n",
    "    #     num_frequencies = dim // 2\n",
    "\n",
    "    #     # Frequencies corresponding to powers of two, starting from 2^2\n",
    "    #     frequency_powers = torch.arange(2, 2 + num_frequencies, dtype=torch.float32, device=x.device)\n",
    "    #     frequencies = 2 ** frequency_powers  # Shape: [num_frequencies]\n",
    "\n",
    "    #     # Compute the angles: [N, num_frequencies]\n",
    "    #     x = x.unsqueeze(1)  # Shape: [N, 1]\n",
    "    #     angles = (2 * torch.pi * x) / frequencies  # Broadcasting over x and frequencies\n",
    "\n",
    "    #     # Compute the positional encodings\n",
    "    #     encoding = torch.zeros(x.size(0), dim, device=x.device)\n",
    "    #     encoding[:, 0::2] = torch.sin(angles)  # Even indices: sin\n",
    "    #     encoding[:, 1::2] = torch.cos(angles)  # Odd indices: cos\n",
    "\n",
    "    #     return encoding\n",
    "\n",
    "    # #binary\n",
    "    # def encode(self, x: torch.Tensor):\n",
    "    #     dim = self.embedding_dim\n",
    "    #     encoding = ((x.unsqueeze(1) >> torch.arange(dim, device=x.device)) & 1).to(torch.float32)\n",
    "    #     return encoding\n",
    "    \n",
    "    # #fourier\n",
    "    # def encode(self, x: torch.Tensor):\n",
    "    #     inps = torch.outer(x, self.w) + self.b\n",
    "    #     scale = np.sqrt(2.0 / self.embedding_dim)\n",
    "    #     embed = scale * torch.cat([torch.cos(inps), torch.sin(inps)], dim=-1)\n",
    "    #     return embed\n",
    "\n",
    "    def forward(self, indices):\n",
    "        \n",
    "        x = self.encode(indices)\n",
    "        x = self.input_layer(x)\n",
    "        x = x + nn.SiLU()(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)  # MLP output with skip connection\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to save the model checkpoint\n",
    "def save_checkpoint(model, params, optimizer, losses, filename=\"checkpoint.pth\"):\n",
    "    \n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'losses': losses,\n",
    "        'params':{}\n",
    "    }\n",
    "\n",
    "    keys = ['embedding_dim', 'output_dim', 'num_layers', 'hidden_dim']\n",
    "    assert all(k in params for k in keys)\n",
    "    for k in keys:\n",
    "        checkpoint['params'][k] = params[k]\n",
    "\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved with loss {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filename=\"checkpoint.pth\"):\n",
    "\n",
    "    checkpoint = torch.load(filename, weights_only=True)\n",
    "    \n",
    "    params = checkpoint['params']\n",
    "    model = AUNNModel(**params)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    losses = checkpoint['losses']\n",
    "\n",
    "    print(f\"Checkpoint loaded: loss {losses[-1]:.4f}\")\n",
    "\n",
    "    return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from array import array\n",
    "\n",
    "def load_mnist(images_path, labels_path, shuffle=False, seed=42):\n",
    "\n",
    "    labels = []\n",
    "    with open(labels_path, 'rb') as file:\n",
    "        magic, size = struct.unpack(\">II\", file.read(8)) \n",
    "        if magic != 2049:\n",
    "            raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "        labels = array(\"B\", file.read())\n",
    "\n",
    "    images = []\n",
    "    rows, cols = None, None\n",
    "    with open(images_path, 'rb') as file:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        if magic != 2051:\n",
    "            raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "        data = array(\"B\", file.read())\n",
    "        for i in range(size):\n",
    "            img = np.array(data[i * rows * cols:(i + 1) * rows * cols], dtype=np.uint8)\n",
    "            img = np.where(img > 0, 1, 0) #binarize\n",
    "            img.resize((rows, cols))\n",
    "            images.append(img)\n",
    "\n",
    "    assert len(images) == len(labels)\n",
    "\n",
    "    if shuffle:\n",
    "        random.seed(seed)\n",
    "        indices = list(range(len(images)))\n",
    "        random.shuffle(indices)\n",
    "        images = [images[i] for i in indices]\n",
    "        labels = [labels[i] for i in indices]\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def make_bitstring(images, labels):\n",
    "    \n",
    "    targets = []\n",
    "    for img, label in zip(images, labels):\n",
    "\n",
    "        #add image to inputs\n",
    "        img_data = img.flatten()\n",
    "        targets.append(img_data)\n",
    "\n",
    "        #create binary label (4 bits), not one-hot encoded\n",
    "        label = np.array([label])\n",
    "        label = label >> np.arange(4) & 1\n",
    "        num_repeats = len(img_data) // 4\n",
    "        assert len(img_data) % 4 == 0, \"Image data length must be divisible by 4\"\n",
    "        label = np.concatenate([label] * num_repeats)\n",
    "        targets.append(label)\n",
    "\n",
    "    targets = np.concatenate(targets, axis=0)\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cur_dir = Path().resolve()\n",
    "input_path = cur_dir / 'mnist'\n",
    "training_images_filepath = input_path / 'train-images.idx3-ubyte'\n",
    "training_labels_filepath = input_path /'train-labels.idx1-ubyte'\n",
    "test_images_filepath = input_path / 't10k-images.idx3-ubyte'\n",
    "test_labels_filepath = input_path / 't10k-labels.idx1-ubyte'\n",
    "\n",
    "train_images, train_labels = load_mnist(training_images_filepath, training_labels_filepath, shuffle=True)\n",
    "train_bitstring = make_bitstring(train_images, train_labels)\n",
    "print(f\"{len(train_bitstring):,} training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_bitstring[0:784]\n",
    "img = img.reshape(28,28)\n",
    "print(img)\n",
    "\n",
    "lbl = train_bitstring[784:1568][0:4]\n",
    "print(lbl)\n",
    "\n",
    "img = train_bitstring[1568:1568+784]\n",
    "img = img.reshape(28,28)\n",
    "print(img)\n",
    "\n",
    "lbl = train_bitstring[1568+784:1568+1568][0:4]\n",
    "print(lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "embedd_dim = 128\n",
    "num_layers = 12    # Must be even and at least 2 (bc of skip connections)\n",
    "hidden_dim = 512  # Size of hidden layers\n",
    "output_dim = 2\n",
    "batch_size = 4096\n",
    "num_epochs = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "\n",
    "model = AUNNModel(embedd_dim, output_dim, num_layers, hidden_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "print(f\"Model has {model.count_params():,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_len = 784 * 2\n",
    "num_ex = 10\n",
    "# train_bitstring = np.concat([train_bitstring[0:ex_len*num_ex]] * 1000)\n",
    "# train_bitstring = torch.tensor(train_bitstring, dtype=torch.uint8).to(device)\n",
    "# train_bitstring = torch.tensor(train_bitstring[0:ex_len*num_ex], dtype=torch.uint8).to(device)\n",
    "# train_bitstring = torch.tensor(train_bitstring, dtype=torch.uint8).to(device)\n",
    "print(len(train_bitstring))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "accumulation_steps = 1\n",
    "\n",
    "num_batches = len(train_bitstring) // batch_size\n",
    "if len(train_bitstring) % batch_size != 0:\n",
    "    num_batches += 1\n",
    "\n",
    "# num_batches = 10\n",
    "\n",
    "for epoch in tqdm(list(range(num_epochs))):\n",
    "    for batch_num in tqdm(list(range(num_batches)), leave=False, disable=False):\n",
    "\n",
    "        start = batch_num * batch_size\n",
    "        end = start + batch_size\n",
    "        end = min(end, len(train_bitstring))\n",
    "        targets = train_bitstring[start:end]\n",
    "        inputs = torch.arange(start, end, device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Scale loss for gradient accumulation\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization every accumulation_steps batches\n",
    "        if (batch_num + 1) % accumulation_steps == 0 or batch_num == num_batches - 1:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Clip gradients\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True) # Flush gradients\n",
    "\n",
    "        if loss != loss:\n",
    "            print(\"ERR loss is NaN\")\n",
    "\n",
    "        cur_loss = loss.item()\n",
    "        losses.append(cur_loss * accumulation_steps)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        accuracy = (preds == targets).float().mean().item()\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        # Batch logging\n",
    "        if batch_num % 1000 == 0 and batch_num != 0:\n",
    "            avg_loss = np.mean(losses[-1000:])\n",
    "            avg_accuracy = np.mean(accuracies[-1000:])\n",
    "            print(f\"Batch [{batch_num}/{num_batches}], Loss: {avg_loss:.8f}, Accuracy: {avg_accuracy:.8f}\", end=\"\\r\")\n",
    "\n",
    "        # if batch_num == 10: break\n",
    "\n",
    "    # Epoch logging\n",
    "    avg_loss = np.mean(losses[-num_batches:])\n",
    "    avg_accuracy = np.mean(accuracies[-num_batches:])\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.8f}, Accuracy: {avg_accuracy:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curves\n",
    "temp = [x if x < 1 else 1 for x in losses] #clip losses to 1\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(temp, label=\"Training Loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(model, {\n",
    "    'embedding_dim': embedd_dim,\n",
    "    'output_dim': output_dim,\n",
    "    'num_layers': num_layers,\n",
    "    'hidden_dim': hidden_dim\n",
    "}, optimizer, losses, filename=\"mnist/checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_num = 0\n",
    "ex_len = 784 * 2\n",
    "bitstring = train_bitstring[entry_num*ex_len:(entry_num+1)*ex_len].cpu().numpy()\n",
    "\n",
    "img = bitstring[0:784]\n",
    "lbl = bitstring[784:784*2]\n",
    "print(img.reshape(28, 28))\n",
    "print(lbl.reshape(-1, 4))\n",
    "\n",
    "start = entry_num * ex_len\n",
    "end = start + ex_len\n",
    "indices = torch.arange(start, end, device=device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(indices)\n",
    "    outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "outputs = outputs.cpu().numpy()\n",
    "img = outputs[0:784]\n",
    "lbl = outputs[784:784*2]\n",
    "print(img.reshape(28, 28))\n",
    "print(lbl.reshape(-1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_num = 1000\n",
    "start = entry_num * ex_len\n",
    "end = start + ex_len\n",
    "indices = torch.arange(start, end, device=device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(indices)\n",
    "    outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "outputs = outputs.cpu().numpy()\n",
    "img = outputs[0:784]\n",
    "lbl = outputs[784:784*2]\n",
    "print(img.reshape(28, 28))\n",
    "print(lbl.reshape(-1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt conditioning\n",
    "bos = np.ones(10, dtype=np.uint8)\n",
    "lbl = np.zeros(10, dtype=np.uint8)\n",
    "lbl[5] = 1\n",
    "\n",
    "target = np.concatenate([bos, lbl], axis=0)\n",
    "target = torch.tensor(target, dtype=torch.uint8, device=device)\n",
    "indices = torch.arange(100000000, 100000000+len(target), device=device)\n",
    "\n",
    "#train the model\n",
    "model.train()\n",
    "num_steps = 10\n",
    "for _ in range(num_steps):\n",
    "    outputs = model(indices)\n",
    "    loss = criterion(outputs, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    print(loss.item())\n",
    "\n",
    "#test the model\n",
    "model.eval()\n",
    "entry_len = 814\n",
    "indices = torch.arange(100000000, 100000000+entry_len, device=device)\n",
    "outputs = model(indices)\n",
    "outputs = torch.argmax(outputs, dim=1)\n",
    "outputs = outputs.cpu().numpy()\n",
    "\n",
    "bos = outputs[:10]\n",
    "lbl = outputs[10:20]\n",
    "img = outputs[20:20+784]\n",
    "lbl2 = outputs[20+784:20+784+10]\n",
    "\n",
    "print('-'*20)\n",
    "print(bos)\n",
    "print(lbl)\n",
    "print(img.reshape(28, 28))\n",
    "print(lbl2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
