{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Classification / Generation using AUNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notes: \n",
    "\n",
    "Probably there's two ways to do this\n",
    "1. By creating a purely binary output layer, binarizing MNIST images, and having classification labels be a one-hot encoding appended to the end\n",
    "2. Having an output dimension of 1+num_classes, encoding binary mnist purely in the 1st dimension, and outputing class after the image via other dimensions.\n",
    "\n",
    "I think I'll initially go with method 1 because having a single dimensional output feels simpler, but can always change later on if needed. Also, to the extent that's possible, we probably want images and labels aligned to borders that are powers of 2, since that'll likely make detecting structural patterns much easier for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#deallocate all cuda memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#print cuda memory\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def positional_encoding(x: torch.Tensor, dim: int = 64):\n",
    "#     \"\"\"Apply fourier positional encoding to the input.\n",
    "\n",
    "#     Args:\n",
    "#       x (torch.Tensor): a 1 dimension tensor of indices\n",
    "#       dim (optional, int): dimension of positional encoding. max index representable is 2^(dim//2+1). Default: 64.\n",
    "#     Returns:\n",
    "#       (torch.Tensor): Positional encoding of the input tensor. dimension: [x.size(0), dim]\n",
    "#     \"\"\"\n",
    "#     position = x.unsqueeze(1)\n",
    "#     device = x.device  # Get the device of x\n",
    "\n",
    "#     # Create div_term on the same device as x\n",
    "#     div_term = torch.exp(\n",
    "#         torch.arange(0, dim, 2, dtype=torch.float32, device=device) *\\\n",
    "#             (-np.log(10000.0) / dim)\n",
    "#     )\n",
    "\n",
    "#     # Create pe on the same device as x\n",
    "#     pe = torch.zeros(x.size(0), dim, device=device)\n",
    "\n",
    "#     # Perform computations\n",
    "#     pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#     pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#     return pe\n",
    "\n",
    "\n",
    "# def positional_encoding(x, dim: int = 48):\n",
    "#     \"\"\"\n",
    "#     Binary positional encoding, where each dimension is a bit in the binary representation of the index.\n",
    "    \n",
    "#     Args:\n",
    "#         x: Input tensor of positions with shape [N]\n",
    "#         dim (int): Number of bits in the binary encoding (output dimension). Default is 48.\n",
    "\n",
    "#     Returns:\n",
    "#         torch.Tensor: A binary encoding tensor with shape [N, dim] where each bit represents a binary position.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Each row corresponds to an element in x; columns are the binary bits\n",
    "#     encoding = ((x.unsqueeze(1) >> torch.arange(dim, device=x.device)) & 1).to(torch.float32)\n",
    "\n",
    "#     return encoding\n",
    "\n",
    "\n",
    "def positional_encoding(x: torch.Tensor, dim: int = 128):\n",
    "    \"\"\"\n",
    "    Positional encoding using sine and cosine functions with frequencies as powers of two,\n",
    "    starting from 2^2 (i.e., skipping 2^0 and 2^1).\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor of positions with shape [N]\n",
    "        dim (int): Total dimension of the encoding. Must be even.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor with shape [N, dim] containing the positional encodings.\n",
    "    \"\"\"\n",
    "    assert dim % 2 == 0, \"Encoding dimension (dim) must be even.\"\n",
    "\n",
    "    # Determine the number of frequencies\n",
    "    num_frequencies = dim // 2\n",
    "\n",
    "    # Frequencies corresponding to powers of two, starting from 2^2\n",
    "    frequency_powers = torch.arange(2, 2 + num_frequencies, dtype=torch.float32, device=x.device)\n",
    "    frequencies = 2 ** frequency_powers  # Shape: [num_frequencies]\n",
    "\n",
    "    # Compute the angles: [N, num_frequencies]\n",
    "    x = x.unsqueeze(1)  # Shape: [N, 1]\n",
    "    angles = (2 * torch.pi * x) / frequencies  # Broadcasting over x and frequencies\n",
    "\n",
    "    # Compute the positional encodings\n",
    "    encoding = torch.zeros(x.size(0), dim, device=x.device)\n",
    "    encoding[:, 0::2] = torch.sin(angles)  # Even indices: sin\n",
    "    encoding[:, 1::2] = torch.cos(angles)  # Odd indices: cos\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUNNModel(nn.Module):\n",
    "    def __init__(self, embedding_dim:int, output_dim:int, num_layers:int, hidden_dim:int):\n",
    "\n",
    "        super(AUNNModel, self).__init__()\n",
    "        \n",
    "        assert num_layers % 2 == 0 and num_layers >= 2, \"Number of layers must be even and at least 2.\"\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Input Layer\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            # nn.RMSNorm(self.hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Hidden Layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(self.num_layers - 2):  # Exclude input and output layers\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "                nn.SiLU(),\n",
    "                # nn.RMSNorm(self.hidden_dim)\n",
    "            ))\n",
    "\n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # He initialization for Swish activation\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        output = self.input_layer(x)\n",
    "        residual = output  # Initialize residual for skip connections\n",
    "\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            output = layer(output)\n",
    "\n",
    "            # Apply skip connection every two layers\n",
    "            if (idx + 1) % 2 == 0:\n",
    "                output = output + residual  # Skip connection\n",
    "                residual = output  # Update residual\n",
    "\n",
    "        output = self.output_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from array import array\n",
    "\n",
    "def load_mnist(images_path, labels_path):\n",
    "\n",
    "    labels = []\n",
    "    with open(labels_path, 'rb') as file:\n",
    "        magic, size = struct.unpack(\">II\", file.read(8)) \n",
    "        if magic != 2049:\n",
    "            raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "        labels = array(\"B\", file.read())\n",
    "\n",
    "    images = []\n",
    "    rows, cols = None, None\n",
    "    with open(images_path, 'rb') as file:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        if magic != 2051:\n",
    "            raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "        data = array(\"B\", file.read())\n",
    "        for i in range(size):\n",
    "            img = np.array(data[i * rows * cols:(i + 1) * rows * cols], dtype=np.uint8)\n",
    "            img = np.where(img > 0, 1, 0) #binarize\n",
    "            img.resize((rows, cols))\n",
    "            images.append(img)\n",
    "\n",
    "    assert len(images) == len(labels)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cur_dir = Path().resolve()\n",
    "input_path = cur_dir / 'mnist'\n",
    "training_images_filepath = input_path / 'train-images.idx3-ubyte'\n",
    "training_labels_filepath = input_path /'train-labels.idx1-ubyte'\n",
    "test_images_filepath = input_path / 't10k-images.idx3-ubyte'\n",
    "test_labels_filepath = input_path / 't10k-labels.idx1-ubyte'\n",
    "\n",
    "# images, labels = load_mnist(test_images_filepath, test_labels_filepath)\n",
    "images, labels = load_mnist(training_images_filepath, training_labels_filepath)\n",
    "\n",
    "keep_labels = [0,1,2,3] #DEBUG\n",
    "label_inds = {}\n",
    "for i, lbl in enumerate(labels):\n",
    "    if lbl in keep_labels:\n",
    "        if lbl not in label_inds:\n",
    "            label_inds[lbl] = []\n",
    "        if len(label_inds[lbl]) < 5500:\n",
    "            label_inds[lbl].append(i)\n",
    "        \n",
    "indices = []\n",
    "for lbl in label_inds:\n",
    "    indices.extend(label_inds[lbl])\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(indices)\n",
    "\n",
    "images = [images[i] for i in indices]\n",
    "labels = [labels[i] for i in indices]\n",
    "\n",
    "num_entries = len(images)\n",
    "img_size = len(images[0].flatten())\n",
    "\n",
    "plt.imshow(images[0], cmap='gray')\n",
    "plt.show()\n",
    "print(labels[0])\n",
    "print(num_entries, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "embedd_dim = 128\n",
    "num_layers = 8     # Must be even and at least 2 (bc of skip connections)\n",
    "hidden_dim = 256   # Size of hidden layers\n",
    "output_dim = 2\n",
    "num_epochs = 100000\n",
    "entries_per_batch = 400\n",
    "assert num_entries % entries_per_batch == 0, \"num_entries must be divisible by entries_per_batch\"\n",
    "num_batches = num_entries // entries_per_batch\n",
    "print(num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_provider(\n",
    "    images, \n",
    "    labels, \n",
    "    num_epochs,\n",
    "    batch_shuffle=True):\n",
    "\n",
    "    print(len(images), len(labels))\n",
    "\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    N = 0\n",
    "\n",
    "    for img, label in zip(images, labels):\n",
    "\n",
    "        img_data = img.flatten()\n",
    "        targets.append(img_data)\n",
    "\n",
    "        #one hot encode label\n",
    "        label_data = np.zeros(len(keep_labels), dtype=np.uint8)\n",
    "        label_data[label] = 1\n",
    "        targets.append(label_data)\n",
    "\n",
    "        start = N * 1024\n",
    "        indices = torch.arange(start, start + len(img_data) + len(label_data))\n",
    "        embedds = positional_encoding(indices, embedd_dim)\n",
    "        inputs.append(embedds)\n",
    "        N += 1\n",
    "\n",
    "    targets = np.concatenate(targets, axis=0)\n",
    "    targets = torch.tensor(targets, dtype=torch.uint8)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    inputs = torch.cat(inputs, dim=0)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    print(inputs.size(), targets.size())\n",
    "\n",
    "    batch_indices = list(range(num_batches))\n",
    "    if batch_shuffle:\n",
    "        random.shuffle(batch_indices)\n",
    "\n",
    "    for epoch in tqdm(list(range(num_epochs))):\n",
    "\n",
    "        for batch_idx in batch_indices:\n",
    "\n",
    "            batch_start = batch_idx * entries_per_batch * (len(img_data) + len(label_data))\n",
    "            batch_end = (batch_idx + 1) * entries_per_batch * (len(img_data) + len(label_data))\n",
    "\n",
    "            yield epoch, inputs[batch_start:batch_end], targets[batch_start:batch_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = AUNNModel(embedd_dim, output_dim, num_layers, hidden_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "model.train()\n",
    "\n",
    "#set learning rate\n",
    "\n",
    "last_epoch = 0\n",
    "epoch_losses = []\n",
    "batch_num = 0\n",
    "for epoch, inputs, targets in data_provider(images, labels, num_epochs):\n",
    "    \n",
    "    if epoch != last_epoch: # epoch logging\n",
    "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch [{last_epoch}/{num_epochs}] completed, Loss: {avg_loss:.8f}\")\n",
    "        batch_num = 0\n",
    "        epoch_losses = []\n",
    "        last_epoch = epoch\n",
    "        \n",
    "    batch_num += 1\n",
    "    \n",
    "    # do optimization\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if loss != loss:\n",
    "        print(\"ERR loss is NaN\")\n",
    "\n",
    "    cur_loss = loss.item()\n",
    "    epoch_losses.append(cur_loss)\n",
    "\n",
    "    # if batch_num % 10 == 0: # batch logging\n",
    "    #     print(f\"Epoch [{last_epoch}/{num_epochs}], Batch [{batch_num}/{num_batches}], Loss: {cur_loss:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curves\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_output(start_idx):\n",
    "    model.eval()\n",
    "    indices = torch.arange(start_idx, start_idx+img_size+len(keep_labels)).to(device)\n",
    "    inputs = positional_encoding(indices, embedd_dim)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    img = predicted[:img_size].cpu().numpy().reshape(28, 28)\n",
    "    label = predicted[img_size:img_size+10].cpu().numpy()\n",
    "    label = np.argmax(label)\n",
    "    return img, label\n",
    "\n",
    "start = num_entries - 2\n",
    "# start = 0\n",
    "for idx in range(start, start+10):\n",
    "    seq_start = idx * 1024\n",
    "    img, label = make_output(seq_start)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 0\n",
    "seq_start = item * 1024\n",
    "img, label = make_output(seq_start)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "print(label)\n",
    "\n",
    "item = 80\n",
    "seq_start = item * 1024\n",
    "img, label = make_output(seq_start)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 0\n",
    "image = images[item]\n",
    "label = labels[item]\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "print(label)\n",
    "\n",
    "item = 80\n",
    "image = images[item]\n",
    "label = labels[item]\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before conditioning\n",
    "print('before conditioning')\n",
    "seq_start = 80 * 1024\n",
    "img, label = make_output(seq_start)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "print(label)\n",
    "\n",
    "print('-'*80)\n",
    "\n",
    "#now let's teach the model that the correct pixels for item 80 are the pixels of item 0\n",
    "#then we'll see if the model changes the corresponding label of 80 from \"1\" to \"0\"\n",
    "\n",
    "target = images[0].flatten()\n",
    "target = torch.tensor(target, dtype=torch.uint8).to(device)\n",
    "inputs = torch.arange(seq_start, seq_start + img_size).to(device)\n",
    "inputs = positional_encoding(inputs, embedd_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "conditioning_optimizer = optim.SGD(model.parameters())\n",
    "model.train()\n",
    "num_conditioning_steps = 100\n",
    "\n",
    "for step in range(num_conditioning_steps):\n",
    "    conditioning_optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, target)\n",
    "    loss.backward()\n",
    "    conditioning_optimizer.step()\n",
    "    print(f\"Conditioning Step {step+1}/{num_conditioning_steps}, Loss: {loss.item():.6f}\")\n",
    "    img, label = make_output(seq_start)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "    print(label)\n",
    "\n",
    "print('-'*80)\n",
    "print('after conditioning')\n",
    "img, label = make_output(seq_start)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to save the model checkpoint\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filename=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved at epoch {epoch} with loss {loss:.4f}\")\n",
    "\n",
    "save_checkpoint(model, optimizer, epoch+1, avg_loss, filename=f\"mnist/checkpoint_epoch_{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, filename=\"checkpoint.pth\"):\n",
    "    checkpoint = torch.load(filename, weights_only=True)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Checkpoint loaded: epoch {epoch}, loss {loss:.4f}\")\n",
    "    return epoch, loss\n",
    "\n",
    "model = AUNNModel(embedd_dim, output_dim, num_layers, hidden_dim).to(device)\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "start_epoch, start_loss = load_checkpoint(model, optimizer, filename=\"mnist/checkpoint_epoch_1000.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
