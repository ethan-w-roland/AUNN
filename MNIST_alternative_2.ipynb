{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Classification / Generation using AUNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup & Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#deallocate all cuda memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#print cuda memory\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(x, dim: int = 48):\n",
    "    \"\"\"\n",
    "    Binary positional encoding, where each dimension is a bit in the binary representation of the index.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor of positions with shape [N]\n",
    "        dim (int): Number of bits in the binary encoding (output dimension). Default is 48.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A binary encoding tensor with shape [N, dim] where each bit represents a binary position.\n",
    "    \"\"\"\n",
    "\n",
    "    # Each row corresponds to an element in x; columns are the binary bits\n",
    "    encoding = ((x.unsqueeze(1) >> torch.arange(dim, device=x.device)) & 1).to(torch.float32)\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUNNModel(nn.Module):\n",
    "    def __init__(self, embedding_dim:int, output_dim:int, num_layers:int, hidden_dim:int):\n",
    "\n",
    "        super(AUNNModel, self).__init__()\n",
    "        \n",
    "        assert num_layers % 2 == 0 and num_layers >= 2, \"Number of layers must be even and at least 2.\"\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Input Layer\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.hidden_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        # Hidden Layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(self.num_layers - 2):  # Exclude input and output layers\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "                nn.SiLU()\n",
    "            ))\n",
    "\n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # He initialization for Swish activation\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        output = self.input_layer(x)\n",
    "        residual = output  # Initialize residual for skip connections\n",
    "\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            output = layer(output)\n",
    "\n",
    "            # Apply skip connection every two layers\n",
    "            if (idx + 1) % 2 == 0:\n",
    "                output = output + residual  # Skip connection\n",
    "                residual = output  # Update residual\n",
    "\n",
    "        output = self.output_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to save the model checkpoint\n",
    "def save_checkpoint(model, params, optimizer, losses, filename=\"checkpoint.pth\"):\n",
    "    \n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'losses': losses,\n",
    "    }\n",
    "\n",
    "    keys = ['embedding_dim', 'output_dim', 'num_layers', 'hidden_dim']\n",
    "    assert all(k in params for k in keys)\n",
    "    for k in keys:\n",
    "        checkpoint[k] = params[k]\n",
    "\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved with loss {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filename=\"checkpoint.pth\"):\n",
    "\n",
    "    checkpoint = torch.load(filename, weights_only=True)\n",
    "    \n",
    "    keys = ['embedding_dim', 'output_dim', 'num_layers', 'hidden_dim']\n",
    "    params = {k: checkpoint[k] for k in keys}\n",
    "    \n",
    "    model = AUNNModel(**params)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    losses = checkpoint['losses']\n",
    "\n",
    "    print(f\"Checkpoint loaded: loss {losses[-1]:.4f}\")\n",
    "\n",
    "    return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from array import array\n",
    "\n",
    "def load_mnist(images_path, labels_path, shuffle=False, seed=42):\n",
    "\n",
    "    labels = []\n",
    "    with open(labels_path, 'rb') as file:\n",
    "        magic, size = struct.unpack(\">II\", file.read(8)) \n",
    "        if magic != 2049:\n",
    "            raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "        labels = array(\"B\", file.read())\n",
    "\n",
    "    images = []\n",
    "    rows, cols = None, None\n",
    "    with open(images_path, 'rb') as file:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        if magic != 2051:\n",
    "            raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "        data = array(\"B\", file.read())\n",
    "        for i in range(size):\n",
    "            img = np.array(data[i * rows * cols:(i + 1) * rows * cols], dtype=np.uint8)\n",
    "            img = np.where(img > 0, 1, 0) #binarize\n",
    "            img.resize((rows, cols))\n",
    "            images.append(img)\n",
    "\n",
    "    assert len(images) == len(labels)\n",
    "\n",
    "    if shuffle:\n",
    "        random.seed(seed)\n",
    "        indices = list(range(len(images)))\n",
    "        random.shuffle(indices)\n",
    "        images = [images[i] for i in indices]\n",
    "        labels = [labels[i] for i in indices]\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cur_dir = Path().resolve()\n",
    "input_path = cur_dir / 'mnist'\n",
    "training_images_filepath = input_path / 'train-images.idx3-ubyte'\n",
    "training_labels_filepath = input_path /'train-labels.idx1-ubyte'\n",
    "test_images_filepath = input_path / 't10k-images.idx3-ubyte'\n",
    "test_labels_filepath = input_path / 't10k-labels.idx1-ubyte'\n",
    "\n",
    "images, labels = load_mnist(training_images_filepath, training_labels_filepath, shuffle=True, seed=0)\n",
    "\n",
    "label2idx = {}\n",
    "for idx, label in enumerate(labels):\n",
    "    if label not in label2idx:\n",
    "        label2idx[label] = []\n",
    "    label2idx[label].append(idx)\n",
    "\n",
    "img_size = len(images[0].flatten())\n",
    "\n",
    "plt.imshow(images[0], cmap='gray')\n",
    "plt.show()\n",
    "print(labels[0])\n",
    "print(img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "embedd_dim = 48\n",
    "num_layers = 8     # Must be even and at least 2 (bc of skip connections)\n",
    "hidden_dim = 512  # Size of hidden layers\n",
    "output_dim = 2\n",
    "\n",
    "eos_bos_len = 8\n",
    "label_len = 10\n",
    "entry_length = eos_bos_len * 2 + label_len * 2 + img_size\n",
    "assert entry_length < 1024, \"Entry length must be less than 1024, improves positional encoding consistency.\"\n",
    "print(entry_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_provider(\n",
    "    images, \n",
    "    labels, \n",
    "    num_epochs,\n",
    "    batch_shuffle=True):\n",
    "\n",
    "    print(len(images), len(labels))\n",
    "\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    N = 0\n",
    "\n",
    "    for img, label in zip(images, labels):\n",
    "\n",
    "        img_data = img.flatten()\n",
    "        targets.append(img_data)\n",
    "\n",
    "        #one hot encode label\n",
    "        label_data = np.zeros(len(keep_labels), dtype=np.uint8)\n",
    "        label_data[label] = 1\n",
    "        targets.append(label_data)\n",
    "\n",
    "        start = N * 1024\n",
    "        indices = torch.arange(start, start + len(img_data) + len(label_data))\n",
    "        embedds = positional_encoding(indices, embedd_dim)\n",
    "        inputs.append(embedds)\n",
    "        N += 1\n",
    "\n",
    "    targets = np.concatenate(targets, axis=0)\n",
    "    targets = torch.tensor(targets, dtype=torch.uint8)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    inputs = torch.cat(inputs, dim=0)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    print(inputs.size(), targets.size())\n",
    "\n",
    "    batch_indices = list(range(num_batches))\n",
    "    if batch_shuffle:\n",
    "        random.shuffle(batch_indices)\n",
    "\n",
    "    for epoch in tqdm(list(range(num_epochs))):\n",
    "\n",
    "        for batch_idx in batch_indices:\n",
    "\n",
    "            batch_start = batch_idx * entries_per_batch * (len(img_data) + len(label_data))\n",
    "            batch_end = (batch_idx + 1) * entries_per_batch * (len(img_data) + len(label_data))\n",
    "\n",
    "            yield epoch, inputs[batch_start:batch_end], targets[batch_start:batch_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = AUNNModel(embedd_dim, output_dim, num_layers, hidden_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "model.train()\n",
    "\n",
    "#set learning rate\n",
    "\n",
    "last_epoch = 0\n",
    "epoch_losses = []\n",
    "batch_num = 0\n",
    "for epoch, inputs, targets in data_provider(images, labels, num_epochs):\n",
    "    \n",
    "    if epoch != last_epoch: # epoch logging\n",
    "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch [{last_epoch}/{num_epochs}] completed, Loss: {avg_loss:.8f}\")\n",
    "        batch_num = 0\n",
    "        epoch_losses = []\n",
    "        last_epoch = epoch\n",
    "        \n",
    "    batch_num += 1\n",
    "    \n",
    "    # do optimization\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if loss != loss:\n",
    "        print(\"ERR loss is NaN\")\n",
    "\n",
    "    cur_loss = loss.item()\n",
    "    epoch_losses.append(cur_loss)\n",
    "\n",
    "    # if batch_num % 10 == 0: # batch logging\n",
    "    #     print(f\"Epoch [{last_epoch}/{num_epochs}], Batch [{batch_num}/{num_batches}], Loss: {cur_loss:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curves\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_output(start_idx):\n",
    "    model.eval()\n",
    "    indices = torch.arange(start_idx, start_idx+img_size+len(keep_labels)).to(device)\n",
    "    inputs = positional_encoding(indices, embedd_dim)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    img = predicted[:img_size].cpu().numpy().reshape(28, 28)\n",
    "    label = predicted[img_size:img_size+10].cpu().numpy()\n",
    "    label = np.argmax(label)\n",
    "    return img, label\n",
    "\n",
    "start = num_entries - 2\n",
    "# start = 0\n",
    "for idx in range(start, start+10):\n",
    "    seq_start = idx * 1024\n",
    "    img, label = make_output(seq_start)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 0\n",
    "seq_start = item * 1024\n",
    "img, label = make_output(seq_start)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "print(label)\n",
    "\n",
    "item = 80\n",
    "seq_start = item * 1024\n",
    "img, label = make_output(seq_start)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 0\n",
    "image = images[item]\n",
    "label = labels[item]\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "print(label)\n",
    "\n",
    "item = 80\n",
    "image = images[item]\n",
    "label = labels[item]\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before conditioning\n",
    "print('before conditioning')\n",
    "seq_start = 80 * 1024\n",
    "img, label = make_output(seq_start)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "print(label)\n",
    "\n",
    "print('-'*80)\n",
    "\n",
    "#now let's teach the model that the correct pixels for item 80 are the pixels of item 0\n",
    "#then we'll see if the model changes the corresponding label of 80 from \"1\" to \"0\"\n",
    "\n",
    "target = images[0].flatten()\n",
    "target = torch.tensor(target, dtype=torch.uint8).to(device)\n",
    "inputs = torch.arange(seq_start, seq_start + img_size).to(device)\n",
    "inputs = positional_encoding(inputs, embedd_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "conditioning_optimizer = optim.SGD(model.parameters())\n",
    "model.train()\n",
    "num_conditioning_steps = 100\n",
    "\n",
    "for step in range(num_conditioning_steps):\n",
    "    conditioning_optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, target)\n",
    "    loss.backward()\n",
    "    conditioning_optimizer.step()\n",
    "    print(f\"Conditioning Step {step+1}/{num_conditioning_steps}, Loss: {loss.item():.6f}\")\n",
    "    img, label = make_output(seq_start)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "    print(label)\n",
    "\n",
    "print('-'*80)\n",
    "print('after conditioning')\n",
    "img, label = make_output(seq_start)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to save the model checkpoint\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filename=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved at epoch {epoch} with loss {loss:.4f}\")\n",
    "\n",
    "save_checkpoint(model, optimizer, epoch+1, avg_loss, filename=f\"mnist/checkpoint_epoch_{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, filename=\"checkpoint.pth\"):\n",
    "    checkpoint = torch.load(filename, weights_only=True)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Checkpoint loaded: epoch {epoch}, loss {loss:.4f}\")\n",
    "    return epoch, loss\n",
    "\n",
    "model = AUNNModel(embedd_dim, output_dim, num_layers, hidden_dim).to(device)\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "start_epoch, start_loss = load_checkpoint(model, optimizer, filename=\"mnist/checkpoint_epoch_1000.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
