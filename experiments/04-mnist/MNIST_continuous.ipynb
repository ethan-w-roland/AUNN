{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Classification / Generation using AUNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#deallocate all cuda memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#print cuda memory\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_dim:int,\n",
    "        output_dim:int, \n",
    "        num_layers:int, \n",
    "        hidden_dim:int):        \n",
    "\n",
    "        assert num_layers >= 2, \"Number of layers must be at least 2\"\n",
    "\n",
    "        super(AUNNModel, self).__init__() \n",
    "    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Input Layer\n",
    "        self.input_layer =  nn.Linear(self.embedding_dim, self.hidden_dim)\n",
    "\n",
    "        # Hidden Layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(self.num_layers - 2):  # Exclude input and output layers\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "                nn.SiLU(),\n",
    "                nn.RMSNorm(self.hidden_dim)\n",
    "            ))\n",
    "\n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # Kaiming He initialization for Swish activation\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    # binary\n",
    "    def encode(self, x: torch.Tensor):\n",
    "        dim = self.embedding_dim\n",
    "        encoding = ((x.unsqueeze(1) >> torch.arange(dim, device=x.device)) & 1).to(torch.float32)\n",
    "        return encoding\n",
    "\n",
    "    def forward(self, indices):\n",
    "        \n",
    "        x = self.encode(indices)\n",
    "        x = self.input_layer(x)\n",
    "        x = x + nn.SiLU()(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)  # MLP output with skip connection\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to save the model checkpoint\n",
    "def save_checkpoint(model, params, optimizer, losses, filename=\"checkpoint.pth\"):\n",
    "    \n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'losses': losses,\n",
    "        'params':{}\n",
    "    }\n",
    "\n",
    "    keys = ['embedding_dim', 'output_dim', 'num_layers', 'hidden_dim']\n",
    "    assert all(k in params for k in keys)\n",
    "    for k in keys:\n",
    "        checkpoint['params'][k] = params[k]\n",
    "\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved with loss {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filename=\"checkpoint.pth\"):\n",
    "\n",
    "    checkpoint = torch.load(filename, weights_only=True)\n",
    "    \n",
    "    params = checkpoint['params']\n",
    "    model = AUNNModel(**params)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    losses = checkpoint['losses']\n",
    "\n",
    "    print(f\"Checkpoint loaded: loss {losses[-1]:.4f}\")\n",
    "\n",
    "    return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from array import array\n",
    "\n",
    "def load_mnist(images_path, labels_path, shuffle:bool=False, binarize:bool=True, seed=42):\n",
    "\n",
    "    labels = []\n",
    "    with open(labels_path, 'rb') as file:\n",
    "        magic, size = struct.unpack(\">II\", file.read(8)) \n",
    "        if magic != 2049:\n",
    "            raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "        labels = array(\"B\", file.read())\n",
    "\n",
    "    images = []\n",
    "    rows, cols = None, None\n",
    "    with open(images_path, 'rb') as file:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        if magic != 2051:\n",
    "            raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "        data = array(\"B\", file.read())\n",
    "        for i in range(size):\n",
    "            img = np.array(data[i * rows * cols:(i + 1) * rows * cols], dtype=np.uint8)\n",
    "            if binarize:\n",
    "                img = np.where(img > 0, 1, 0) \n",
    "            img.resize((rows, cols))\n",
    "            images.append(img)\n",
    "\n",
    "    assert len(images) == len(labels)\n",
    "\n",
    "    if shuffle:\n",
    "        random.seed(seed)\n",
    "        indices = list(range(len(images)))\n",
    "        random.shuffle(indices)\n",
    "        images = [images[i] for i in indices]\n",
    "        labels = [labels[i] for i in indices]\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def encode_patches(img, N):\n",
    "    \"\"\"\n",
    "    Encodes a (28x28) binary image into patches of size NxN.\n",
    "    Each NxN patch (N^2 bits) is converted into a single integer \n",
    "    in [0..(2^(N^2)-1)].\n",
    "    \n",
    "    Returns a flattened 1D array of length (28//N)*(28//N).\n",
    "    \"\"\"\n",
    "    # 1. Reshape: (dim//N, N, dim//N, N)\n",
    "    # 2. Transpose to group NxN patches -> (28//N, 28//N, N, N)\n",
    "    #    so each (i, j) index references one NxN patch\n",
    "    h = w = img.shape[0]\n",
    "    assert img.shape[0] == img.shape[1], \"Image must be square.\"\n",
    "    assert h % N == 0 and w % N == 0, \"Image dimensions must be divisible by N.\"\n",
    "    arr_NxN = img.reshape(h//N, N, w//N, N).transpose(0, 2, 1, 3)\n",
    "    # shape: (h//N, w//N, N, N)\n",
    "\n",
    "    # 3. Flatten NxN -> N^2 bits: shape becomes (h//N, w//N, N^2)\n",
    "    arr_flat = arr_NxN.reshape(h//N, w//N, N*N)\n",
    "\n",
    "    # 4. Multiply each bit by powers of 2 to combine into single integers\n",
    "    #    For example, if N=2, bitweights = [1, 2, 4, 8].\n",
    "    bitweights = np.array([1 << i for i in range(N*N)], dtype=arr_flat.dtype)\n",
    "    encoded = (arr_flat * bitweights).sum(axis=-1)  # shape (h//N, w//N)\n",
    "\n",
    "    # 5. Flatten to 1D array\n",
    "    return encoded.ravel()\n",
    "\n",
    "\n",
    "def decode_patches(encoded, N, dim=28):\n",
    "    \"\"\"\n",
    "    Decodes a flattened array of integers (each representing N^2 bits)\n",
    "    back to a (28x28) binary image with NxN patches.\n",
    "    \"\"\"\n",
    "    h = w = dim\n",
    "    num_patches = (h // N) * (w // N)\n",
    "    \n",
    "    # 1. Reshape from (num_patches,) -> (h//N, w//N)\n",
    "    arr_2d = encoded.reshape(h//N, w//N)\n",
    "\n",
    "    # 2. Extract bits for each integer using bitwise AND with [1, 2, 4, ..., 2^(N^2-1)]\n",
    "    bitweights = np.array([1 << i for i in range(N*N)], dtype=np.uint16)\n",
    "    # shape -> (h//N, w//N, N^2)\n",
    "    bits = ((arr_2d[..., None] & bitweights) > 0).astype(np.uint16)\n",
    "\n",
    "    # 3. Reshape bits back to NxN patches: (h//N, w//N, N, N)\n",
    "    arr_patches = bits.reshape(h//N, w//N, N, N)\n",
    "\n",
    "    # 4. Transpose to (h//N, N, w//N, N) then reshape to (h, w)\n",
    "    arr_patches = arr_patches.transpose(0, 2, 1, 3)  # shape -> (h//N, N, w//N, N)\n",
    "    decoded = arr_patches.reshape(h, w)\n",
    "\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def make_string(images, labels):\n",
    "    \n",
    "    targets = []\n",
    "    for img, label in zip(images, labels):\n",
    "\n",
    "        #prep img data, conver 0-255 to 0-1 float\n",
    "        img = img.flatten().astype(np.float32) / 255.0\n",
    "        img = img.reshape(-1, 1) #784x1\n",
    "        print(img.shape)\n",
    "        #add 10 columns of just zeros, new shape = 784x11\n",
    "        img = np.concatenate([img, np.zeros((len(img), 10), dtype=np.float32)], axis=1)\n",
    "        display(img)\n",
    "        display(img[:5]) #display first 5 rows\n",
    "        print(img.shape)\n",
    "\n",
    "        #prep label data - create one-hot encoding\n",
    "        label_data = np.zeros(10+1, dtype=np.float32)\n",
    "        label_data[label+1] = 1.0\n",
    "        num_repeats = len(img) // 8\n",
    "        label_data = np.tile(label_data, num_repeats).reshape(num_repeats, -1)\n",
    "        display(label_data)\n",
    "        display(label_data[:5]) #display first 5 rows\n",
    "        break\n",
    "\n",
    "        #add to target\n",
    "        targets.append(label)\n",
    "        targets.append(img)\n",
    "        targets.append(label)\n",
    "\n",
    "        #define lengths\n",
    "        img_len = len(img)\n",
    "        lbl_len = len(label)\n",
    "        ex_len = img_len + lbl_len * 2\n",
    "\n",
    "    targets = np.concatenate(targets, axis=0)\n",
    "    return (ex_len, img_len, lbl_len), targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros(10)\n",
    "x[1] = 1\n",
    "print(x)\n",
    "x = np.tile(x, 3).reshape(3, -1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m test_labels_filepath \u001b[38;5;241m=\u001b[39m input_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt10k-labels.idx1-ubyte\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m train_images, train_labels \u001b[38;5;241m=\u001b[39m load_mnist(training_images_filepath, training_labels_filepath, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binarize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 11\u001b[0m lengths, train_data  \u001b[38;5;241m=\u001b[39m \u001b[43mmake_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_data)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m training samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[31], line 128\u001b[0m, in \u001b[0;36mmake_string\u001b[1;34m(images, labels)\u001b[0m\n\u001b[0;32m    125\u001b[0m     lbl_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(label)\n\u001b[0;32m    126\u001b[0m     ex_len \u001b[38;5;241m=\u001b[39m img_len \u001b[38;5;241m+\u001b[39m lbl_len \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m--> 128\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (ex_len, img_len, lbl_len), targets\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cur_dir = Path().resolve()\n",
    "input_path = cur_dir / 'mnist'\n",
    "training_images_filepath = input_path / 'train-images.idx3-ubyte'\n",
    "training_labels_filepath = input_path /'train-labels.idx1-ubyte'\n",
    "test_images_filepath = input_path / 't10k-images.idx3-ubyte'\n",
    "test_labels_filepath = input_path / 't10k-labels.idx1-ubyte'\n",
    "\n",
    "train_images, train_labels = load_mnist(training_images_filepath, training_labels_filepath, shuffle=True, binarize=False)\n",
    "lengths, train_data  = make_string(train_images, train_labels)\n",
    "print(f\"{len(train_data):,} training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_images[999]\n",
    "img = np.where(img > 0.1, 1, 0)  #binarize\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "enc = encode_patches(img, 1)\n",
    "\n",
    "img = decode_patches(enc, 1)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_len, img_len, lbl_len = lengths\n",
    "print(f\"Example length: {ex_len}, Image length: {img_len}, Label length: {lbl_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_num = 100\n",
    "offset = ex_num * ex_len\n",
    "\n",
    "#get label bytes\n",
    "lbl_1 = train_data[offset:offset+lbl_len]\n",
    "offset += lbl_len\n",
    "print(lbl_1)\n",
    "\n",
    "#get image bytes\n",
    "img = train_data[offset:offset+img_len]\n",
    "print(img.reshape(28,28))\n",
    "offset += img_len\n",
    "img = decode_patches(img, 1)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "#get label bytes\n",
    "lbl_2 = train_data[offset:offset+lbl_len]\n",
    "print(lbl_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor(train_data, dtype=torch.long).to(device)\n",
    "print(f\"train_data: {train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "embedd_dim = 32\n",
    "num_layers = 12    # Must be even and at least 2 (bc of skip connections)\n",
    "hidden_dim = 768   # Size of hidden layers\n",
    "output_dim = 2**4  # bc N=2 patches\n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "model = AUNNModel(\n",
    "    embedding_dim=embedd_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_layers=num_layers, \n",
    "    hidden_dim=hidden_dim).to(device)\n",
    "print(f\"Model has {model.count_params():,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize, loss function, and optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "model.train()\n",
    "context_len = 4096\n",
    "history = []\n",
    "base_len = 1024\n",
    "\n",
    "for i in tqdm(range(len(train_data) - context_len), total=len(train_data) - context_len):\n",
    "        \n",
    "    start = i\n",
    "    end = i + context_len + 1\n",
    "    targets = train_data[start:end]\n",
    "    abs_indices = torch.arange(start, end)\n",
    "    data_indices = abs_indices % ex_len\n",
    "    assert base_len >= ex_len\n",
    "    ex_indices = abs_indices // ex_len\n",
    "    ex_indices = ex_indices * base_len\n",
    "    indices = ex_indices + data_indices\n",
    "    indices = indices.to(device)\n",
    "        \n",
    "    j = 0\n",
    "    while True:\n",
    "\n",
    "        outputs = model(indices)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss_val = loss.item()\n",
    "\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "        a = targets[-1].item()\n",
    "        b = predicted[-1].item()\n",
    "\n",
    "        if j == 0:\n",
    "            cur_ind = indices[-1].item()\n",
    "            rel_ind = cur_ind % base_len\n",
    "\n",
    "            if rel_ind == 0:\n",
    "                display(f'----- EXAMPLE {cur_ind // base_len} START -----')\n",
    "                display('LABEL START')\n",
    "\n",
    "            elif rel_ind == lbl_len:\n",
    "                display(f\"LABEL END\")\n",
    "                display('IMAGE START')\n",
    "                # see what the model would-have predicted for img at this point\n",
    "                img_inds = torch.arange(cur_ind, cur_ind + img_len).to(device)\n",
    "                img_outputs = model(img_inds)\n",
    "                img_predicted = img_outputs.argmax(dim=1)\n",
    "                img_predicted = img_predicted.cpu().numpy()\n",
    "                img_data = decode_patches(img_predicted, 1)\n",
    "                plt.imshow(img_data, cmap='gray')\n",
    "                plt.show()\n",
    "\n",
    "            elif rel_ind == lbl_len + img_len:\n",
    "                display('IMAGE END')\n",
    "                if len(history) >= img_len:\n",
    "                    img_data = history[-img_len:]\n",
    "                    img_data = np.array(img_data, dtype=np.uint16)\n",
    "                    img_data = decode_patches(img_data, 1)\n",
    "                    plt.imshow(img_data, cmap='gray')\n",
    "                    plt.show()\n",
    "                display('LABEL START')\n",
    "\n",
    "            elif rel_ind == lbl_len + img_len + lbl_len:\n",
    "                display('LABEL END')\n",
    "            losses.append(loss_val)\n",
    "            accuracies.append(a == b)\n",
    "            history.append(b)\n",
    "            print(a,b,'T' if a == b else 'F', f\"{loss_val:f} @{cur_ind}\" )\n",
    "        j += 1\n",
    "\n",
    "        loss_thresh = 0.0001\n",
    "        if a == b and loss_val < loss_thresh:\n",
    "            break\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(model, {\n",
    "    'embedding_dim': embedd_dim,\n",
    "    'output_dim': output_dim,\n",
    "    'num_layers': num_layers,\n",
    "    'hidden_dim': hidden_dim\n",
    "}, optimizer, losses, filename=\"mnist/checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "downsample = 1\n",
    "window = 100\n",
    "\n",
    "temp = []\n",
    "for epoch in losses[num_batches*2::downsample]:\n",
    "    avg = np.mean(epoch)\n",
    "    temp.append(avg)\n",
    "temp = np.convolve(temp, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(temp, label=\"Training Loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy curve\n",
    "temp = []\n",
    "for epoch in accuracies[num_batches*2::downsample]:\n",
    "    avg = np.mean(epoch)\n",
    "    temp.append(avg)\n",
    "temp = np.convolve(temp, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(temp, label=\"Training Accuracy\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_num = num_ex - 100\n",
    "offset = entry_num * ex_len\n",
    "data = train_data[offset:offset+ex_len].cpu().numpy()\n",
    "\n",
    "lbl_1 = data[0:lbl_len]\n",
    "img = data[lbl_len:lbl_len+img_len]\n",
    "lbl_2 = data[lbl_len+img_len:]\n",
    "\n",
    "print(lbl_1)\n",
    "img = decode_patches(img, 2)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "print(lbl_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_num = num_ex - 100\n",
    "offset = entry_num * ex_len\n",
    "abs_indices = torch.arange(offset, offset+ex_len, device=device)\n",
    "data_indices = abs_indices % ex_len\n",
    "ex_indices = abs_indices // ex_len\n",
    "ex_indices = ex_indices * 256\n",
    "indices = ex_indices + data_indices\n",
    "with torch.no_grad():\n",
    "    outputs = model(indices)\n",
    "    outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "outputs = outputs.cpu().numpy()\n",
    "lbl_1 = outputs[0:lbl_len]\n",
    "img = outputs[lbl_len:lbl_len+img_len]\n",
    "lbl_2 = outputs[lbl_len+img_len:]\n",
    "\n",
    "print(lbl_1)\n",
    "img = decode_patches(img, 2)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "print(lbl_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    entry_num = num_ex+100+i #unseen data\n",
    "    offset = entry_num * ex_len\n",
    "    abs_indices = torch.arange(offset, offset+ex_len, device=device)\n",
    "    data_indices = abs_indices % ex_len\n",
    "    ex_indices = abs_indices // ex_len\n",
    "    ex_indices = ex_indices * 256\n",
    "    indices = ex_indices + data_indices\n",
    "    with torch.no_grad():\n",
    "        outputs = model(indices)\n",
    "        outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    outputs = outputs.cpu().numpy()\n",
    "    lbl_1 = outputs[0:lbl_len]\n",
    "    img = outputs[lbl_len:lbl_len+img_len]\n",
    "    lbl_2 = outputs[lbl_len+img_len:]\n",
    "\n",
    "    print(lbl_1)\n",
    "    img = decode_patches(img, 2)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "    print(lbl_2)\n",
    "    print('-'*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
