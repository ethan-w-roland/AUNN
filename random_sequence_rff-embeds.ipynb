{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup & Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#deallocate all cuda memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#print cuda memory\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_dim:int, \n",
    "        output_dim:int, \n",
    "        num_layers:int, \n",
    "        hidden_dim:int,\n",
    "        context_len:int=10_000):        \n",
    "\n",
    "        assert num_layers >= 2, \"Number of layers must be at least 2\"\n",
    "\n",
    "        super(AUNNModel, self).__init__() \n",
    "    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Initialize embedding parameters\n",
    "        thresh = 0.1\n",
    "        alpha = -np.log(thresh) / (context_len ** 2)\n",
    "        M = embedding_dim // 2\n",
    "        self.w = torch.tensor(np.random.normal(0, np.sqrt(2 * alpha), M), device=device, dtype=torch.float32)\n",
    "        self.b = torch.tensor(np.random.uniform(0, 2 * np.pi, M), device=device, dtype=torch.float32)\n",
    "\n",
    "        # Input Layer\n",
    "        self.input_layer =  nn.Linear(self.embedding_dim, self.hidden_dim)\n",
    "\n",
    "        # Hidden Layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(self.num_layers - 2):  # Exclude input and output layers\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "                nn.SiLU()\n",
    "            ))\n",
    "\n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # Kaiming He initialization for Swish activation\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def encode(self, x: torch.Tensor):\n",
    "\n",
    "        dim = self.embedding_dim\n",
    "        inps = torch.outer(x, self.w) + self.b\n",
    "        embed = np.sqrt(2 / dim) * torch.concatenate([\n",
    "            torch.cos(inps),\n",
    "            torch.sin(inps)\n",
    "        ], dim=-1)\n",
    "        return embed\n",
    "\n",
    "    def forward(self, indices):\n",
    "        \n",
    "        x = self.encode(indices)\n",
    "        x = self.input_layer(x)\n",
    "        x = x + nn.SiLU()(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)  # MLP output with skip connection\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to save the model checkpoint\n",
    "def save_checkpoint(model, params, optimizer, losses, filename=\"checkpoint_rff.pth\"):\n",
    "    \n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'losses': losses,\n",
    "    }\n",
    "\n",
    "    keys = ['embedding_dim', 'output_dim', 'num_layers', 'hidden_dim', 'context_len']\n",
    "    assert all(k in params for k in keys)\n",
    "    for k in keys:\n",
    "        checkpoint[k] = params[k]\n",
    "\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved with loss {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filename=\"checkpoint_rff.pth\"):\n",
    "\n",
    "    checkpoint = torch.load(filename, weights_only=True)\n",
    "    \n",
    "    keys = ['embedding_dim', 'output_dim', 'num_layers', 'hidden_dim', 'context_len']\n",
    "    params = {k: checkpoint[k] for k in keys}\n",
    "    \n",
    "    model = AUNNModel(**params)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    losses = checkpoint['losses']\n",
    "\n",
    "    print(f\"Checkpoint loaded: loss {losses[-1]:.4f}\")\n",
    "\n",
    "    return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "embedd_dim = 4096\n",
    "num_layers = 4     # Must be even and at least 2 (bc of skip connections)\n",
    "hidden_dim = 128   # Size of hidden layers\n",
    "batch_size = 8192  # Adjust batch size for efficiency\n",
    "num_epochs = 2000\n",
    "context_len = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#really long repeated pattern\n",
    "'''\n",
    "text = \"abc\" * 10_000  # Repeat the sequence to create a long string\n",
    "'''\n",
    "\n",
    "#really long random pattern\n",
    "options = ['|aaa','|bbb','|ccc']\n",
    "num_sequences = 15_000\n",
    "num_repeats = num_sequences * len(options)\n",
    "num_repeats = num_repeats - num_repeats % batch_size # ensure num_repeats is a multiple of batch_size\n",
    "options = options * num_repeats\n",
    "random.seed(42)\n",
    "random.shuffle(options)\n",
    "text = ''.join(options)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "token_to_id = {token: id for id, token in enumerate(vocab)}\n",
    "id_to_token = {id: token for token, id in token_to_id.items()}\n",
    "token_ids = [token_to_id[char] for char in text]\n",
    "token_ids = torch.tensor(token_ids, dtype=torch.long).to(device)\n",
    "indices = torch.arange(len(token_ids)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = AUNNModel(embedd_dim, len(vocab), num_layers, hidden_dim, context_len=context_len).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "losses = []\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "total_steps = len(text) // batch_size\n",
    "if len(text) % batch_size != 0:\n",
    "    total_steps += 1\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    start_idxs = list(range(0, len(text), batch_size))\n",
    "    # random.seed(epoch)\n",
    "    # random.shuffle(start_idxs)\n",
    "    for start_idx in start_idxs:\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        inputs = indices[start_idx:end_idx]\n",
    "        targets = token_ids[start_idx:end_idx]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cur_loss = loss.item()\n",
    "        losses.append(cur_loss)\n",
    "\n",
    "        #calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct = (predicted == targets).sum().item()\n",
    "        cur_acc = correct / batch_size\n",
    "        accuracies.append(cur_acc)\n",
    "        \n",
    "    avg_loss = sum(losses[-len(start_idxs):]) / len(start_idxs)\n",
    "    avg_acc = sum(accuracies[-len(start_idxs):]) / len(start_idxs)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed, Average Loss: {avg_loss:.6f}, Average Accuracy: {avg_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(text) // batch_size\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'embedding_dim': embedd_dim,\n",
    "    'output_dim': len(vocab),\n",
    "    'num_layers': num_layers,\n",
    "    'hidden_dim': hidden_dim\n",
    "}\n",
    "save_checkpoint(model, params, optimizer, losses, filename=\"sequence/checkpoint_rff.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, losses = load_checkpoint(filename=\"sequence/checkpoint_rff.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curves\n",
    "plt.figure(figsize=(40, 6))\n",
    "plt.plot(losses[::], label=\"Training Loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy curves\n",
    "plt.figure(figsize=(40, 6))\n",
    "plt.plot(accuracies[::], label=\"Training Accuracy\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Text Function\n",
    "def generate_text(model, start_index, length):\n",
    "    model.eval()\n",
    "    generated_tokens = []\n",
    "    indices = torch.arange(start_index, start_index + length).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(indices)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    for id in predicted:\n",
    "        token = id_to_token.get(id.item(), \"<UNK>\")\n",
    "        generated_tokens.append(token)\n",
    "    return ''.join(generated_tokens)\n",
    "\n",
    "# Generate text starting from the next index after the training data\n",
    "start_index = 0\n",
    "generated_text = generate_text(model, start_index=start_index, length=1000)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n",
    "\n",
    "# Generate text starting from the next index after the training data\n",
    "start_index = len(text)\n",
    "generated_text = generate_text(model, start_index=start_index, length=100)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(text) - 1\n",
    "preds = generate_text(model, start_index=N+1, length=50_000)\n",
    "count = {}\n",
    "for char in preds:\n",
    "    count[char] = count.get(char, 0) + 1\n",
    "\n",
    "plt.bar(count.keys(), count.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of the model on the dataset via random sample of 100k characters\n",
    "import random\n",
    "\n",
    "# Number of samples\n",
    "num_samples = 100_000\n",
    "\n",
    "# Randomly select indices from the dataset\n",
    "sample_indices = random.sample(range(len(text)), num_samples)\n",
    "\n",
    "# Get positions and targets for the sampled indices\n",
    "inputs = []\n",
    "outputs = []\n",
    "for idx in sample_indices:\n",
    "    inp = embeds[idx]\n",
    "    inputs.append(inp)\n",
    "    output = token_ids[idx]\n",
    "    outputs.append(output)\n",
    "\n",
    "# Concatenate the inputs and outputs\n",
    "inputs = torch.stack(inputs).to(device)\n",
    "outputs = torch.stack(outputs).to(device)\n",
    "\n",
    "# Get model predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(inputs)\n",
    "    _, predicted = torch.max(predictions, 1)\n",
    "    \n",
    "# Calculate accuracy\n",
    "correct = (predicted == outputs).sum().item()\n",
    "accuracy = correct / num_samples\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(text) - 1\n",
    "\n",
    "#see value before conditioning\n",
    "generated_text = generate_text(model, start_index=N+1, length=150)\n",
    "print(\"Original Text:\")\n",
    "print(generated_text) # \"|a...\"\n",
    "print(\"\")\n",
    "\n",
    "#conditioning the model\n",
    "conditioning_targets = ['|','c']  # Desired tokens at N+1 and N+2, this also works if you use \"|b\"\n",
    "conditioning_positions = [N+1, N+2]\n",
    "# conditioning_targets = ['c','a','b'] #show conditioning works on discontinuous tokens\n",
    "# conditioning_positions = [N+2, N+7, N+12]\n",
    "\n",
    "conditioning_target_indices = [token_to_id[token] for token in conditioning_targets]\n",
    "targets_tensor = torch.tensor(conditioning_target_indices, dtype=torch.long).to(device)\n",
    "positions_tensor = torch.tensor(conditioning_positions)\n",
    "inputs = positional_encoding(positions_tensor, embedd_dim).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "conditioning_optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "model.train()\n",
    "num_conditioning_steps = 10\n",
    "\n",
    "for step in range(num_conditioning_steps):\n",
    "    conditioning_optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets_tensor)\n",
    "    loss.backward()\n",
    "    conditioning_optimizer.step()\n",
    "    print(f\"Conditioning Step {step+1}/{num_conditioning_steps}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "#see value after conditioning\n",
    "generated_text = generate_text(model, start_index=N+1, length=150)\n",
    "print(\"\")\n",
    "print(\"Text after conditioning:\")\n",
    "print(generated_text) # \"|ccc|...\" shows conditioning works because of new values for N+3 and N+4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
